{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76b5130-236d-4f97-bee8-2c1e15b081bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: requests in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (80.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "WARNING:tensorflow:From C:\\Users\\tjdwl\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
    "## í•„ìš”í•œ ë¼ì´ë²„ëŸ¬ë¦¬ë“¤ì´ ì—†ëŠ” ê²½ìš° ì•„ë˜ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜\n",
    "!pip install sentence-transformers xgboost scikit-learn pandas numpy joblib pyyaml tqdm\n",
    "!pip install tf-keras\n",
    "!pip install sentence-transformers\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "import joblib\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xgboost as xgb\n",
    "\n",
    "## ë¡œê¹… ì„¤ì •\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c82b16f9-6612-417b-bccb-62dbbbcbfda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "   (í´ë”ê°€ ì—†ë‹¤ë©´ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”)\n"
     ]
    }
   ],
   "source": [
    "# í´ë”ê°€ ì´ë¯¸ ë§Œë“¤ì–´ì ¸ ìˆë‹¤ë©´ ì•„ë˜ ì½”ë“œëŠ” ì‹¤í–‰í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.\n",
    "# í•„ìš”ì‹œ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.\n",
    "\n",
    "# def create_project_structure():\n",
    "#     \"\"\"í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "#     \n",
    "#     directories = [\n",
    "#         'data/raw',\n",
    "#         'data/processed', \n",
    "#         'data/embeddings',\n",
    "#         'models/xgboost',\n",
    "#         'models/encoders',\n",
    "#         'src',\n",
    "#         'notebooks',\n",
    "#         'saved_models',\n",
    "#         'config',\n",
    "#         'logs'\n",
    "#     ]\n",
    "#     \n",
    "#     for directory in directories:\n",
    "#         Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "#     \n",
    "#     print(\"âœ… í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# create_project_structure()  # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ\n",
    "\n",
    "print(\"ğŸ“ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"   (í´ë”ê°€ ì—†ë‹¤ë©´ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ba1325-1757-4c23-95c2-0ae84b5f2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì„¤ì • íŒŒì¼ ìƒì„±\n",
    "\n",
    "config = {\n",
    "    'model' : {\n",
    "        'sbert_model' : 'snunlp/KR-SBERT-V40K-klueNLI-augSTS',\n",
    "        'embedding_dim' : 768,\n",
    "        'reduced_dim' : 128,\n",
    "        'dimensionality_reduction': 'PCA' ,\n",
    "        'xgboost_params' : {\n",
    "            'max_depth' : 6,\n",
    "            'learning_rate' : 0.1,\n",
    "            'n_estimators' : 100,\n",
    "            'random_state' : 42\n",
    "        }\n",
    "    },\n",
    "    'data' : {\n",
    "        'raw_file' : 'data/raw/gangwon_places_100.csv',\n",
    "        'processed_file' : 'data/processed/gangwon_places_100_processed.csv',\n",
    "        'embeddings_file' : 'data/embeddings/place_embeddings_pca128.npy'\n",
    "    },\n",
    "    'paths': {\n",
    "        'models' : 'models',\n",
    "        'encoders' : 'models/encoders',\n",
    "        'logs' : 'logs'\n",
    "    }\n",
    "}\n",
    "\n",
    "## config í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "os.makedirs('config', exist_ok=True)\n",
    "\n",
    "## ì„¤ì • íŒŒì¼ ì €ì¥\n",
    "with open('config/config.yaml', 'w', encoding='utf-8') as f: \n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac4009e-fadd-425f-9ac0-d59c3da4c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "## ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "class DataPreprocessor: \n",
    "    \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.season_encoder = None\n",
    "        self.nature_encoder = MultiLabelBinarizer()\n",
    "        self.vibe_encoder = MultiLabelBinarizer()\n",
    "        self.target_encoder = MultiLabelBinarizer()\n",
    "\n",
    "    def parse_multi_label_string(self, text: str) -> List[str]:\n",
    "        \"\"\"ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "\n",
    "        # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ê³µë°± ì œê±°\n",
    "        items = [item.strip() for item in str(text).split(',')]\n",
    "        return [item for item in items if item] # ë¹ˆ ë¬¸ìì—´ ì œê±°\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame: \n",
    "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜\"\"\"\n",
    "        # ë³µì‚¬ë³¸ ìƒì„±\n",
    "        processed_df = df.copy()\n",
    "\n",
    "        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n",
    "        required_cols = ['name', 'season', 'nature', 'vibe', 'target', 'short_description']\n",
    "        missing_cols = [col for col in required_cols if col not in processed_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}\")\n",
    "\n",
    "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "        processed_df['short_description'] = processed_df['short_description'].fillna('')\n",
    "        processed_df['season'] = processed_df['season'].fillna('ì‚¬ê³„ì ˆ')\n",
    "        processed_df['nature'] = processed_df['nature'].fillna('')\n",
    "        processed_df['vibe'] = processed_df['vibe'].fillna('')\n",
    "        processed_df['target'] = processed_df['target'].fillna('')\n",
    "\n",
    "        # ë‹¤ì¤‘ ë¼ë²¨ íŒŒì‹±\n",
    "        processed_df['nature_list'] = processed_df['nature'].apply(self.parse_multi_label_string)\n",
    "        processed_df['vibe_list'] = processed_df['vibe'].apply(self.parse_multi_label_string)\n",
    "        processed_df['target_list'] = processed_df['target'].apply(self.parse_multi_label_string)\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ì •ê·œí™”\n",
    "        processed_df['short_description'] = processed_df['short_description'].apply(\n",
    "        lambda x: re.sub(r'[^\\w\\s]', '', str(x)) if pd.notna(x) else ''\n",
    "        )\n",
    "        return processed_df\n",
    "\n",
    "    def fit_encoders(self, df: pd.DataFrame):\n",
    "        \"\"\"ì¸ì½”ë”ë“¤ì„ í•™ìŠµ ë°ì´í„°ì— ë§ì¶¤\"\"\"\n",
    "\n",
    "        # ê³„ì ˆì€ ë‹¨ì¼ ë¼ë²¨ì´ë¯€ë¡œ LabelEncoder ëŒ€ì‹  ì§ì ‘ ì²˜ë¦¬\n",
    "        self.season_categories = sorted(df['season'].unique())\n",
    "\n",
    "        # ë‹¤ì¤‘ ë¼ë²¨ ì¸ì½”ë” í•™ìŠµ\n",
    "        self.nature_encoder.fit(df['nature_list'])\n",
    "        self.vibe_encoder.fit(df['vibe_list'])\n",
    "        self.target_encoder.fit(df['target_list'])\n",
    "\n",
    "        print(f\"ì¸ì½”ë” í•™ìŠµ ì™„ë£Œ\")\n",
    "        print(f\"   - ê³„ì ˆ ì¹´í…Œê³ ë¦¬: {self.season_categories}\")\n",
    "        print(f\"   - ìì—°í™˜ê²½ ì¹´í…Œê³ ë¦¬: {len(self.nature_encoder.classes_)}ê°œ\")\n",
    "        print(f\"   - ë¶„ìœ„ê¸° ì¹´í…Œê³ ë¦¬: {len(self.vibe_encoder.classes_)}ê°œ\")\n",
    "        print(f\"   - ëŒ€ìƒ ì¹´í…Œê³ ë¦¬: {len(self.target_encoder.classes_)}ê°œ\")\n",
    "\n",
    "    def encode_labels(self, df: pd.DataFrame) -> Dict[str,np.ndarray]:\n",
    "        \"\"\"ë¼ë²¨ë“¤ì„ ì¸ì½”ë”©\"\"\"\n",
    "\n",
    "        # ê³„ì ˆ ì¸ì½”ë”©(ì›-í•« ì¸ì½”ë”©)\n",
    "        season_encoded = np.zeros((len(df), len(self.season_categories)))\n",
    "        for i, season in enumerate(df['season']):\n",
    "            if season in self.season_categories:\n",
    "                season_idx = self.season_categories.index(season)\n",
    "                season_encoded[i, season_idx] = 1\n",
    "\n",
    "        # ë‹¤ì¤‘ ë¼ë²¨ ì¸ì½”ë“±\n",
    "        nature_encoded = self.nature_encoder.transform(df['nature_list'])\n",
    "        vibe_encoded = self.vibe_encoder.transform(df['vibe_list'])\n",
    "        target_encoded = self.target_encoder.transform(df['target_list'])\n",
    "\n",
    "        return{\n",
    "            'season' : season_encoded,\n",
    "            'nature' : nature_encoded,\n",
    "            'vibe' : vibe_encoded,\n",
    "            'target' : target_encoded\n",
    "        }\n",
    "\n",
    "    def save_encoders(self, base_path: str):\n",
    "        \"\"\"ì¸ì½”ë”ë“¤ì„ ì €ì¥\"\"\"\n",
    "        # ê³„ì ˆ ì¹´í…Œê³ ë¦¬ ì €ì¥\n",
    "        joblib.dump(self.season_categories, f\"{base_path}/season_encoder.joblib\")\n",
    "\n",
    "        # ë‹¤ì¤‘ ë¼ë²¨ ì¸ì½”ë” ì €ì¥\n",
    "        joblib.dump(self.nature_encoder, f\"{base_path}/nature_encoder.joblib\")\n",
    "        joblib.dump(self.vibe_encoder, f\"{base_path}/vibe_encoder.joblib\")\n",
    "        joblib.dump(self.target_encoder, f\"{base_path}/target_encoder.joblib\")\n",
    "\n",
    "        print(f\"ì¸ì½”ë” ì €ì¥ ì™„ë£Œ: {base_path}\")\n",
    "\n",
    "    def load_encoders(self, base_path: str):\n",
    "        \"\"\"ì¸ì½”ë” ë¡œë“œ\"\"\"\n",
    "\n",
    "        self.season_categories = joblib.load(f\"{base_path}/season_encoder.joblib\")\n",
    "        self.nature_encoder = joblib.load(f\"{base_path}/nature_encoder.joblib\")\n",
    "        self.vibe_encoder = joblib.load(f\"{base_path}/vibe_encoder.joblib\")\n",
    "        self.target_encoder = joblib.load(f\"{base_path}/target_encoder.joblib\")\n",
    "\n",
    "        print(f\"ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ: {base_path}\")\n",
    "\n",
    "print(f\"ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e775a6e-6fdb-4c22-a1c7-024a7e48df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© ìƒì„± í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "## ì„ë² ë”© ìƒì„± í´ë˜ìŠ¤ ì •ì˜\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"SBERT ì„ë² ë”© ìƒì„± ë° ì°¨ì› ì¶•ì†Œ í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.dimension_reducer = None\n",
    "        self.reduced_dim = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"SBERT ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        print(f\"SBERT ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}\")\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        print(\"SBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ì„ë² ë”© ìƒì„±\"\"\"\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "        print(f\"ì„ë² ë”© ìƒì„± ì¤‘... (ì´ {len(texts)}ê°œ í…ìŠ¤íŠ¸)\")\n",
    "  \n",
    "        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„±(ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)\n",
    "        batch_size = 32\n",
    "        embeddings = []\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch_texts, convert_to_numpy=True)\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        print(f\"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def fit_dimension_reducer(self, embeddings: np.ndarray, method: str = 'PCA',\n",
    "                              target_dim: int = 128):\n",
    "        \"\"\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "\n",
    "        self.reduced_dim = target_dim\n",
    "\n",
    "        if method =='PCA':\n",
    "            self.dimension_reducer = PCA(n_components=target_dim, random_state=42)\n",
    "        elif method =='TruncatedSVD':\n",
    "            self.dimension_reducer = TruncatedSVD(n_components=target_dim, random_state=42)\n",
    "        else: \n",
    "            raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì°¨ì› ì¶•ì†Œ ë°©ë²•: {method}\")\n",
    "\n",
    "        print(f\"{method}ë¥¼ ì‚¬ìš©í•˜ì—¬ {embeddings.shape[1]}ì°¨ì› -> {target_dim}ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ\")\n",
    "        self.dimension_reducer.fit(embeddings)\n",
    "\n",
    "        # ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ ì¶œë ¥(PCAì˜ ê²½ìš°)\n",
    "        if method =='PCA':\n",
    "            explained_variance_ratio = self.dimension_reducer.explained_variance_ratio_\n",
    "            cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "            print(f\"ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨: {cumulative_variance[-1]:.4f}\")\n",
    "\n",
    "        print(f\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "    def reduce_dimensions(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ì„ë² ë”© ì°¨ì› ì¶•ì†Œ\"\"\"\n",
    "\n",
    "        if self.dimension_reducer is None:\n",
    "            raise ValueError(\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        reduced_embeddings = self.dimension_reducer.transform(embeddings)\n",
    "        print(f\"ì°¨ì› ì¶•ì†Œ ì™„ë£Œ: {embeddings.shape} -> {reduced_embeddings.shape}\")\n",
    "\n",
    "        return reduced_embeddings\n",
    "\n",
    "    def save_dimension_reducer(self, filepath: str):\n",
    "        \"\"\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ ì €ì¥\"\"\"\n",
    "\n",
    "        model_data = {\n",
    "        'reducer': self.dimension_reducer,\n",
    "        'reduced_dim' : self.reduced_dim,\n",
    "        'model_name' : self.model_name\n",
    "        }\n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ ì €ì¥: {filepath}\")\n",
    "\n",
    "    def load_dimension_reducer(self, filepath: str):\n",
    "        \"\"\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "\n",
    "        model_data = joblib.load(filepath)\n",
    "        self.dimension_reducer = model_data['reducer']\n",
    "        self.reduced_dim = model_data['reduced_dim']\n",
    "        self.model_name = model_data['model_name']\n",
    "\n",
    "        print(f\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ ë¡œë“œ: {filepath}\")\n",
    "\n",
    "print(\"ì„ë² ë”© ìƒì„± í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232b2813-e02a-4589-9a3e-25c354b01830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost í•™ìŠµ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "### XGBoost í•™ìŠµ í´ë˜ìŠ¤ ì •ì˜\n",
    "\n",
    "class XGBoostTrainer:\n",
    "    \"\"\"XGBoost ë¶„ë¥˜ê¸° í•™ìŠµ í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self, xgb_params: Dict):\n",
    "        self.xgb_params = xgb_params\n",
    "        self.models = {}\n",
    "        self.label_types = ['season', 'nature', 'vibe', 'target']\n",
    "\n",
    "    def train_models(self, feature: np.ndarray, labels: Dict[str, np.ndarray]):\n",
    "        \"\"\"ëª¨ë“  ë¼ë²¨ íƒ€ì…ì— ëŒ€í•´ ë¶„ë¥˜ê¸° í•™ìŠµ\"\"\"\n",
    "\n",
    "        print(\"XGBoost ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            print(f\"\\n{label_type} ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘...\")\n",
    "\n",
    "            y = labels[label_type]\n",
    "\n",
    "            if label_type == 'season':\n",
    "                #ë‹¨ì¼ ë¼ë²¨: ì›-í•«ì—ì„œ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "                y_single = np.argmax(y, axis=1)\n",
    "\n",
    "                model = xgb.XGBClassifier(**self.xgb_params)\n",
    "                model.fit(features, y_single)\n",
    "                           \n",
    "            else: \n",
    "                #ë‹¤ì¤‘ ë¼ë²¨: OneVsRestClassifier ì‚¬ìš©\n",
    "                model = OneVsRestClassifier(\n",
    "                    xgb.XGBClassifier(**self.xgb_params)\n",
    "                )\n",
    "                model.fit(features, y)\n",
    "\n",
    "            self.models[label_type] = model\n",
    "            print(f\"ëª¨ë“  XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "    def evaluate_models(self, features: np.ndarray, labels: Dict[str,np.ndarray]):\n",
    "        \"\"\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
    "\n",
    "        print(\"\\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€===\")\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            print(f\"\\n[{label_type}] ì„±ëŠ¥ í‰ê°€: \")\n",
    "\n",
    "\n",
    "            y_true = labels[label_type]\n",
    "            model = self.models[label_type]\n",
    "\n",
    "            if label_type == 'season':\n",
    "                # ë‹¨ì¼ ë¼ë²¨ í‰ê°€\n",
    "\n",
    "                y_true_single = np.argmax(y_true, axis=1)\n",
    "                y_pred = model.predict(features)\n",
    "\n",
    "                accuracy = accuracy_score(y_true_single, y_pred)\n",
    "                f1 = f1_score(y_true_single, y_pred, average='weighted')\n",
    "\n",
    "                print(f\"Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "            else: \n",
    "                # ë‹¤ì¤‘ ë¼ë²¨ í‰ê°€\n",
    "                y_pred = model.predict(features)\n",
    "\n",
    "                accuracy = accuracy_score(y_true, y_pred)\n",
    "                f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "                f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "                print(f\"Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"F1-Score (Micro): {f1_micro:.4f}\")\n",
    "                print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
    "                \n",
    "    def save_models(self,base_path: str):\n",
    "        \"\"\"ëª¨ë¸ë“¤ ì €ì¥\"\"\"\n",
    "        for label_type in self.label_types:\n",
    "            model_path = f\"{base_path}/xgboost/{label_type}_model.joblib\"\n",
    "            joblib.dump(self.models[label_type], model_path)\n",
    "            print(f\"{label_type} ëª¨ë¸ ì €ì¥: {model_path}\")\n",
    "\n",
    "    def load_models(self,base_path: str):\n",
    "        \"\"\"ëª¨ë¸ë“¤ ë¡œë“œ\"\"\"\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            model_path = f\"{base_path}/xgboost/{label_type}_model.joblib\"\n",
    "            self.models[label_type] = joblib.load(model_path)\n",
    "            print(f\"{label_type} ëª¨ë¸ ë¡œë“œ: {model_path}\")\n",
    "\n",
    "print(\"XGBoost í•™ìŠµ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5e54007-c9a4-41e9-90c0-cb6ee76bd9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìˆ˜ì •ëœ ì¶”ì²œ ì‹œìŠ¤í…œ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "## ì¶”ì²œ ì‹œìŠ¤í…œ í´ë˜ìŠ¤ ì •ì˜\n",
    "\n",
    "# ìƒˆë¡œìš´ ì…€ì—ì„œ GangwonPlaceRecommender í´ë˜ìŠ¤ ì¬ì •ì˜\n",
    "class GangwonPlaceRecommender:\n",
    "    \"\"\"ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤ (ìˆ˜ì •ëœ ë²„ì „)\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = 'config/config.yaml'):\n",
    "        # ì„¤ì • ë¡œë“œ\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        \n",
    "        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”\n",
    "        self.preprocessor = DataPreprocessor()\n",
    "        self.embedding_generator = EmbeddingGenerator(\n",
    "            self.config['model']['sbert_model']\n",
    "        )\n",
    "        self.xgb_trainer = XGBoostTrainer(\n",
    "            self.config['model']['xgboost_params']\n",
    "        )\n",
    "        \n",
    "        # ë°ì´í„° ì €ì¥ìš©\n",
    "        self.df = None\n",
    "        self.place_embeddings = None\n",
    "        self.place_names = None\n",
    "        \n",
    "        # íƒœê·¸ ë§¤í•‘ (ììœ  ë¬¸ì¥ íŒŒì‹±ìš©)\n",
    "        self.tag_mapping = {\n",
    "            'season': {\n",
    "                'ë´„': ['ë´„', '3ì›”', '4ì›”', '5ì›”', 'ë²šê½ƒ', 'ê½ƒ'],\n",
    "                'ì—¬ë¦„': ['ì—¬ë¦„', '6ì›”', '7ì›”', '8ì›”', 'ë°”ë‹¤', 'í•´ë³€', 'ì‹œì›', 'ë¬¼'],\n",
    "                'ê°€ì„': ['ê°€ì„', '9ì›”', '10ì›”', '11ì›”', 'ë‹¨í’', 'ì–µìƒˆ', 'ë¹¨ê°„'],\n",
    "                'ê²¨ìš¸': ['ê²¨ìš¸', '12ì›”', '1ì›”', '2ì›”', 'ëˆˆ', 'ìŠ¤í‚¤', 'ì¶”ìš´'],\n",
    "                'ì‚¬ê³„ì ˆ': ['ì‚¬ê³„ì ˆ', 'ì—°ì¤‘', 'ì–¸ì œë‚˜']\n",
    "            },\n",
    "            'nature': {\n",
    "                'ì‚°': ['ì‚°', 'ë“±ì‚°', 'íŠ¸ë ˆí‚¹', 'í•˜ì´í‚¹', 'ì‚°ì±…', 'ì˜¤ë¥´ë§‰'],\n",
    "                'ë°”ë‹¤': ['ë°”ë‹¤', 'í•´ë³€', 'ë°”ë‹·ê°€', 'ìˆ˜ì˜', 'íŒŒë„'],\n",
    "                'í˜¸ìˆ˜': ['í˜¸ìˆ˜', 'ì—°ëª»', 'ë¬¼ê°€', 'ì €ìˆ˜ì§€'],\n",
    "                'ê³„ê³¡': ['ê³„ê³¡', 'ì‹œëƒ‡ë¬¼', 'ê°œìš¸', 'ë¬¼ì†Œë¦¬'],\n",
    "                'ìì—°': ['ìì—°', 'ìˆ²', 'ë‚˜ë¬´', 'í’€', 'ì‹ë¬¼'],\n",
    "                'ë„ì‹œ': ['ë„ì‹œ', 'ì‹œë‚´', 'ë²ˆí™”ê°€', 'ìƒì ']\n",
    "            },\n",
    "            'vibe': {\n",
    "                'ê°ì„±': ['ê°ì„±', 'ê°ì„±ì ', 'ë¡œë§¨í‹±', 'ë‚­ë§Œ', 'ì˜ˆìœ'],\n",
    "                'í™œë ¥': ['í™œë ¥', 'í™œê¸°', 'ì‹ ë‚˜ëŠ”', 'ì¦ê±°ìš´', 'ì¬ë¯¸'],\n",
    "                'íœ´ì‹': ['íœ´ì‹', 'ì‰¬ëŠ”', 'í¸ì•ˆ', 'ì¡°ìš©', 'í‰ì˜¨', 'íë§'],\n",
    "                'ì‚°ì±…': ['ì‚°ì±…', 'ê±·ê¸°', 'ê±°ë‹ê¸°', 'ì²œì²œíˆ'],\n",
    "                'ëª¨í—˜': ['ëª¨í—˜', 'ìŠ¤ë¦´', 'ë„ì „', 'ìµìŠ¤íŠ¸ë¦¼']\n",
    "            },\n",
    "            'target': {\n",
    "                'ì—°ì¸': ['ì—°ì¸', 'ì»¤í”Œ', 'ë‚¨ì¹œ', 'ì—¬ì¹œ', 'ì• ì¸'],\n",
    "                'ê°€ì¡±': ['ê°€ì¡±', 'ë¶€ëª¨', 'ì•„ì´', 'ìë…€', 'ì•„ê¸°'],\n",
    "                'ì¹œêµ¬': ['ì¹œêµ¬', 'ì¹œêµ¬ë“¤', 'ë™ë£Œ', 'ê°™ì´'],\n",
    "                'í˜¼ì': ['í˜¼ì', 'ë‚˜ë§Œ', 'ë‹¨ë…', 'ì†”ë¡œ']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def parse_user_input(self, user_input: Dict) -> Dict:\n",
    "        \"\"\"ì‚¬ìš©ì ì…ë ¥ì„ íŒŒì‹±í•˜ì—¬ í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "        \n",
    "        parsed = {\n",
    "            'season': None,\n",
    "            'nature': [],\n",
    "            'vibe': [],\n",
    "            'target': []\n",
    "        }\n",
    "        \n",
    "        # ììœ  ë¬¸ì¥ ì…ë ¥ ì²˜ë¦¬\n",
    "        if 'free_text' in user_input:\n",
    "            text = user_input['free_text'].lower()\n",
    "            \n",
    "            # ê° íƒœê·¸ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë§¤ì¹­\n",
    "            for category, tag_dict in self.tag_mapping.items():\n",
    "                for tag, keywords in tag_dict.items():\n",
    "                    if any(keyword in text for keyword in keywords):\n",
    "                        if category == 'season':\n",
    "                            parsed['season'] = tag\n",
    "                        else:\n",
    "                            if tag not in parsed[category]:\n",
    "                                parsed[category].append(tag)\n",
    "        \n",
    "        # ì§ì ‘ íƒœê·¸ ì…ë ¥ ì²˜ë¦¬\n",
    "        else:\n",
    "            if 'season' in user_input:\n",
    "                parsed['season'] = user_input['season']\n",
    "            \n",
    "            for category in ['nature', 'vibe', 'target']:\n",
    "                if category in user_input:\n",
    "                    if isinstance(user_input[category], list):\n",
    "                        parsed[category] = user_input[category]\n",
    "                    else:\n",
    "                        parsed[category] = [user_input[category]]\n",
    "        \n",
    "        return parsed\n",
    "    \n",
    "    def calculate_hybrid_score(self, user_input: Dict, \n",
    "                             similarity_weight: float = 0.6,\n",
    "                             tag_weight: float = 0.4) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"ìœ ì‚¬ë„ ì ìˆ˜ì™€ íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "        \n",
    "        # ì‚¬ìš©ì ì…ë ¥ íŒŒì‹±\n",
    "        parsed_input = self.parse_user_input(user_input)\n",
    "        \n",
    "        # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°\n",
    "        if 'free_text' in user_input:\n",
    "            query_text = user_input['free_text']\n",
    "        else:\n",
    "            # íƒœê·¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\n",
    "            query_parts = []\n",
    "            if parsed_input['season']:\n",
    "                query_parts.append(f\"{parsed_input['season']}ì—\")\n",
    "            if parsed_input['target']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['target'])}ì™€\")\n",
    "            if parsed_input['nature']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['nature'])}ì—ì„œ\")\n",
    "            if parsed_input['vibe']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['vibe'])} ì—¬í–‰\")\n",
    "            \n",
    "            query_text = ' '.join(query_parts)\n",
    "        \n",
    "        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
    "        if self.embedding_generator.model is None:\n",
    "            self.embedding_generator.load_model()\n",
    "        \n",
    "        query_embedding = self.embedding_generator.model.encode([query_text])\n",
    "        \n",
    "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° - [0] ì¸ë±ìŠ¤ë¡œ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜\n",
    "        similarity_scores = cosine_similarity(\n",
    "            query_embedding, \n",
    "            self.place_embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # 2. íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°\n",
    "        tag_scores = np.zeros(len(self.df))\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            score = 0\n",
    "            total_weight = 0\n",
    "            \n",
    "            # ê³„ì ˆ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.3)\n",
    "            if parsed_input['season'] and row['season'] == parsed_input['season']:\n",
    "                score += 0.3\n",
    "            total_weight += 0.3\n",
    "            \n",
    "            # ìì—°í™˜ê²½ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.25)\n",
    "            if parsed_input['nature']:\n",
    "                nature_match = len(set(parsed_input['nature']) & set(row['nature_list']))\n",
    "                if nature_match > 0:\n",
    "                    score += 0.25 * (nature_match / len(parsed_input['nature']))\n",
    "            total_weight += 0.25\n",
    "            \n",
    "            # ë¶„ìœ„ê¸° ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.25)\n",
    "            if parsed_input['vibe']:\n",
    "                vibe_match = len(set(parsed_input['vibe']) & set(row['vibe_list']))\n",
    "                if vibe_match > 0:\n",
    "                    score += 0.25 * (vibe_match / len(parsed_input['vibe']))\n",
    "            total_weight += 0.25\n",
    "            \n",
    "            # ëŒ€ìƒ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.2)\n",
    "            if parsed_input['target']:\n",
    "                target_match = len(set(parsed_input['target']) & set(row['target_list']))\n",
    "                if target_match > 0:\n",
    "                    score += 0.2 * (target_match / len(parsed_input['target']))\n",
    "            total_weight += 0.2\n",
    "            \n",
    "            # ì •ê·œí™”\n",
    "            tag_scores[idx] = score / total_weight if total_weight > 0 else 0\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°\n",
    "        hybrid_scores = (similarity_weight * similarity_scores + \n",
    "                        tag_weight * tag_scores)\n",
    "        \n",
    "        return hybrid_scores, similarity_scores, tag_scores\n",
    "    \n",
    "    def recommend_places(self, user_input: Dict, top_k: int = 10) -> Dict:\n",
    "        \"\"\"ê´€ê´‘ì§€ ì¶”ì²œ ë©”ì¸ í•¨ìˆ˜\"\"\"\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°\n",
    "        hybrid_scores, similarity_scores, tag_scores = self.calculate_hybrid_score(user_input)\n",
    "        \n",
    "        # ìƒìœ„ kê°œ ì¶”ì²œì§€ ì„ íƒ\n",
    "        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "        \n",
    "        # ì¶”ì²œ ê²°ê³¼ êµ¬ì„±\n",
    "        recommendations = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            place_info = {\n",
    "                'name': self.df.iloc[idx]['name'],\n",
    "                'season': self.df.iloc[idx]['season'],\n",
    "                'nature': self.df.iloc[idx]['nature_list'],\n",
    "                'vibe': self.df.iloc[idx]['vibe_list'],\n",
    "                'target': self.df.iloc[idx]['target_list'],\n",
    "                'description': self.df.iloc[idx]['short_description'],\n",
    "                'hybrid_score': float(hybrid_scores[idx]),\n",
    "                'similarity_score': float(similarity_scores[idx]),\n",
    "                'tag_score': float(tag_scores[idx])\n",
    "            }\n",
    "            recommendations.append(place_info)\n",
    "        \n",
    "        # íŒŒì‹±ëœ ì‚¬ìš©ì ì…ë ¥ ì •ë³´ ì¶”ê°€\n",
    "        parsed_input = self.parse_user_input(user_input)\n",
    "        \n",
    "        result = {\n",
    "            'user_input': user_input,\n",
    "            'parsed_input': parsed_input,\n",
    "            'recommendations': recommendations,\n",
    "            'total_places': len(self.df)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"âœ… ìˆ˜ì •ëœ ì¶”ì²œ ì‹œìŠ¤í…œ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef561287-776f-4f82-9f5e-f2adba3a736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì‹¤ì œ csv íŒŒì¼ ë¡œë“œ ë° ê²€ì¦\n",
    "\n",
    "def load_and_validate_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"ì‹¤ì œ CSV íŒŒì¼ ë¡œë“œ ë° ê²€ì¦\"\"\"\n",
    "\n",
    "    # ì—…ë¡œë“œëœ CSV íŒŒì¼ ì½ê¸°\n",
    "    try: \n",
    "        #  íŒŒì¼ì´ ì—…ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  data/raw í´ë”ë¡œ ë³µì‚¬\n",
    "        if os.path.exists('gangwon_places_100.csv'):\n",
    "            # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìˆëŠ” íŒŒì¼ì„ data/raw í´ë”ë¡œ ë³µì‚¬\n",
    "            os.makedirs('data/raw', exist_ok=True)\n",
    "            import shutil\n",
    "            shutil.copy('gangwon_places_100.csv', 'data/raw/gangwon_places_100.csv')\n",
    "            print(\"CSV íŒŒì¼ì„ data/raw í´ë”ë¡œ ë³µì‚¬ ì™„ë£Œ\")\n",
    "\n",
    "        # CSV íŒŒì´ ë¡œë“œ\n",
    "        df = pd.read_csv('data/raw/gangwon_places_100.csv', encoding='utf-8')\n",
    "        print(f\"âœ… CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {df.shape}\")\n",
    "\n",
    "        # ì»¬ëŸ¼ ì •ë³´ ì¶œë ¥\n",
    "        print(f\"ì»¬ëŸ¼ ì •ë³´: {df.columns.tolist()}\")\n",
    "\n",
    "        # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦\n",
    "        required_columns = ['name', 'season', 'nature', 'vibe', 'target', 'short_description']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"âš ï¸  í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}\")\n",
    "            print(\"ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ê³  ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(\"âœ… ëª¨ë“  í•„ìˆ˜ ì»¬ëŸ¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "        # ë°ì´í„° íƒ€ì… ë° ê²°ì¸¡ì¹˜ ì •ë³´ ì¶œë ¥\n",
    "        print(f\"\\në°ì´í„° ì •ë³´: \")\n",
    "        print(f\"- ì´ í–‰ ìˆ˜: {len(df)}\")\n",
    "        print(f\"- ê²°ì¸¡ì¹˜ í˜„í™©: \")\n",
    "        for col in required_columns:\n",
    "            if col in df.columns: \n",
    "                missing_count = df[col].isnull().sum()\n",
    "                missing_pct = (missing_count / len(df)) * 100\n",
    "                print(f\" {col}: {missing_couint}ê°œ {missing_pct:.1f}%)\")\n",
    "\n",
    "\n",
    "        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "        print(f\"\\n ìƒ˜í”Œ ë°ì´í„° (ìƒìœ„ 3ê°œ): \")\n",
    "        for idx, row in df.head(3).iterrows():\n",
    "            print(f\"\\n{idx+1}. {row.get('name', 'N/A')}\")\n",
    "            print(f\"ê³„ì ˆ: {row.get('season', 'N/A')}\")\n",
    "            print(f\"ìì—° í™˜ê²½:  {row.get('nature', 'N/A')}\")\n",
    "            print(f\"ë¶„ìœ„ê¸°:  {row.get('vibe', 'N/A')}\")\n",
    "            print(f\"ëŒ€ìƒ:  {row.get('vibe', 'N/A')}\")\n",
    "            print(f\"ì„¤ëª…:  {row.get('short_description', 'N/A')[:50]}...\")\n",
    "\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"gangwon_places_100.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"íŒŒì¼ì„ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì—…ë¡œë“œí•˜ê±°ë‚˜ data/raw/ í´ë”ì— ì €ì¥í•´ì£¼ì„¸ìš”.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "        return None\n",
    "    # ì‹¤ì œ CSV íŒŒì¼ ë¡œë“œ\n",
    "    print(\"== ì‹¤ì œ CSV íŒŒì¼ ë¡œë“œ ===\")\n",
    "    df_loaded = load_and_validate_csv('data/raw/gangwon_places_100.csv')\n",
    "\n",
    "    if df_loaded is not None:\n",
    "        print(\"\\n ì‹¤ì œ ë°ì´í„° íŒŒì¼ ë¡œë“œ ì„±ê³µ\")\n",
    "    else:\n",
    "        print(\"\\n ë°ì´í„° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ - í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        #ì‹¤ì œ Jupyter í™˜ê²½ì—ì„œëŠ” ë‹¤ìŒ ì…€ ì‹¤í–‰ì„ ì¤‘ë‹¨í•˜ê±°ë‚˜ ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a52475d3-0a3c-43c4-8849-7588f03fb598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì‹¤ì œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬===\n",
      "ì›ë³¸ ë°ì´í„°: (100, 13)\n",
      "ì»¬ëŸ¼: ['name', 'season', 'nature', 'vibe', 'target', 'fee', 'parking', 'address', 'open_time', 'latitude', 'longitude', 'full_address', 'short_description']\n",
      "\n",
      " ì‹¤ì œ ë°ì´í„° ì •ë³´:\n",
      "ì´ ê´€ê´‘ì§€ ìˆ˜: 100\n",
      "ì „ì²´ ì»¬ëŸ¼ ìˆ˜: 13\n",
      "-season ì¹´í…Œê³ ë¦¬: 5ê°œ ì¢…ë¥˜\n",
      " ì˜ˆì‹œ: ['ì‚¬ê³„ì ˆ', 'ë´„', 'ì—¬ë¦„', 'ê°€ì„', 'ê²¨ìš¸']\n",
      "-nature ì¹´í…Œê³ ë¦¬: 19ê°œ ì¢…ë¥˜\n",
      " ì˜ˆì‹œ: ['ì‚°, í˜¸ìˆ˜', 'ì‚°, ìì—°, í˜¸ìˆ˜', 'ì‚°, ìì—°', 'ë°”ë‹¤, ì‚°, ìì—°', 'ë°”ë‹¤, ì‚°']\n",
      "-vibe ì¹´í…Œê³ ë¦¬: 40ê°œ ì¢…ë¥˜\n",
      " ì˜ˆì‹œ: ['ì•¡í‹°ë¹„í‹°, ì—­ì‚¬', 'ì‚°ì±…, ì•¡í‹°ë¹„í‹°, íë§', 'ì‚°ì±…', 'ì‚¬ì§„ëª…ì†Œ, ì‚°ì±…, ì—­ì‚¬', 'ì‚°ì±…, ì—­ì‚¬']\n",
      "-target ì¹´í…Œê³ ë¦¬: 7ê°œ ì¢…ë¥˜\n",
      " ì˜ˆì‹œ: ['ê°€ì¡±', 'ì¹œêµ¬', 'ì—°ì¸', 'ì—°ì¸, ì¹œêµ¬', 'ê°€ì¡±, ì¹œêµ¬']\n",
      "\n",
      " ì „ì²˜ë¦¬ ëœ ë°ì´í„°: (100, 16)\n",
      "\n",
      " ì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ (ìƒìœ„ 3ê°œ):\n",
      "\n",
      "1. ê°•ë¦‰ ëª¨ë˜ë‚´ í•œê³¼ë§ˆì„(ê°ˆê³¨í•œê³¼)\n",
      "   ê³„ì ˆ: ì‚¬ê³„ì ˆ\n",
      "   ìì—°í™˜ê²½ (ë¦¬ìŠ¤íŠ¸): ['ì‚°', 'í˜¸ìˆ˜']\n",
      "   ë¶„ìœ„ê¸° (ë¦¬ìŠ¤íŠ¸): ['ì•¡í‹°ë¹„í‹°', 'ì—­ì‚¬']\n",
      "   ëŒ€ìƒ (ë¦¬ìŠ¤íŠ¸): ['ê°€ì¡±']\n",
      "   ì„¤ëª…: ê°•ë¦‰ ëª¨ë˜ë‚´ í•œê³¼ë§ˆì„ê°ˆê³¨í•œê³¼ì€ëŠ” ì‚¬ê³„ì ˆì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì•¡í‹°ë¹„í‹° ë¶„ìœ„ê¸°...\n",
      "\n",
      "2. êµ­ë¦½ ì‚¼ë´‰ìì—°íœ´ì–‘ë¦¼\n",
      "   ê³„ì ˆ: ë´„\n",
      "   ìì—°í™˜ê²½ (ë¦¬ìŠ¤íŠ¸): ['ì‚°', 'ìì—°', 'í˜¸ìˆ˜']\n",
      "   ë¶„ìœ„ê¸° (ë¦¬ìŠ¤íŠ¸): ['ì‚°ì±…', 'ì•¡í‹°ë¹„í‹°', 'íë§']\n",
      "   ëŒ€ìƒ (ë¦¬ìŠ¤íŠ¸): ['ê°€ì¡±']\n",
      "   ì„¤ëª…: êµ­ë¦½ ì‚¼ë´‰ìì—°íœ´ì–‘ë¦¼ì€ëŠ” ë´„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì‚°ì±… ë¶„ìœ„ê¸°ë¡œ ê°€ì¡±ì—ê²Œ ì¶”ì²œ...\n",
      "\n",
      "3. ì„¤ì•…ì‚°êµ­ë¦½ê³µì›(ë‚´ì„¤ì•…)\n",
      "   ê³„ì ˆ: ì—¬ë¦„\n",
      "   ìì—°í™˜ê²½ (ë¦¬ìŠ¤íŠ¸): ['ì‚°', 'ìì—°']\n",
      "   ë¶„ìœ„ê¸° (ë¦¬ìŠ¤íŠ¸): ['ì‚°ì±…']\n",
      "   ëŒ€ìƒ (ë¦¬ìŠ¤íŠ¸): []\n",
      "   ì„¤ëª…: ì„¤ì•…ì‚°êµ­ë¦½ê³µì›ë‚´ì„¤ì•…ì€ëŠ” ì—¬ë¦„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì‚°ì±… ë¶„ìœ„ê¸°ë¡œ ëª¨ë‘ì—ê²Œ ì¶”...\n",
      "ì¸ì½”ë” í•™ìŠµ ì™„ë£Œ\n",
      "   - ê³„ì ˆ ì¹´í…Œê³ ë¦¬: ['ê°€ì„', 'ê²¨ìš¸', 'ë´„', 'ì‚¬ê³„ì ˆ', 'ì—¬ë¦„']\n",
      "   - ìì—°í™˜ê²½ ì¹´í…Œê³ ë¦¬: 5ê°œ\n",
      "   - ë¶„ìœ„ê¸° ì¹´í…Œê³ ë¦¬: 8ê°œ\n",
      "   - ëŒ€ìƒ ì¹´í…Œê³ ë¦¬: 3ê°œ\n",
      "\n",
      " ì¸ì½”ë”© ê²°ê³¼:\n"
     ]
    }
   ],
   "source": [
    "## ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "\n",
    "# ì¶”ì²œ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "recommender = GangwonPlaceRecommender()\n",
    "\n",
    "# ì‹¤ì œ ë°ì´í„° ë¡œë“œ(ì—…ë¡œë“œëœ CSV íŒŒì¼ ì‚¬ìš©)\n",
    "print(\"=== ì‹¤ì œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬===\")\n",
    "\n",
    "# ì—…ë¡œë“œëœ íŒŒì¼ì„ data/rawë¡œ ë³µì‚¬ (íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìˆëŠ” ê²½ìš°)\n",
    "if os.path.exists('gangwon_places_100.csv'):\n",
    "    import shutil\n",
    "    shutil.copy('gangwon_places_100.csv', 'data/raw/gangwon_places_100.csv')\n",
    "    print(\"âœ… ì—…ë¡œë“œëœ CSV íŒŒì¼ì„ data/rawë¡œ ë³µì‚¬ ì™„ë£Œ\")\n",
    "\n",
    "# CSV íŒŒì¼ ë¡œë“œ\n",
    "df = pd.read_csv('data/raw/gangwon_places_100.csv', encoding='utf-8')\n",
    "print(f\"ì›ë³¸ ë°ì´í„°: {df.shape}\")\n",
    "print(f\"ì»¬ëŸ¼: {df.columns.tolist()}\")\n",
    "\n",
    "# ì¶”ê°€ ì»¬ëŸ¼ ì •ë³´ ì¶œë ¥\n",
    "print(f\"\\n ì‹¤ì œ ë°ì´í„° ì •ë³´:\")\n",
    "print(f\"ì´ ê´€ê´‘ì§€ ìˆ˜: {len(df)}\")\n",
    "print(f\"ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}\")\n",
    "\n",
    "# ê° ì¹´í…Œê³ ë¦¬ë³„ ê³ ìœ ê°’ í™•ì¸\n",
    "categorical_columns = ['season', 'nature', 'vibe', 'target']\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        unique_values = df[col].dropna().unique()\n",
    "        print(f\"-{col} ì¹´í…Œê³ ë¦¬: {len(unique_values)}ê°œ ì¢…ë¥˜\")\n",
    "        print(f\" ì˜ˆì‹œ: {list(unique_values)[:5]}\")\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬\n",
    "processed_df = recommender.preprocessor.preprocess_data(df)\n",
    "print(f\"\\n ì „ì²˜ë¦¬ ëœ ë°ì´í„°: {processed_df.shape}\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸\n",
    "print(f\"\\n ì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ (ìƒìœ„ 3ê°œ):\")\n",
    "for idx,row in processed_df.head(3).iterrows():\n",
    "    print(f\"\\n{idx+1}. {row['name']}\")\n",
    "    print(f\"   ê³„ì ˆ: {row['season']}\")\n",
    "    print(f\"   ìì—°í™˜ê²½ (ë¦¬ìŠ¤íŠ¸): {row['nature_list']}\")\n",
    "    print(f\"   ë¶„ìœ„ê¸° (ë¦¬ìŠ¤íŠ¸): {row['vibe_list']}\")\n",
    "    print(f\"   ëŒ€ìƒ (ë¦¬ìŠ¤íŠ¸): {row['target_list']}\")\n",
    "    print(f\"   ì„¤ëª…: {row['short_description'][:50]}...\")\n",
    "\n",
    "# ì¸ì½”ë” í•™ìŠµ\n",
    "recommender.preprocessor.fit_encoders(processed_df)\n",
    "\n",
    "# ë¼ë²¨ ì¸ì½”ë”©\n",
    "encoded_labels = recommender.preprocessor.encode_labels(processed_df)\n",
    "\n",
    "# ì¸ì½”ë”© ê²°ê³¼ í™•ì¸\n",
    "print(f\"\\n ì¸ì½”ë”© ê²°ê³¼:\") \n",
    "\n",
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "processed_df.to_csv('data/processed/gangwon_places_100_processed.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# ì¶”ì²œ ì‹œìŠ¤í…œì— ë°ì´í„° ì €ì¥\n",
    "recommender.df = processed_df\n",
    "recommender.place_names = processed_df['name'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52ae69c-d465-4196-acac-111ee6df6c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: snunlp/KR-SBERT-V40K-klueNLI-augSTS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SBERT ì„ë² ë”© ìƒì„± ë° ì°¨ì› ì¶•ì†Œ\n",
      "SBERT ëª¨ë¸ ë¡œë“œ ì¤‘: snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
      "SBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "ì„ë² ë”© ìƒì„± ì¤‘... (ì´ 100ê°œ í…ìŠ¤íŠ¸)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.40s/it]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                               | 1/4 [00:01<00:04,  1.40s/it]\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.03s/it]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 2/4 [00:02<00:02,  1.19s/it]\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.18s/it]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 3/4 [00:03<00:01,  1.19s/it]\n",
      "Batches:   0%|                                                                                   | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.07it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© ìƒì„± ì™„ë£Œ: (100, 768)\n",
      "ğŸ“Š ì„ë² ë”© í˜•íƒœ: (100, 768)\n",
      "ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.29 MB\n",
      "âœ… 768ì°¨ì› ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì™„ë£Œ\n",
      "   íŒŒì¼ ì €ì¥\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## SBERT ì„ë² ë”© ìƒì„±(768ì°¨ì› ìœ ì§€)\n",
    "print(\"\\n SBERT ì„ë² ë”© ìƒì„± ë° ì°¨ì› ì¶•ì†Œ\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "texts = processed_df['short_description'].tolist()\n",
    "\n",
    "# SBERT ì„ë² ë”© ìƒì„±\n",
    "embeddings = recommender.embedding_generator.generate_embeddings(texts)\n",
    "\n",
    "print(f\"ğŸ“Š ì„ë² ë”© í˜•íƒœ: {embeddings.shape}\")\n",
    "print(f\"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "\"\"\"\n",
    "# ì°¨ì› ì¶•ì†Œ ëª¨ë¸ í•™ìŠµ\n",
    "recommender.embedding_generator.fit_dimension_reducer(\n",
    "    embeddings,\n",
    "    method = recommender.config['model']['dimensionality_reduction'],\n",
    "    target_dim = recommender.config['model']['reduced_dim']\n",
    ")\n",
    "# ì°¨ì› ì¶•ì†Œ ì ìš©\n",
    "reduced_embeddings = recommender.embedding_generator.reduce_dimensions(embeddings)\n",
    "\"\"\"\n",
    "# ì°¨ì› ì¶•ì†Œ ì—†ì´ ì›ë³¸ 768ì°¨ì› ì‚¬ìš©\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "np.save('data/embeddings/place_embeddings_full768.npy', embeddings)\n",
    "\n",
    "# # ì„ë² ë”© ì €ì¥ \n",
    "# np.save('data/embeddings/place_embeddings_pca128.npy', reduced_embeddings)\n",
    "\n",
    "# ì¶”ì²œ ì‹œìŠ¤í…œì— ì„ë² ë”© ì €ì¥\n",
    "recommender.place_embeddings = embeddings\n",
    "\n",
    "\n",
    "print(\"âœ… 768ì°¨ì› ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì™„ë£Œ\")\n",
    "print(f\"   íŒŒì¼ ì €ì¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d72f8fd-1306-4750-9edc-c8373bcfb6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " === XGBoost ëª¨ë¸ í•™ìŠµ===\n",
      "XGBoost ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n",
      "\n",
      "season ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘...\n",
      "ëª¨ë“  XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n",
      "\n",
      "nature ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘...\n",
      "ëª¨ë“  XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n",
      "\n",
      "vibe ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘...\n",
      "ëª¨ë“  XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n",
      "\n",
      "target ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘...\n",
      "ëª¨ë“  XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n",
      "\n",
      "=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€===\n",
      "\n",
      "[season] ì„±ëŠ¥ í‰ê°€: \n",
      "Accuracy: 1.0000\n",
      "F1-Score: 1.0000\n",
      "\n",
      "[nature] ì„±ëŠ¥ í‰ê°€: \n",
      "Accuracy: 1.0000\n",
      "F1-Score (Micro): 1.0000\n",
      "F1-Score (Macro): 1.0000\n",
      "\n",
      "[vibe] ì„±ëŠ¥ í‰ê°€: \n",
      "Accuracy: 0.9900\n",
      "F1-Score (Micro): 0.9977\n",
      "F1-Score (Macro): 0.8750\n",
      "\n",
      "[target] ì„±ëŠ¥ í‰ê°€: \n",
      "Accuracy: 1.0000\n",
      "F1-Score (Micro): 1.0000\n",
      "F1-Score (Macro): 1.0000\n",
      "\n",
      "=== XGBoost ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ===\n"
     ]
    }
   ],
   "source": [
    "## XGBoost ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
    "print(\"\\n === XGBoost ëª¨ë¸ í•™ìŠµ===\")\n",
    "\n",
    "# íŠ¹ì„±ê³¼ ë¼ë²¨ ì¤€ë¹„\n",
    "features = embeddings\n",
    "labels = encoded_labels\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "recommender.xgb_trainer.train_models(features, labels)\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€ \n",
    "recommender.xgb_trainer.evaluate_models(features, labels)\n",
    "\n",
    "print(\"\\n=== XGBoost ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "125e37bf-50e5-4faf-937b-0394617108ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ëª¨ë¸ ë° ì¸ì½”ë” ì €ì¥\n",
      "ì¸ì½”ë” ì €ì¥ ì™„ë£Œ: models/encoders\n",
      "season ëª¨ë¸ ì €ì¥: models/xgboost/season_model.joblib\n",
      "nature ëª¨ë¸ ì €ì¥: models/xgboost/nature_model.joblib\n",
      "vibe ëª¨ë¸ ì €ì¥: models/xgboost/vibe_model.joblib\n",
      "target ëª¨ë¸ ì €ì¥: models/xgboost/target_model.joblib\n",
      " ëª¨ë“  ëª¨ë¸ ë° ì¸ì½”ë” ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "## ëª¨ë¸ ë° ì¸ì½”ë” ì €ì¥\n",
    "print(\"\\n ëª¨ë¸ ë° ì¸ì½”ë” ì €ì¥\")\n",
    "\n",
    "# í´ë” ìƒì„±\n",
    "os.makedirs('models/xgboost', exist_ok=True)\n",
    "os.makedirs('models/encoders', exist_ok=True)\n",
    "\n",
    "# ì¸ì½”ë” ì €ì¥\n",
    "recommender.preprocessor.save_encoders('models/encoders')\n",
    "\n",
    "# XGBoost ëª¨ë¸ ì €ì¥\n",
    "recommender.xgb_trainer.save_models('models')\n",
    "\n",
    "print(\" ëª¨ë“  ëª¨ë¸ ë° ì¸ì½”ë” ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0eba0b-7140-4112-af28-ae7eb2adb4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===\n",
      "í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: íƒœê·¸ ê¸°ë°˜ ì…ë ¥\n",
      "ì…ë ¥: {'season': 'ì—¬ë¦„', 'nature': ['ë°”ë‹¤', 'ìì—°'], 'vibe': ['íœ´ì‹', 'ê°ì„±'], 'target': ['ì—°ì¸']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " íŒŒì‹±ëœ ì…ë ¥: {'season': 'ì—¬ë¦„', 'nature': ['ë°”ë‹¤', 'ìì—°'], 'vibe': ['íœ´ì‹', 'ê°ì„±'], 'target': ['ì—°ì¸']}\n",
      "ì´ 100ê°œ ê´€ê´‘ì§€ ì¤‘ ìƒìœ„ 5ê°œ ì¶”ì²œ:\n",
      "\n",
      "1. ì˜ì§„í•´ë³€\n",
      "   ì„¤ëª…: ì˜ì§„í•´ë³€ì€ëŠ” ì—¬ë¦„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ë°”ë‹¤ ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ê°ì„± ë¶„ìœ„ê¸°ë¡œ ê°€ì¡± ì—°ì¸ ì¹œêµ¬ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ì—¬ë¦„ | ['ë°”ë‹¤'] | ['ê°ì„±', 'ì‚¬ì§„ëª…ì†Œ', 'ì¡°ìš©í•œ'] | ['ê°€ì¡±', 'ì—°ì¸', 'ì¹œêµ¬']\n",
      "   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ=0.7400, ìœ ì‚¬ë„=0.7334, íƒœê·¸=0.7500\n",
      "\n",
      "2. ìš©ì†Œí­í¬(ì—°í•˜ê³„ê³¡)\n",
      "   ì„¤ëª…: ìš©ì†Œí­í¬ì—°í•˜ê³„ê³¡ì€ëŠ” ì—¬ë¦„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ê°ì„± ë¶„ìœ„ê¸°ë¡œ ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ì—¬ë¦„ | ['ì‚°', 'ìì—°'] | ['ê°ì„±', 'ì‚¬ì§„ëª…ì†Œ', 'ì‚°ì±…', 'ì—­ì‚¬', 'íë§'] | ['ì—°ì¸']\n",
      "   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ=0.7277, ìœ ì‚¬ë„=0.7129, íƒœê·¸=0.7500\n",
      "\n",
      "3. ì‚¬ê·¼ì§„ í•´ì¤‘ê³µì› ì „ë§ëŒ€\n",
      "   ì„¤ëª…: ì‚¬ê·¼ì§„ í•´ì¤‘ê³µì› ì „ë§ëŒ€ì€ëŠ” ì—¬ë¦„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ë°”ë‹¤ ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ê°ì„± ë¶„ìœ„ê¸°ë¡œ ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ì—¬ë¦„ | ['ë°”ë‹¤', 'í˜¸ìˆ˜'] | ['ê°ì„±', 'ì‚¬ì§„ëª…ì†Œ', 'ì‚°ì±…'] | ['ì—°ì¸']\n",
      "   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ=0.7204, ìœ ì‚¬ë„=0.7007, íƒœê·¸=0.7500\n",
      "\n",
      "4. ìˆœë‹´ê³„ê³¡\n",
      "   ì„¤ëª…: ìˆœë‹´ê³„ê³¡ì€ëŠ” ì—¬ë¦„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ê°ì„± ë¶„ìœ„ê¸°ë¡œ ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ì—¬ë¦„ | ['ì‚°', 'ìì—°', 'í˜¸ìˆ˜'] | ['ê°ì„±', 'ì‚°ì±…', 'íë§'] | ['ì—°ì¸']\n",
      "   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ=0.7053, ìœ ì‚¬ë„=0.6755, íƒœê·¸=0.7500\n",
      "\n",
      "5. ì²­í‰ì‚¬ê³„ê³¡\n",
      "   ì„¤ëª…: ì²­í‰ì‚¬ê³„ê³¡ì€ëŠ” ì—¬ë¦„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° íë§ ë¶„ìœ„ê¸°ë¡œ ì—°ì¸ ì¹œêµ¬ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ì—¬ë¦„ | ['ì‚°', 'ìì—°', 'í˜¸ìˆ˜'] | ['íë§'] | ['ì—°ì¸', 'ì¹œêµ¬']\n",
      "   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ=0.6693, ìœ ì‚¬ë„=0.6989, íƒœê·¸=0.6250\n",
      "\n",
      "==================================================\n",
      "í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ììœ  ë¬¸ì¥ ì…ë ¥\n",
      "ì…ë ¥ : {'fress_text': 'ê²¨ìš¸ì— ê°€ì¡±ê³¼ í•¨ê»˜ ìŠ¤í‚¤ë¥¼ íƒ€ê³  ì‹¶ì–´ìš”'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ë°€ë¸Œë¦¿ì§€\n",
      "   ì„¤ëª…: ë°€ë¸Œë¦¿ì§€ì€ëŠ” ì‚¬ê³„ì ˆì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì‚°ì±… ë¶„ìœ„ê¸°ë¡œ ê°€ì¡± ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ì‚¬ê³„ì ˆ | ['ì‚°', 'ìì—°'] | ['ì‚°ì±…', 'ì•¡í‹°ë¹„í‹°', 'íë§'] | ['ê°€ì¡±', 'ì—°ì¸']\n",
      "   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ=0.1073, ìœ ì‚¬ë„=0.1788, íƒœê·¸=0.0000\n",
      "\n",
      "2. ê²½í¬í”Œë¼ì›Œê°€ë“ \n",
      "   ì„¤ëª…: ê²½í¬í”Œë¼ì›Œê°€ë“ ì€ëŠ” ë´„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ìì—° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì•¡í‹°ë¹„í‹° ë¶„ìœ„ê¸°ë¡œ ê°€ì¡±ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ë´„ | ['ìì—°', 'í˜¸ìˆ˜'] | ['ì•¡í‹°ë¹„í‹°', 'íë§'] | ['ê°€ì¡±']\n",
      "   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ=0.0948, ìœ ì‚¬ë„=0.1580, íƒœê·¸=0.0000\n",
      "\n",
      "3. ì‚¼ì–‘ë¼ìš´ë“œí\n",
      "   ì„¤ëª…: ì‚¼ì–‘ë¼ìš´ë“œíì€ëŠ” ì‚¬ê³„ì ˆì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì•¡í‹°ë¹„í‹° ë¶„ìœ„ê¸°ë¡œ ê°€ì¡± ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ì‚¬ê³„ì ˆ | ['ì‚°', 'ìì—°', 'í˜¸ìˆ˜'] | ['ì•¡í‹°ë¹„í‹°', 'íë§'] | ['ê°€ì¡±', 'ì—°ì¸']\n",
      "   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ=0.0894, ìœ ì‚¬ë„=0.1490, íƒœê·¸=0.0000\n",
      "\n",
      "4. ë¥´ê¼¬ë”°ì¥¬\n",
      "   ì„¤ëª…: ë¥´ê¼¬ë”°ì¥¬ì€ëŠ” ì‚¬ê³„ì ˆì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ê°ì„± ë¶„ìœ„ê¸°ë¡œ ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ì‚¬ê³„ì ˆ | ['ì‚°', 'ìì—°', 'í˜¸ìˆ˜'] | ['ê°ì„±', 'ì‚¬ì§„ëª…ì†Œ', 'ì—­ì‚¬', 'íë§'] | ['ì—°ì¸']\n",
      "   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ=0.0876, ìœ ì‚¬ë„=0.1459, íƒœê·¸=0.0000\n",
      "\n",
      "5. í•˜ëŠ¬ë¼ë²¤ë”íŒœ\n",
      "   ì„¤ëª…: í•˜ëŠ¬ë¼ë²¤ë”íŒœì€ëŠ” ê²¨ìš¸ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ëª¨ë‘ ë¶„ìœ„ê¸°ë¡œ ëª¨ë‘ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ê²¨ìš¸ | ['ì‚°', 'ìì—°', 'í˜¸ìˆ˜'] | [] | []\n",
      "   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ=0.0856, ìœ ì‚¬ë„=0.1426, íƒœê·¸=0.0000\n",
      "\n",
      " ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n=== ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: íƒœê·¸ ê¸°ë°˜ ì…ë ¥\n",
    "test_input_1 = {\n",
    "    \"season\": \"ì—¬ë¦„\",\n",
    "    \"nature\": [\"ë°”ë‹¤\", \"ìì—°\"],\n",
    "    \"vibe\": [\"íœ´ì‹\", \"ê°ì„±\"],\n",
    "    \"target\": [\"ì—°ì¸\"]\n",
    "}\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: íƒœê·¸ ê¸°ë°˜ ì…ë ¥\")\n",
    "print(f\"ì…ë ¥: {test_input_1}\")\n",
    "\n",
    "result_1 = recommender.recommend_places(test_input_1, top_k=5)\n",
    "\n",
    "print(f\"\\n íŒŒì‹±ëœ ì…ë ¥: {result_1['parsed_input']}\")\n",
    "print(f\"ì´ {result_1['total_places']}ê°œ ê´€ê´‘ì§€ ì¤‘ ìƒìœ„ 5ê°œ ì¶”ì²œ:\")\n",
    "\n",
    "for i, place in enumerate(result_1['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ì„¤ëª…: {place['description']}\")\n",
    "    print(f\"   íƒœê·¸: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ={place['hybrid_score']:.4f}, ìœ ì‚¬ë„={place['similarity_score']:.4f}, íƒœê·¸={place['tag_score']:.4f}\")\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ììœ  ë¬¸ì¥ ì…ë ¥\n",
    "test_input_2 = {\n",
    "    \"fress_text\": \"ê²¨ìš¸ì— ê°€ì¡±ê³¼ í•¨ê»˜ ìŠ¤í‚¤ë¥¼ íƒ€ê³  ì‹¶ì–´ìš”\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" *50)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ììœ  ë¬¸ì¥ ì…ë ¥\")\n",
    "print(f\"ì…ë ¥ : {test_input_2}\")\n",
    "\n",
    "result_2 = recommender.recommend_places(test_input_2, top_k=5)\n",
    "\n",
    "for i, place in enumerate(result_2['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ì„¤ëª…: {place['description']}\")\n",
    "    print(f\"   íƒœê·¸: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ={place['hybrid_score']:.4f}, ìœ ì‚¬ë„={place['similarity_score']:.4f}, íƒœê·¸={place['tag_score']:.4f}\")\n",
    "\n",
    "print(\"\\n ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4911af44-7e05-479f-9303-c1566010e4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: snunlp/KR-SBERT-V40K-klueNLI-augSTS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " === ëª¨ë¸ ë¡œë“œ ë° ì¬ì‚¬ìš© í…ŒìŠ¤íŠ¸===\n",
      "ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ: models/encoders\n",
      "season ëª¨ë¸ ë¡œë“œ: models/xgboost/season_model.joblib\n",
      "nature ëª¨ë¸ ë¡œë“œ: models/xgboost/nature_model.joblib\n",
      "vibe ëª¨ë¸ ë¡œë“œ: models/xgboost/vibe_model.joblib\n",
      "target ëª¨ë¸ ë¡œë“œ: models/xgboost/target_model.joblib\n",
      "í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: ìˆ˜ì •ëœ ëª¨ë¸ë¡œ ì¶”ì²œ\n",
      "ì…ë ¥: {'free_text': 'ë´„ì— í˜¼ì ì¡°ìš©í•œ ì‚°ì—ì„œ íë§í•˜ê³  ì‹¶ì–´ìš”'}\n",
      "SBERT ëª¨ë¸ ë¡œë“œ ì¤‘: snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
      "SBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "íŒŒì‹±ëœ ì…ë ¥: {'season': 'ë´„', 'nature': ['ì‚°'], 'vibe': ['íœ´ì‹'], 'target': ['í˜¼ì']}\n",
      "ìƒìœ„ 3ê°œ ì¶”ì²œ:\n",
      "\n",
      "1. êµ­ë¦½ ì‚¼ë´‰ìì—°íœ´ì–‘ë¦¼\n",
      "   ì„¤ëª…: êµ­ë¦½ ì‚¼ë´‰ìì—°íœ´ì–‘ë¦¼ì€ëŠ” ë´„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì‚°ì±… ë¶„ìœ„ê¸°ë¡œ ê°€ì¡±ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ë´„ | ['ì‚°', 'ìì—°', 'í˜¸ìˆ˜'] | ['ì‚°ì±…', 'ì•¡í‹°ë¹„í‹°', 'íë§'] | ['ê°€ì¡±']\n",
      "   í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: 0.5915\n",
      "\n",
      "2. ê°•ë¦‰ ì†”í–¥ìˆ˜ëª©ì›\n",
      "   ì„¤ëª…: ê°•ë¦‰ ì†”í–¥ìˆ˜ëª©ì›ì€ëŠ” ë´„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì‚°ì±… ë¶„ìœ„ê¸°ë¡œ ê°€ì¡±ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ë´„ | ['ì‚°', 'ìì—°', 'í˜¸ìˆ˜'] | ['ì‚°ì±…', 'íë§'] | ['ê°€ì¡±']\n",
      "   í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: 0.5867\n",
      "\n",
      "3. ë™ê°•(ì˜ì›”)\n",
      "   ì„¤ëª…: ë™ê°•ì˜ì›”ì€ëŠ” ë´„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì‚¬ì§„ëª…ì†Œ ë¶„ìœ„ê¸°ë¡œ ê°€ì¡±ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   íƒœê·¸: ë´„ | ['ì‚°', 'í˜¸ìˆ˜'] | ['ì‚¬ì§„ëª…ì†Œ', 'ì•¡í‹°ë¹„í‹°', 'ì—­ì‚¬'] | ['ê°€ì¡±']\n",
      "   í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: 0.5768\n",
      "\n",
      "âœ… ìˆ˜ì •ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## ëª¨ë¸ ë¡œë“œ ë° ì¬ì‚¬ìš© í…ŒìŠ¤íŠ¸\n",
    "print(f\"\\n === ëª¨ë¸ ë¡œë“œ ë° ì¬ì‚¬ìš© í…ŒìŠ¤íŠ¸===\")\n",
    "# ìƒˆë¡œìš´ ì¶”ì²œ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìˆ˜ì •ëœ í´ë˜ìŠ¤ ì‚¬ìš©)\n",
    "new_recommender = GangwonPlaceRecommender()\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "new_recommender.df = pd.read_csv('data/processed/gangwon_places_100_processed.csv')\n",
    "new_recommender.df = new_recommender.df.reset_index(drop=True)  # ì¸ë±ìŠ¤ ë¦¬ì…‹\n",
    "\n",
    "# ì„ë² ë”© ë¡œë“œ\n",
    "new_recommender.place_embeddings = np.load('data/embeddings/place_embeddings_full768.npy')\n",
    "\n",
    "# ì¸ì½”ë” ë¡œë“œ\n",
    "new_recommender.preprocessor.load_encoders('models/encoders')\n",
    "\n",
    "# XGBoost ëª¨ë¸ ë¡œë“œ\n",
    "new_recommender.xgb_trainer.load_models('models')\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "test_input_3 = {\n",
    "    \"free_text\": \"ë´„ì— í˜¼ì ì¡°ìš©í•œ ì‚°ì—ì„œ íë§í•˜ê³  ì‹¶ì–´ìš”\"\n",
    "}\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: ìˆ˜ì •ëœ ëª¨ë¸ë¡œ ì¶”ì²œ\")\n",
    "print(f\"ì…ë ¥: {test_input_3}\")\n",
    "\n",
    "result_3 = new_recommender.recommend_places(test_input_3, top_k=3)\n",
    "\n",
    "print(f\"\\níŒŒì‹±ëœ ì…ë ¥: {result_3['parsed_input']}\")\n",
    "print(f\"ìƒìœ„ 3ê°œ ì¶”ì²œ:\")\n",
    "\n",
    "for i, place in enumerate(result_3['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ì„¤ëª…: {place['description']}\")\n",
    "    print(f\"   íƒœê·¸: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {place['hybrid_score']:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… ìˆ˜ì •ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e46069c-0d8d-4e6f-89ed-786967358f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ===\n",
      "ì…ë ¥: {'free_text': 'ë´„ì— í˜¼ì ì¡°ìš©í•œ ì‚°ì—ì„œ íë§í•˜ê³  ì‹¶ì–´ìš”'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "íŒŒì‹±ëœ ì…ë ¥: {'season': 'ë´„', 'nature': ['ì‚°'], 'vibe': ['íœ´ì‹'], 'target': ['í˜¼ì']}\n",
      "ìƒìœ„ 3ê°œ ì¶”ì²œ:\n",
      "\n",
      "1. í–‡ì‚´ë§ˆì„ì²´í—˜ê´€\n",
      "   ì„¤ëª…: í–‡ì‚´ë§ˆì„ì²´í—˜ê´€ì€ëŠ” ì‚¬ê³„ì ˆì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì‚°ì±… ë¶„ìœ„ê¸°ë¡œ ê°€ì¡±ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   ìœ ì‚¬ë„ ì ìˆ˜: 0.6215\n",
      "\n",
      "2. êµ­ë¦½ ì‚¼ë´‰ìì—°íœ´ì–‘ë¦¼\n",
      "   ì„¤ëª…: êµ­ë¦½ ì‚¼ë´‰ìì—°íœ´ì–‘ë¦¼ì€ëŠ” ë´„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì‚°ì±… ë¶„ìœ„ê¸°ë¡œ ê°€ì¡±ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   ìœ ì‚¬ë„ ì ìˆ˜: 0.6191\n",
      "\n",
      "3. ì„¤ì•…ì‚°ì±…\n",
      "   ì„¤ëª…: ì„¤ì•…ì‚°ì±…ì€ëŠ” ê²¨ìš¸ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ê°ì„± ë¶„ìœ„ê¸°ë¡œ ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤\n",
      "   ìœ ì‚¬ë„ ì ìˆ˜: 0.6152\n",
      "\n",
      "âœ… ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš© ì¶”ì²œ í•¨ìˆ˜\n",
    "def simple_recommend_test(recommender, user_input, top_k=3):\n",
    "    \"\"\"ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš© ì¶”ì²œ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # íŒŒì‹±ëœ ì…ë ¥\n",
    "    parsed_input = recommender.parse_user_input(user_input)\n",
    "    \n",
    "    # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    if 'free_text' in user_input:\n",
    "        query_text = user_input['free_text']\n",
    "    else:\n",
    "        query_parts = []\n",
    "        if parsed_input['season']:\n",
    "            query_parts.append(f\"{parsed_input['season']}ì—\")\n",
    "        if parsed_input['nature']:\n",
    "            query_parts.append(f\"{', '.join(parsed_input['nature'])}ì—ì„œ\")\n",
    "        if parsed_input['vibe']:\n",
    "            query_parts.append(f\"{', '.join(parsed_input['vibe'])} ì—¬í–‰\")\n",
    "        query_text = ' '.join(query_parts)\n",
    "    \n",
    "    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
    "    if recommender.embedding_generator.model is None:\n",
    "        recommender.embedding_generator.load_model()\n",
    "    \n",
    "    query_embedding = recommender.embedding_generator.model.encode([query_text])\n",
    "    \n",
    "    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarity_scores = cosine_similarity(query_embedding, recommender.place_embeddings)[0]\n",
    "    \n",
    "    # ìƒìœ„ ì¶”ì²œì§€ ì„ íƒ\n",
    "    top_indices = np.argsort(similarity_scores)[::-1][:top_k]\n",
    "    \n",
    "    # ê²°ê³¼ êµ¬ì„±\n",
    "    recommendations = []\n",
    "    for idx in top_indices:\n",
    "        place_info = {\n",
    "            'name': recommender.df.iloc[idx]['name'],\n",
    "            'description': recommender.df.iloc[idx]['short_description'],\n",
    "            'similarity_score': float(similarity_scores[idx])\n",
    "        }\n",
    "        recommendations.append(place_info)\n",
    "    \n",
    "    return {\n",
    "        'parsed_input': parsed_input,\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "test_input_3 = {\n",
    "    \"free_text\": \"ë´„ì— í˜¼ì ì¡°ìš©í•œ ì‚°ì—ì„œ íë§í•˜ê³  ì‹¶ì–´ìš”\"\n",
    "}\n",
    "\n",
    "print(\"=== ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ===\")\n",
    "print(f\"ì…ë ¥: {test_input_3}\")\n",
    "\n",
    "result_3 = simple_recommend_test(new_recommender, test_input_3, top_k=3)\n",
    "\n",
    "print(f\"\\níŒŒì‹±ëœ ì…ë ¥: {result_3['parsed_input']}\")\n",
    "print(f\"ìƒìœ„ 3ê°œ ì¶”ì²œ:\")\n",
    "\n",
    "for i, place in enumerate(result_3['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ì„¤ëª…: {place['description']}\")\n",
    "    print(f\"   ìœ ì‚¬ë„ ì ìˆ˜: {place['similarity_score']:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b47e897c-9917-4559-aa67-ec8c98367711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flask API ì—°ë™ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "## Flask API ì—°ë™ì„ ìœ„í•œ JSON ë³€í™˜ í•¨ìˆ˜\n",
    "\n",
    "def create_api_response(recommendation_result: Dict) -> Dict:\n",
    "    \"\"\"Flask API ì‘ë‹µì„ ìœ„í•œ JSON í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "\n",
    "    api_response = {\n",
    "        \n",
    "        'status': 'success',\n",
    "        'data' : {\n",
    "        'user_input': recommendation_result['user_input'],\n",
    "        'parsed_input': recommendation_result['parsed_input'],\n",
    "        'total_places': recommendation_result['total_places'],\n",
    "        'recommendations':[]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for place in recommendation_result['recommendations']:\n",
    "        place_data = {\n",
    "            'name': place['name'],\n",
    "            'description': place['description'],\n",
    "            'tags': {\n",
    "                'season': place['season'],\n",
    "                'nature': place['nature'],\n",
    "                'vibe': place['vibe'],\n",
    "                'target': place['target']\n",
    "            },\n",
    "            'scores' :{\n",
    "                'hybrid': round(place['hybrid_score'], 4),\n",
    "                'similarity': round(place['similarity_score'], 4),\n",
    "                'tag_match': round(place['tag_score'], 4)\n",
    "            }\n",
    "        }\n",
    "        api_response['data']['recommendations'].append(place_data)\n",
    "\n",
    "        return api_response\n",
    "\n",
    "print(\"Flask API ì—°ë™ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "581073d5-364b-4cd7-839b-02231836a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== API í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON ë¬¸ìì—´ ì…ë ¥ í…ŒìŠ¤íŠ¸:\n",
      "Status: success\n",
      "ì¶”ì²œ ê²°ê³¼: 1ê°œ\n",
      "  1. ì‘ì€í›„ì§„í•´ìˆ˜ìš•ì¥ (ì ìˆ˜: 0.4974)\n",
      "\n",
      " API í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## ì‚¬ìš©ì ì •ì˜ ì¶”ì²œ í•¨ìˆ˜(Flask APIìš©)\n",
    "\n",
    "def recommend_places_api(user_input: Union[Dict, str], top_k: int = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Flask APIì—ì„œ ì‚¬ìš©í•  ì¶”ì²œ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        user_input: ì‚¬ìš©ì ì…ë ¥ (Dict ë˜ëŠ” JSON ë¬¸ìì—´)\n",
    "        top_k: ì¶”ì²œí•  ê´€ê´‘ì§€ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        API ì‘ë‹µ í˜•íƒœì˜ Dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ë¬¸ìì—´ì¸ ê²½ìš° JSON íŒŒì‹±\n",
    "        if isinstance(user_input, str):\n",
    "            user_input = json.loads(user_input)\n",
    "\n",
    "        # ì…ë ¥ ê²€ì¦\n",
    "        if not isinstance(user_input, dict):\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': 'ì˜ëª»ëœ ì…ë ¥ ë°©ì‹ì…ë‹ˆë‹¤.',\n",
    "                'data': None\n",
    "            }\n",
    "        # ì¶”ì²œ ì‹¤í–‰\n",
    "        result = recommender.recommend_places(user_input, top_k= top_k)\n",
    "\n",
    "        # API ì‘ë‹µ ìƒì„±\n",
    "        api_response = create_api_response(result)\n",
    "\n",
    "        return api_response\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'message': f'ì¶”ì²œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',\n",
    "            'data': None\n",
    "        }\n",
    "# API í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n=== API í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===\")\n",
    "\n",
    "# JSON ë¬¸ìì—´ ì…ë ¥ í…ŒìŠ¤íŠ¸\n",
    "json_input = '{\"free_text\": \"ì—¬ë¦„ì— ë°”ë‹¤ì—ì„œ ì„œí•‘í•˜ê³  ì‹¶ì–´ìš”\"}'\n",
    "api_result = recommend_places_api(json_input, top_k=3)\n",
    "\n",
    "print(\"JSON ë¬¸ìì—´ ì…ë ¥ í…ŒìŠ¤íŠ¸:\")\n",
    "print(f\"Status: {api_result['status']}\")\n",
    "if api_result['status'] == 'success':\n",
    "    print(f\"ì¶”ì²œ ê²°ê³¼: {len(api_result['data']['recommendations'])}ê°œ\")\n",
    "    for i, place in enumerate(api_result['data']['recommendations']):\n",
    "        print(f\"  {i+1}. {place['name']} (ì ìˆ˜: {place['scores']['hybrid']})\")\n",
    "\n",
    "print(\"\\n API í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff17e0ce-fe1e-4adf-ab0c-0adfc3de8293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤===\n",
      "í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: ë³µí•© íƒœê·¸ ì…ë ¥\n",
      "ì…ë ¥: {'season': 'ê°€ì„', 'nature': ['ì‚°', 'ìì—°'], 'vibe': ['ê°ì„±', 'íœ´ì‹'], 'target': ['í˜¼ì']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "íŒŒì‹±ëœ ì…ë ¥: {'season': 'ê°€ì„', 'nature': ['ì‚°', 'ìì—°'], 'vibe': ['ê°ì„±', 'íœ´ì‹'], 'target': ['í˜¼ì']}\n",
      "ìƒìœ„ 3ê°œ ì¶”ì²œ:\n",
      "\n",
      "1. ì² ì•”ë‹¨í’êµ°ë½ì§€\n",
      "   ì„¤ëª…: ì² ì•”ë‹¨í’êµ°ë½ì§€ì€ëŠ” ê°€ì„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ê°ì„± ë¶„ìœ„ê¸°ë¡œ ê°€ì¡± ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤...\n",
      "   ì ìˆ˜: 0.6273\n",
      "\n",
      "2. í™ì²œ ì€í–‰ë‚˜ë¬´ìˆ²\n",
      "   ì„¤ëª…: í™ì²œ ì€í–‰ë‚˜ë¬´ìˆ²ì€ëŠ” ê°€ì„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ì‚¬ì§„ëª…ì†Œ ë¶„ìœ„ê¸°ë¡œ ì¹œêµ¬ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤...\n",
      "   ì ìˆ˜: 0.5592\n",
      "\n",
      "3. ë¯¼ë‘¥ì‚°\n",
      "   ì„¤ëª…: ë¯¼ë‘¥ì‚°ì€ëŠ” ê°€ì„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ë°”ë‹¤ ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° ëª¨ë‘ ë¶„ìœ„ê¸°ë¡œ ì¹œêµ¬ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤...\n",
      "   ì ìˆ˜: 0.5339\n",
      "\n",
      "==================================================\n",
      "í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: ë‹¤ì–‘í•œ ììœ  ë¬¸ì¥ ì…ë ¥\n",
      "\n",
      "ğŸ“ í…ŒìŠ¤íŠ¸ 1: ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜ ì‹ ë‚˜ëŠ” ì—¬ë¦„ íœ´ê°€ë¥¼ ë³´ë‚´ê³  ì‹¶ì–´ìš”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì‹±ëœ ì…ë ¥: {'season': 'ì—¬ë¦„', 'nature': [], 'vibe': ['í™œë ¥'], 'target': ['ì¹œêµ¬']}\n",
      "ì¶”ì²œ ê²°ê³¼:\n",
      "  1. ì„¤ì•…í•´ìˆ˜ìš•ì¥ (ì ìˆ˜: 0.5624)\n",
      "  2. ì˜ì§„í•´ë³€ (ì ìˆ˜: 0.5594)\n",
      "\n",
      "ğŸ“ í…ŒìŠ¤íŠ¸ 2: ì—°ì¸ê³¼ ë¡œë§¨í‹±í•œ ê°€ì„ ë°ì´íŠ¸ ì¥ì†Œë¥¼ ì°¾ê³  ìˆì–´ìš”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì‹±ëœ ì…ë ¥: {'season': 'ê°€ì„', 'nature': [], 'vibe': ['ê°ì„±'], 'target': ['ì—°ì¸']}\n",
      "ì¶”ì²œ ê²°ê³¼:\n",
      "  1. ì² ì•”ë‹¨í’êµ°ë½ì§€ (ì ìˆ˜: 0.6652)\n",
      "  2. ë¹„ë°€ì˜ì •ì› (ì ìˆ˜: 0.6001)\n",
      "\n",
      "ğŸ“ í…ŒìŠ¤íŠ¸ 3: ê°€ì¡±ê³¼ í•¨ê»˜ ì•ˆì „í•˜ê³  êµìœ¡ì ì¸ ê³³ì„ ê°€ê³  ì‹¶ìŠµë‹ˆë‹¤\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 18.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì‹±ëœ ì…ë ¥: {'season': None, 'nature': [], 'vibe': [], 'target': ['ê°€ì¡±']}\n",
      "ì¶”ì²œ ê²°ê³¼:\n",
      "  1. í–‡ì‚´ë§ˆì„ì²´í—˜ê´€ (ì ìˆ˜: 0.4633)\n",
      "  2. ê±°ì§„í•­ (ì ìˆ˜: 0.4528)\n",
      "\n",
      "âœ… ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "\n",
    "print(\"\\n=== ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤===\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: ë³µí•© íƒœê·¸ ì…ë ¥\n",
    "test_input_4 = {\n",
    "    \"season\": \"ê°€ì„\",\n",
    "    \"nature\": [\"ì‚°\", \"ìì—°\"],\n",
    "    \"vibe\": [\"ê°ì„±\", \"íœ´ì‹\"],\n",
    "    \"target\": [\"í˜¼ì\"]\n",
    "}\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: ë³µí•© íƒœê·¸ ì…ë ¥\")\n",
    "print(f\"ì…ë ¥: {test_input_4}\")\n",
    "\n",
    "result_4 = recommender.recommend_places(test_input_4, top_k=3)\n",
    "\n",
    "print(f\"\\níŒŒì‹±ëœ ì…ë ¥: {result_4['parsed_input']}\")\n",
    "print(f\"ìƒìœ„ 3ê°œ ì¶”ì²œ:\")\n",
    "\n",
    "for i, place in enumerate(result_4['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ì„¤ëª…: {place['description'][:100]}...\")\n",
    "    print(f\"   ì ìˆ˜: {place['hybrid_score']:.4f}\")\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: ë‹¤ì–‘í•œ ììœ  ë¬¸ì¥ ì…ë ¥\n",
    "test_cases = [\n",
    "    \"ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜ ì‹ ë‚˜ëŠ” ì—¬ë¦„ íœ´ê°€ë¥¼ ë³´ë‚´ê³  ì‹¶ì–´ìš”\",\n",
    "    \"ì—°ì¸ê³¼ ë¡œë§¨í‹±í•œ ê°€ì„ ë°ì´íŠ¸ ì¥ì†Œë¥¼ ì°¾ê³  ìˆì–´ìš”\",\n",
    "    \"ê°€ì¡±ê³¼ í•¨ê»˜ ì•ˆì „í•˜ê³  êµìœ¡ì ì¸ ê³³ì„ ê°€ê³  ì‹¶ìŠµë‹ˆë‹¤\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: ë‹¤ì–‘í•œ ììœ  ë¬¸ì¥ ì…ë ¥\")\n",
    "\n",
    "for i, test_text in enumerate(test_cases):\n",
    "    print(f\"\\nğŸ“ í…ŒìŠ¤íŠ¸ {i+1}: {test_text}\")\n",
    "    \n",
    "    test_input = {\"free_text\": test_text}\n",
    "    result = recommender.recommend_places(test_input, top_k=2)\n",
    "    \n",
    "    print(f\"íŒŒì‹±ëœ ì…ë ¥: {result['parsed_input']}\")\n",
    "    print(f\"ì¶”ì²œ ê²°ê³¼:\")\n",
    "    for j, place in enumerate(result['recommendations']):\n",
    "        print(f\"  {j+1}. {place['name']} (ì ìˆ˜: {place['hybrid_score']:.4f})\")\n",
    "\n",
    "print(\"\\nâœ… ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63e763b5-818c-49f4-b574-20ae1d164340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " === ì„±ëŠ¥ ë¶„ì„ ===\n",
      "ğŸ“Š ì¶”ì²œ ì ìˆ˜ ë¶„í¬ ë¶„ì„:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "í…ŒìŠ¤íŠ¸ 1: {'season': 'ì—¬ë¦„', 'nature': ['ë°”ë‹¤'], 'vibe': ['íœ´ì‹'], 'target': ['ì—°ì¸']}\n",
      "  í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ë²”ìœ„: 0.6072 ~ 0.6832\n",
      "  ìœ ì‚¬ë„ ì ìˆ˜ í‰ê· : 0.6454\n",
      "  íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ í‰ê· : 0.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "í…ŒìŠ¤íŠ¸ 2: {'season': 'ê²¨ìš¸', 'nature': ['ì‚°'], 'vibe': ['ëª¨í—˜'], 'target': ['ì¹œêµ¬']}\n",
      "  í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ë²”ìœ„: 0.5691 ~ 0.6482\n",
      "  ìœ ì‚¬ë„ ì ìˆ˜ í‰ê· : 0.6122\n",
      "  íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ í‰ê· : 0.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "í…ŒìŠ¤íŠ¸ 3: {'season': 'ë´„', 'nature': ['ìì—°'], 'vibe': ['ê°ì„±'], 'target': ['í˜¼ì']}\n",
      "  í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ë²”ìœ„: 0.5478 ~ 0.6735\n",
      "  ìœ ì‚¬ë„ ì ìˆ˜ í‰ê· : 0.5999\n",
      "  íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ í‰ê· : 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "í…ŒìŠ¤íŠ¸ 4: {'free_text': 'ê°€ì„ì— ë‹¨í’ ë³´ëŸ¬ ê°€ê³  ì‹¶ì–´ìš”'}\n",
      "  í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ë²”ìœ„: 0.4283 ~ 0.4933\n",
      "  ìœ ì‚¬ë„ ì ìˆ˜ í‰ê· : 0.5673\n",
      "  íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ í‰ê· : 0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "í…ŒìŠ¤íŠ¸ 5: {'free_text': 'ìŠ¤í‚¤ì¥ì—ì„œ ìŠ¤ë¦´ ë„˜ì¹˜ëŠ” ê²¨ìš¸ì„ ë³´ë‚´ê³  ì‹¶ìŠµë‹ˆë‹¤'}\n",
      "  í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ë²”ìœ„: 0.4209 ~ 0.4773\n",
      "  ìœ ì‚¬ë„ ì ìˆ˜ í‰ê· : 0.5455\n",
      "  íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ í‰ê· : 0.3000\n",
      "\n",
      "ğŸ”§ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì •ë³´:\n",
      "- ì „ì²´ ê´€ê´‘ì§€ ìˆ˜: 100\n",
      "- ì„ë² ë”© ì°¨ì›: 768\n",
      "- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 0.29 MB\n",
      "- í•™ìŠµëœ ëª¨ë¸ ìˆ˜: 4\n",
      "\n",
      "âœ… ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "## ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™”\n",
    "print(\"\\n === ì„±ëŠ¥ ë¶„ì„ ===\")\n",
    "\n",
    "#ì¶”ì²œ ì ìˆ˜ ë¶„í¬ ë¶„ì„\n",
    "def analyze_recommendation_scores():\n",
    "    \"\"\"ì¶”ì²œ ì ìˆ˜ ë¶„í¬ ë¶„ì„\"\"\"\n",
    "\n",
    "    # ìƒ˜í”Œ ë°ì´í„° ì…ë ¥ë“¤\n",
    "    sample_inputs = [\n",
    "        {\"season\": \"ì—¬ë¦„\", \"nature\": [\"ë°”ë‹¤\"], \"vibe\": [\"íœ´ì‹\"], \"target\": [\"ì—°ì¸\"]},\n",
    "        {\"season\": \"ê²¨ìš¸\", \"nature\": [\"ì‚°\"], \"vibe\": [\"ëª¨í—˜\"], \"target\": [\"ì¹œêµ¬\"]},\n",
    "        {\"season\": \"ë´„\", \"nature\": [\"ìì—°\"], \"vibe\": [\"ê°ì„±\"], \"target\": [\"í˜¼ì\"]},\n",
    "        {\"free_text\": \"ê°€ì„ì— ë‹¨í’ ë³´ëŸ¬ ê°€ê³  ì‹¶ì–´ìš”\"},\n",
    "        {\"free_text\": \"ìŠ¤í‚¤ì¥ì—ì„œ ìŠ¤ë¦´ ë„˜ì¹˜ëŠ” ê²¨ìš¸ì„ ë³´ë‚´ê³  ì‹¶ìŠµë‹ˆë‹¤\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ“Š ì¶”ì²œ ì ìˆ˜ ë¶„í¬ ë¶„ì„:\")\n",
    "\n",
    "    for i, test_input in enumerate(sample_inputs):\n",
    "        result = recommender.recommend_places(test_input, top_k=5)\n",
    "\n",
    "        hybrid_scores = [place['hybrid_score'] for place in result ['recommendations']]\n",
    "        similarity_scores = [place['similarity_score'] for place in result['recommendations']]\n",
    "        tag_scores = [place['tag_score'] for place in result['recommendations']]\n",
    "        \n",
    "        print(f\"\\ní…ŒìŠ¤íŠ¸ {i+1}: {test_input}\")\n",
    "        print(f\"  í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ë²”ìœ„: {min(hybrid_scores):.4f} ~ {max(hybrid_scores):.4f}\")\n",
    "        print(f\"  ìœ ì‚¬ë„ ì ìˆ˜ í‰ê· : {np.mean(similarity_scores):.4f}\")\n",
    "        print(f\"  íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ í‰ê· : {np.mean(tag_scores):.4f}\")\n",
    "\n",
    "analyze_recommendation_scores()\n",
    "\n",
    "# ì‹œìŠ¤í…œ ì„±ëŠ¥ ì •ë³´\n",
    "print(f\"\\nğŸ”§ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì •ë³´:\")\n",
    "print(f\"- ì „ì²´ ê´€ê´‘ì§€ ìˆ˜: {len(recommender.df)}\")\n",
    "print(f\"- ì„ë² ë”© ì°¨ì›: {recommender.place_embeddings.shape[1]}\")\n",
    "print(f\"- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {recommender.place_embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"- í•™ìŠµëœ ëª¨ë¸ ìˆ˜: {len(recommender.xgb_trainer.models)}\")\n",
    "\n",
    "print(\"\\nâœ… ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48edfebd-8839-4121-a71f-76235ffa8d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ‰ ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ ìƒì„±ëœ íŒŒì¼ êµ¬ì¡°:\n",
      "\n",
      "project_root/\n",
      "â”œâ”€â”€ data/\n",
      "â”‚   â”œâ”€â”€ raw/gangwon_places_100.csv                 # ì›ë³¸ ë°ì´í„°\n",
      "â”‚   â”œâ”€â”€ processed/gangwon_places_100_processed.csv # ì „ì²˜ë¦¬ëœ ë°ì´í„°\n",
      "â”‚   â””â”€â”€ embeddings/place_embeddings_full768.npy    # 768ì°¨ì› ì„ë² ë”©\n",
      "â”œâ”€â”€ models/\n",
      "â”‚   â”œâ”€â”€ xgboost/\n",
      "â”‚   â”‚   â”œâ”€â”€ season_model.joblib                    # ê³„ì ˆ ë¶„ë¥˜ ëª¨ë¸\n",
      "â”‚   â”‚   â”œâ”€â”€ nature_model.joblib                    # ìì—°í™˜ê²½ ë¶„ë¥˜ ëª¨ë¸\n",
      "â”‚   â”‚   â”œâ”€â”€ vibe_model.joblib                      # ë¶„ìœ„ê¸° ë¶„ë¥˜ ëª¨ë¸\n",
      "â”‚   â”‚   â””â”€â”€ target_model.joblib                    # ëŒ€ìƒ ë¶„ë¥˜ ëª¨ë¸\n",
      "â”‚   â””â”€â”€ encoders/\n",
      "â”‚       â”œâ”€â”€ season_encoder.joblib                  # ê³„ì ˆ ì¸ì½”ë”\n",
      "â”‚       â”œâ”€â”€ nature_encoder.joblib                  # ìì—°í™˜ê²½ ì¸ì½”ë”\n",
      "â”‚       â”œâ”€â”€ vibe_encoder.joblib                    # ë¶„ìœ„ê¸° ì¸ì½”ë”\n",
      "â”‚       â””â”€â”€ target_encoder.joblib                  # ëŒ€ìƒ ì¸ì½”ë”\n",
      "â””â”€â”€ config/config.yaml                             # ì„¤ì • íŒŒì¼\n",
      "\n",
      "\n",
      "ğŸš€ ì‚¬ìš©ë²•:\n",
      "\n",
      "1. íƒœê·¸ ê¸°ë°˜ ì¶”ì²œ:\n",
      "   user_input = {\n",
      "       \"season\": \"ì—¬ë¦„\",\n",
      "       \"nature\": [\"ë°”ë‹¤\", \"ìì—°\"],\n",
      "       \"vibe\": [\"ê°ì„±\", \"íœ´ì‹\"],\n",
      "       \"target\": [\"ì—°ì¸\"]\n",
      "   }\n",
      "   result = recommender.recommend_places(user_input, top_k=5)\n",
      "\n",
      "2. ììœ  ë¬¸ì¥ ê¸°ë°˜ ì¶”ì²œ:\n",
      "   user_input = {\n",
      "       \"free_text\": \"ê²¨ìš¸ì— ê°€ì¡±ê³¼ í•¨ê»˜ ìŠ¤í‚¤ë¥¼ íƒ€ê³  ì‹¶ì–´ìš”\"\n",
      "   }\n",
      "   result = recommender.recommend_places(user_input, top_k=5)\n",
      "\n",
      "3. Flask API ì—°ë™:\n",
      "   api_response = recommend_places_api(user_input, top_k=10)\n",
      "\n",
      "\n",
      "âš™ï¸ ì£¼ìš” ê¸°ëŠ¥:\n",
      "\n",
      "âœ… SBERT ê¸°ë°˜ í•œêµ­ì–´ ì„ë² ë”© ìƒì„± (768ì°¨ì› ìœ ì§€)\n",
      "âœ… XGBoost ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ (season, nature, vibe, target)\n",
      "âœ… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ 60% + íƒœê·¸ 40%)\n",
      "âœ… ììœ  ë¬¸ì¥ ì…ë ¥ íŒŒì‹± ë° íƒœê·¸ ì¶”ì¶œ\n",
      "âœ… ëª¨ë¸ ë° ì¸ì½”ë” ì €ì¥/ë¡œë“œ\n",
      "âœ… Flask API ì—°ë™ ì¤€ë¹„\n",
      "âœ… JSON ì…ì¶œë ¥ ì§€ì›\n",
      "âœ… ì„±ëŠ¥ ë¶„ì„ ë„êµ¬\n",
      "âœ… ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì§€ì›\n",
      "\n",
      "\n",
      "ğŸ“Š ì„±ëŠ¥ ì§€í‘œ:\n",
      "- ë°ì´í„°: 100ê°œ ê´€ê´‘ì§€\n",
      "- ì„ë² ë”© ì°¨ì›: 768ì°¨ì›\n",
      "- ëª¨ë¸ íƒ€ì…: XGBoost (season: ë‹¨ì¼ë¼ë²¨, nature/vibe/target: ë‹¤ì¤‘ë¼ë²¨)\n",
      "- ì¶”ì²œ ë°©ì‹: í•˜ì´ë¸Œë¦¬ë“œ (ìœ ì‚¬ë„ 60% + íƒœê·¸ 40%)\n",
      "- ì§€ì› ì…ë ¥: íƒœê·¸ ê¸°ë°˜ + ììœ  ë¬¸ì¥ ì…ë ¥\n",
      "\n",
      "ğŸ”„ ëª¨ë¸ ì¬ì‚¬ìš©:\n",
      "\n",
      "# ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\n",
      "new_recommender = GangwonPlaceRecommender()\n",
      "new_recommender.df = pd.read_csv('data/processed/gangwon_places_100_processed.csv')\n",
      "new_recommender.place_embeddings = np.load('data/embeddings/place_embeddings_full768.npy')\n",
      "new_recommender.preprocessor.load_encoders('models/encoders')\n",
      "new_recommender.xgb_trainer.load_models('models')\n",
      "\n",
      "# ì¶”ì²œ ì‹¤í–‰\n",
      "result = new_recommender.recommend_places(user_input, top_k=5)\n",
      "\n",
      "\n",
      "ğŸ’¡ ì¶”ê°€ í™œìš© ë°©ì•ˆ:\n",
      "\n",
      "1. ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì—°ë™:\n",
      "   - Flask/Django ë°±ì—”ë“œì— recommend_places_api() í•¨ìˆ˜ í™œìš©\n",
      "   - REST API ì—”ë“œí¬ì¸íŠ¸ êµ¬ì„±\n",
      "   - ì‹¤ì‹œê°„ ì¶”ì²œ ì„œë¹„ìŠ¤ ì œê³µ\n",
      "\n",
      "2. ëª¨ë°”ì¼ ì•± ì—°ë™:\n",
      "   - JSON í˜•íƒœì˜ API ì‘ë‹µ í™œìš©\n",
      "   - ì‚¬ìš©ì ì…ë ¥ íŒŒì‹± ê¸°ëŠ¥ í™œìš©\n",
      "   - ì˜¤í”„ë¼ì¸ ëª¨ë¸ ë°°í¬ ê°€ëŠ¥\n",
      "\n",
      "3. ì„±ëŠ¥ ìµœì í™”:\n",
      "   - ì„ë² ë”© ìºì‹±ìœ¼ë¡œ ì‘ë‹µ ì†ë„ í–¥ìƒ\n",
      "   - ë°°ì¹˜ ì¶”ì²œ ì²˜ë¦¬\n",
      "   - ëª¨ë¸ ì••ì¶• ë° ê²½ëŸ‰í™”\n",
      "\n",
      "4. ê¸°ëŠ¥ í™•ì¥:\n",
      "   - ì‚¬ìš©ì í”¼ë“œë°± í•™ìŠµ\n",
      "   - í˜‘ì—… í•„í„°ë§ ì¶”ê°€\n",
      "   - ê°œì¸í™” ì¶”ì²œ êµ¬í˜„\n",
      "   - ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ\n",
      "   - ì§€ì—­ë³„ í•„í„°ë§ ê¸°ëŠ¥\n",
      "\n",
      "\n",
      "ğŸ“ ì£¼ì˜ì‚¬í•­:\n",
      "\n",
      "- ì²« ì‹¤í–‰ ì‹œ SBERT ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
      "- GPU ì‚¬ìš© ì‹œ ë” ë¹ ë¥¸ ì„ë² ë”© ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤\n",
      "- ì‹¤ì œ ì„œë¹„ìŠ¤ ë°°í¬ ì‹œ ë³´ì•ˆ ë° ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ê°•í™”í•˜ì„¸ìš”\n",
      "- ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œ ëª¨ë¸ ì¬í•™ìŠµì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
      "- ì¶”ì²œ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì •ê¸°ì ì¸ ëª¨ë¸ íŠœë‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤\n",
      "\n",
      "\n",
      "ğŸŒŸ ì¶”ì²œ ì‹œìŠ¤í…œ íŠ¹ì§•:\n",
      "\n",
      "- í•œêµ­ì–´ íŠ¹í™” SBERT ëª¨ë¸ ì‚¬ìš© (snunlp/KR-SBERT-V40K-klueNLI-augSTS)\n",
      "- í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ (ì˜ë¯¸ì  ìœ ì‚¬ë„ + íƒœê·¸ ë§¤ì¹­)\n",
      "- ììœ  ë¬¸ì¥ ì…ë ¥ ì§€ì›ìœ¼ë¡œ ì‚¬ìš©ì í¸ì˜ì„± í–¥ìƒ\n",
      "- ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ë¡œ ì •í™•í•œ íƒœê·¸ ì˜ˆì¸¡\n",
      "- ëª¨ë¸ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ìš´ì˜\n",
      "- Flask API ì—°ë™ìœ¼ë¡œ ì›¹ ì„œë¹„ìŠ¤ í™•ì¥ ê°€ëŠ¥\n",
      "- ì„±ëŠ¥ ë¶„ì„ ë„êµ¬ë¡œ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥\n",
      "\n",
      "\n",
      "ğŸ¯ ì¶”ì²œ ì‹œìŠ¤í…œ ì„±ëŠ¥:\n",
      "\n",
      "- ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (60% ê°€ì¤‘ì¹˜)\n",
      "- íƒœê·¸ ë§¤ì¹­ ê¸°ë°˜ ì •í™•ë„ í–¥ìƒ (40% ê°€ì¤‘ì¹˜)\n",
      "- ê³„ì ˆ, ìì—°í™˜ê²½, ë¶„ìœ„ê¸°, ëŒ€ìƒë³„ ì„¸ë¶„í™”ëœ ì¶”ì²œ\n",
      "- ììœ  ë¬¸ì¥ íŒŒì‹±ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì‚¬ìš©ì ê²½í—˜\n",
      "- ìƒìœ„ Kê°œ ì¶”ì²œìœ¼ë¡œ ë‹¤ì–‘í•œ ì„ íƒì§€ ì œê³µ\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "   ì´ì œ ë‹¤ì–‘í•œ ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ ì •í™•í•œ ê´€ê´‘ì§€ ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "   Flask API ì—°ë™ì„ í†µí•´ ì›¹ ì„œë¹„ìŠ¤ë¡œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "   ëª¨ë“  ì˜¤ë¥˜ê°€ ìˆ˜ì •ë˜ì–´ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.\n",
      "   íƒœê·¸ ê¸°ë°˜ ì¶”ì²œê³¼ ììœ  ë¬¸ì¥ ì…ë ¥ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ:\n",
      "\n",
      "- SBERT ëª¨ë¸ ìƒì„¸ ì •ë³´: https://huggingface.co/snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
      "- XGBoost ê³µì‹ ë¬¸ì„œ: https://xgboost.readthedocs.io/\n",
      "- Scikit-learn ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜: https://scikit-learn.org/stable/modules/multiclass.html\n",
      "- Flask API ê°œë°œ ê°€ì´ë“œ: https://flask.palletsprojects.com/\n",
      "\n",
      "\n",
      "ğŸ”— ë‹¤ìŒ ë‹¨ê³„:\n",
      "\n",
      "1. ì›¹ ì¸í„°í˜ì´ìŠ¤ ê°œë°œ (HTML/CSS/JavaScript)\n",
      "2. Flask/Django ë°±ì—”ë“œ API êµ¬ì¶•\n",
      "3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (PostgreSQL/MySQL)\n",
      "4. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ì‹œìŠ¤í…œ\n",
      "5. ì¶”ì²œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ\n",
      "6. ëª¨ë°”ì¼ ì•± ì—°ë™\n",
      "7. ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
      "\n",
      "\n",
      "âœ¨ ì™„ë£Œëœ ê¸°ëŠ¥ë“¤:\n",
      "\n",
      "âœ… ë°ì´í„° ì „ì²˜ë¦¬ ë° ì •ì œ\n",
      "âœ… SBERT ì„ë² ë”© ìƒì„± (768ì°¨ì›)\n",
      "âœ… XGBoost ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n",
      "âœ… í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„\n",
      "âœ… ììœ  ë¬¸ì¥ ì…ë ¥ íŒŒì‹± ì‹œìŠ¤í…œ\n",
      "âœ… íƒœê·¸ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ\n",
      "âœ… ëª¨ë¸ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥\n",
      "âœ… Flask API ì—°ë™ ì¤€ë¹„\n",
      "âœ… ì„±ëŠ¥ ë¶„ì„ ë„êµ¬\n",
      "âœ… ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
      "âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë””ë²„ê¹…\n",
      "âœ… ì™„ì „í•œ ë¬¸ì„œí™”\n",
      "\n",
      "\n",
      "ğŸŠ ì¶•í•˜í•©ë‹ˆë‹¤! ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "ì´ì œ ì‹¤ì œ ì‚¬ìš©ìë“¤ì—ê²Œ ì •í™•í•˜ê³  ìœ ìš©í•œ ê´€ê´‘ì§€ ì¶”ì²œì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì œ\n",
      "============================================================\n",
      "\n",
      "ğŸ“ ì˜ˆì œ 1: ê°„ë‹¨í•œ ì¶”ì²œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 24.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥: ë´„ì— ì‚°ì—ì„œ íë§\n",
      "ì¶”ì²œ ê²°ê³¼:\n",
      "  1. êµ­ë¦½ ì‚¼ë´‰ìì—°íœ´ì–‘ë¦¼ (ì ìˆ˜: 0.661)\n",
      "  2. ê°•ë¦‰ ì†”í–¥ìˆ˜ëª©ì› (ì ìˆ˜: 0.644)\n",
      "  3. íƒœë°± êµ¬ì™€ìš°ë§ˆì„(ê³ ì›ììƒì‹ë¬¼ì›) (ì ìˆ˜: 0.631)\n",
      "\n",
      "ğŸ“ ì˜ˆì œ 2: íƒœê·¸ ì¡°í•© ì¶”ì²œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 23.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥: {'season': 'ì—¬ë¦„', 'nature': ['ë°”ë‹¤'], 'target': ['ê°€ì¡±']}\n",
      "ì¶”ì²œ ê²°ê³¼:\n",
      "  1. ì‘ì€í›„ì§„í•´ìˆ˜ìš•ì¥ (ì ìˆ˜: 0.643)\n",
      "  2. í™”ì§„í¬í•´ìˆ˜ìš•ì¥ (ì ìˆ˜: 0.643)\n",
      "  3. ì¥í˜¸ì–´ì´Œì²´í—˜ë§ˆì„ (ì ìˆ˜: 0.637)\n",
      "\n",
      "============================================================\n",
      "ğŸš€ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ë§ˆìŒê» ì‚¬ìš©í•˜ì„¸ìš”!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### ìµœì¢… ì •ë¦¬ ë° ì‚¬ìš©ë²• ì•ˆë‚´\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“ ìƒì„±ëœ íŒŒì¼ êµ¬ì¡°:\")\n",
    "print(\"\"\"\n",
    "project_root/\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ raw/gangwon_places_100.csv                 # ì›ë³¸ ë°ì´í„°\n",
    "â”‚   â”œâ”€â”€ processed/gangwon_places_100_processed.csv # ì „ì²˜ë¦¬ëœ ë°ì´í„°\n",
    "â”‚   â””â”€â”€ embeddings/place_embeddings_full768.npy    # 768ì°¨ì› ì„ë² ë”©\n",
    "â”œâ”€â”€ models/\n",
    "â”‚   â”œâ”€â”€ xgboost/\n",
    "â”‚   â”‚   â”œâ”€â”€ season_model.joblib                    # ê³„ì ˆ ë¶„ë¥˜ ëª¨ë¸\n",
    "â”‚   â”‚   â”œâ”€â”€ nature_model.joblib                    # ìì—°í™˜ê²½ ë¶„ë¥˜ ëª¨ë¸\n",
    "â”‚   â”‚   â”œâ”€â”€ vibe_model.joblib                      # ë¶„ìœ„ê¸° ë¶„ë¥˜ ëª¨ë¸\n",
    "â”‚   â”‚   â””â”€â”€ target_model.joblib                    # ëŒ€ìƒ ë¶„ë¥˜ ëª¨ë¸\n",
    "â”‚   â””â”€â”€ encoders/\n",
    "â”‚       â”œâ”€â”€ season_encoder.joblib                  # ê³„ì ˆ ì¸ì½”ë”\n",
    "â”‚       â”œâ”€â”€ nature_encoder.joblib                  # ìì—°í™˜ê²½ ì¸ì½”ë”\n",
    "â”‚       â”œâ”€â”€ vibe_encoder.joblib                    # ë¶„ìœ„ê¸° ì¸ì½”ë”\n",
    "â”‚       â””â”€â”€ target_encoder.joblib                  # ëŒ€ìƒ ì¸ì½”ë”\n",
    "â””â”€â”€ config/config.yaml                             # ì„¤ì • íŒŒì¼\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸš€ ì‚¬ìš©ë²•:\")\n",
    "print(\"\"\"\n",
    "1. íƒœê·¸ ê¸°ë°˜ ì¶”ì²œ:\n",
    "   user_input = {\n",
    "       \"season\": \"ì—¬ë¦„\",\n",
    "       \"nature\": [\"ë°”ë‹¤\", \"ìì—°\"],\n",
    "       \"vibe\": [\"ê°ì„±\", \"íœ´ì‹\"],\n",
    "       \"target\": [\"ì—°ì¸\"]\n",
    "   }\n",
    "   result = recommender.recommend_places(user_input, top_k=5)\n",
    "\n",
    "2. ììœ  ë¬¸ì¥ ê¸°ë°˜ ì¶”ì²œ:\n",
    "   user_input = {\n",
    "       \"free_text\": \"ê²¨ìš¸ì— ê°€ì¡±ê³¼ í•¨ê»˜ ìŠ¤í‚¤ë¥¼ íƒ€ê³  ì‹¶ì–´ìš”\"\n",
    "   }\n",
    "   result = recommender.recommend_places(user_input, top_k=5)\n",
    "\n",
    "3. Flask API ì—°ë™:\n",
    "   api_response = recommend_places_api(user_input, top_k=10)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâš™ï¸ ì£¼ìš” ê¸°ëŠ¥:\")\n",
    "print(\"\"\"\n",
    "âœ… SBERT ê¸°ë°˜ í•œêµ­ì–´ ì„ë² ë”© ìƒì„± (768ì°¨ì› ìœ ì§€)\n",
    "âœ… XGBoost ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ (season, nature, vibe, target)\n",
    "âœ… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ 60% + íƒœê·¸ 40%)\n",
    "âœ… ììœ  ë¬¸ì¥ ì…ë ¥ íŒŒì‹± ë° íƒœê·¸ ì¶”ì¶œ\n",
    "âœ… ëª¨ë¸ ë° ì¸ì½”ë” ì €ì¥/ë¡œë“œ\n",
    "âœ… Flask API ì—°ë™ ì¤€ë¹„\n",
    "âœ… JSON ì…ì¶œë ¥ ì§€ì›\n",
    "âœ… ì„±ëŠ¥ ë¶„ì„ ë„êµ¬\n",
    "âœ… ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì§€ì›\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ“Š ì„±ëŠ¥ ì§€í‘œ:\")\n",
    "print(f\"- ë°ì´í„°: {len(recommender.df)}ê°œ ê´€ê´‘ì§€\")\n",
    "print(f\"- ì„ë² ë”© ì°¨ì›: {recommender.place_embeddings.shape[1]}ì°¨ì›\")\n",
    "print(f\"- ëª¨ë¸ íƒ€ì…: XGBoost (season: ë‹¨ì¼ë¼ë²¨, nature/vibe/target: ë‹¤ì¤‘ë¼ë²¨)\")\n",
    "print(f\"- ì¶”ì²œ ë°©ì‹: í•˜ì´ë¸Œë¦¬ë“œ (ìœ ì‚¬ë„ 60% + íƒœê·¸ 40%)\")\n",
    "print(f\"- ì§€ì› ì…ë ¥: íƒœê·¸ ê¸°ë°˜ + ììœ  ë¬¸ì¥ ì…ë ¥\")\n",
    "\n",
    "print(\"\\nğŸ”„ ëª¨ë¸ ì¬ì‚¬ìš©:\")\n",
    "print(\"\"\"\n",
    "# ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\n",
    "new_recommender = GangwonPlaceRecommender()\n",
    "new_recommender.df = pd.read_csv('data/processed/gangwon_places_100_processed.csv')\n",
    "new_recommender.place_embeddings = np.load('data/embeddings/place_embeddings_full768.npy')\n",
    "new_recommender.preprocessor.load_encoders('models/encoders')\n",
    "new_recommender.xgb_trainer.load_models('models')\n",
    "\n",
    "# ì¶”ì²œ ì‹¤í–‰\n",
    "result = new_recommender.recommend_places(user_input, top_k=5)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ì¶”ê°€ í™œìš© ë°©ì•ˆ:\")\n",
    "print(\"\"\"\n",
    "1. ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì—°ë™:\n",
    "   - Flask/Django ë°±ì—”ë“œì— recommend_places_api() í•¨ìˆ˜ í™œìš©\n",
    "   - REST API ì—”ë“œí¬ì¸íŠ¸ êµ¬ì„±\n",
    "   - ì‹¤ì‹œê°„ ì¶”ì²œ ì„œë¹„ìŠ¤ ì œê³µ\n",
    "\n",
    "2. ëª¨ë°”ì¼ ì•± ì—°ë™:\n",
    "   - JSON í˜•íƒœì˜ API ì‘ë‹µ í™œìš©\n",
    "   - ì‚¬ìš©ì ì…ë ¥ íŒŒì‹± ê¸°ëŠ¥ í™œìš©\n",
    "   - ì˜¤í”„ë¼ì¸ ëª¨ë¸ ë°°í¬ ê°€ëŠ¥\n",
    "\n",
    "3. ì„±ëŠ¥ ìµœì í™”:\n",
    "   - ì„ë² ë”© ìºì‹±ìœ¼ë¡œ ì‘ë‹µ ì†ë„ í–¥ìƒ\n",
    "   - ë°°ì¹˜ ì¶”ì²œ ì²˜ë¦¬\n",
    "   - ëª¨ë¸ ì••ì¶• ë° ê²½ëŸ‰í™”\n",
    "\n",
    "4. ê¸°ëŠ¥ í™•ì¥:\n",
    "   - ì‚¬ìš©ì í”¼ë“œë°± í•™ìŠµ\n",
    "   - í˜‘ì—… í•„í„°ë§ ì¶”ê°€\n",
    "   - ê°œì¸í™” ì¶”ì²œ êµ¬í˜„\n",
    "   - ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ\n",
    "   - ì§€ì—­ë³„ í•„í„°ë§ ê¸°ëŠ¥\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ“ ì£¼ì˜ì‚¬í•­:\")\n",
    "print(\"\"\"\n",
    "- ì²« ì‹¤í–‰ ì‹œ SBERT ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "- GPU ì‚¬ìš© ì‹œ ë” ë¹ ë¥¸ ì„ë² ë”© ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤\n",
    "- ì‹¤ì œ ì„œë¹„ìŠ¤ ë°°í¬ ì‹œ ë³´ì•ˆ ë° ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ê°•í™”í•˜ì„¸ìš”\n",
    "- ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œ ëª¨ë¸ ì¬í•™ìŠµì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "- ì¶”ì²œ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì •ê¸°ì ì¸ ëª¨ë¸ íŠœë‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸŒŸ ì¶”ì²œ ì‹œìŠ¤í…œ íŠ¹ì§•:\")\n",
    "print(\"\"\"\n",
    "- í•œêµ­ì–´ íŠ¹í™” SBERT ëª¨ë¸ ì‚¬ìš© (snunlp/KR-SBERT-V40K-klueNLI-augSTS)\n",
    "- í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ (ì˜ë¯¸ì  ìœ ì‚¬ë„ + íƒœê·¸ ë§¤ì¹­)\n",
    "- ììœ  ë¬¸ì¥ ì…ë ¥ ì§€ì›ìœ¼ë¡œ ì‚¬ìš©ì í¸ì˜ì„± í–¥ìƒ\n",
    "- ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ë¡œ ì •í™•í•œ íƒœê·¸ ì˜ˆì¸¡\n",
    "- ëª¨ë¸ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ìš´ì˜\n",
    "- Flask API ì—°ë™ìœ¼ë¡œ ì›¹ ì„œë¹„ìŠ¤ í™•ì¥ ê°€ëŠ¥\n",
    "- ì„±ëŠ¥ ë¶„ì„ ë„êµ¬ë¡œ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ¯ ì¶”ì²œ ì‹œìŠ¤í…œ ì„±ëŠ¥:\")\n",
    "print(\"\"\"\n",
    "- ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (60% ê°€ì¤‘ì¹˜)\n",
    "- íƒœê·¸ ë§¤ì¹­ ê¸°ë°˜ ì •í™•ë„ í–¥ìƒ (40% ê°€ì¤‘ì¹˜)\n",
    "- ê³„ì ˆ, ìì—°í™˜ê²½, ë¶„ìœ„ê¸°, ëŒ€ìƒë³„ ì„¸ë¶„í™”ëœ ì¶”ì²œ\n",
    "- ììœ  ë¬¸ì¥ íŒŒì‹±ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì‚¬ìš©ì ê²½í—˜\n",
    "- ìƒìœ„ Kê°œ ì¶”ì²œìœ¼ë¡œ ë‹¤ì–‘í•œ ì„ íƒì§€ ì œê³µ\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"   ì´ì œ ë‹¤ì–‘í•œ ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ ì •í™•í•œ ê´€ê´‘ì§€ ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "print(\"   Flask API ì—°ë™ì„ í†µí•´ ì›¹ ì„œë¹„ìŠ¤ë¡œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"   ëª¨ë“  ì˜¤ë¥˜ê°€ ìˆ˜ì •ë˜ì–´ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.\")\n",
    "print(\"   íƒœê·¸ ê¸°ë°˜ ì¶”ì²œê³¼ ììœ  ë¬¸ì¥ ì…ë ¥ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ:\")\n",
    "print(\"\"\"\n",
    "- SBERT ëª¨ë¸ ìƒì„¸ ì •ë³´: https://huggingface.co/snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
    "- XGBoost ê³µì‹ ë¬¸ì„œ: https://xgboost.readthedocs.io/\n",
    "- Scikit-learn ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜: https://scikit-learn.org/stable/modules/multiclass.html\n",
    "- Flask API ê°œë°œ ê°€ì´ë“œ: https://flask.palletsprojects.com/\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ”— ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"\"\"\n",
    "1. ì›¹ ì¸í„°í˜ì´ìŠ¤ ê°œë°œ (HTML/CSS/JavaScript)\n",
    "2. Flask/Django ë°±ì—”ë“œ API êµ¬ì¶•\n",
    "3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (PostgreSQL/MySQL)\n",
    "4. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ì‹œìŠ¤í…œ\n",
    "5. ì¶”ì²œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ\n",
    "6. ëª¨ë°”ì¼ ì•± ì—°ë™\n",
    "7. ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâœ¨ ì™„ë£Œëœ ê¸°ëŠ¥ë“¤:\")\n",
    "print(\"\"\"\n",
    "âœ… ë°ì´í„° ì „ì²˜ë¦¬ ë° ì •ì œ\n",
    "âœ… SBERT ì„ë² ë”© ìƒì„± (768ì°¨ì›)\n",
    "âœ… XGBoost ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n",
    "âœ… í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„\n",
    "âœ… ììœ  ë¬¸ì¥ ì…ë ¥ íŒŒì‹± ì‹œìŠ¤í…œ\n",
    "âœ… íƒœê·¸ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ\n",
    "âœ… ëª¨ë¸ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥\n",
    "âœ… Flask API ì—°ë™ ì¤€ë¹„\n",
    "âœ… ì„±ëŠ¥ ë¶„ì„ ë„êµ¬\n",
    "âœ… ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë””ë²„ê¹…\n",
    "âœ… ì™„ì „í•œ ë¬¸ì„œí™”\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸŠ ì¶•í•˜í•©ë‹ˆë‹¤! ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"ì´ì œ ì‹¤ì œ ì‚¬ìš©ìë“¤ì—ê²Œ ì •í™•í•˜ê³  ìœ ìš©í•œ ê´€ê´‘ì§€ ì¶”ì²œì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ================================\n",
    "# ë³´ë„ˆìŠ¤: ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì œ\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì œ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ì˜ˆì œ 1: ê°„ë‹¨í•œ ì¶”ì²œ\n",
    "print(\"\\nğŸ“ ì˜ˆì œ 1: ê°„ë‹¨í•œ ì¶”ì²œ\")\n",
    "simple_input = {\"free_text\": \"ë´„ì— ì‚°ì—ì„œ íë§\"}\n",
    "simple_result = recommender.recommend_places(simple_input, top_k=3)\n",
    "print(f\"ì…ë ¥: {simple_input['free_text']}\")\n",
    "print(\"ì¶”ì²œ ê²°ê³¼:\")\n",
    "for i, place in enumerate(simple_result['recommendations']):\n",
    "    print(f\"  {i+1}. {place['name']} (ì ìˆ˜: {place['hybrid_score']:.3f})\")\n",
    "\n",
    "# ì˜ˆì œ 2: íƒœê·¸ ì¡°í•© ì¶”ì²œ\n",
    "print(\"\\nğŸ“ ì˜ˆì œ 2: íƒœê·¸ ì¡°í•© ì¶”ì²œ\")\n",
    "tag_input = {\"season\": \"ì—¬ë¦„\", \"nature\": [\"ë°”ë‹¤\"], \"target\": [\"ê°€ì¡±\"]}\n",
    "tag_result = recommender.recommend_places(tag_input, top_k=3)\n",
    "print(f\"ì…ë ¥: {tag_input}\")\n",
    "print(\"ì¶”ì²œ ê²°ê³¼:\")\n",
    "for i, place in enumerate(tag_result['recommendations']):\n",
    "    print(f\"  {i+1}. {place['name']} (ì ìˆ˜: {place['hybrid_score']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ë§ˆìŒê» ì‚¬ìš©í•˜ì„¸ìš”!\")\n",
    "print(\"=\"*60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea4517-59d3-4c5f-be74-c3a011ebe00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
