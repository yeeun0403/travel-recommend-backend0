{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76b5130-236d-4f97-bee8-2c1e15b081bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: requests in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (80.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "WARNING:tensorflow:From C:\\Users\\tjdwl\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 라이브러리 설치 및 임포트\n",
    "## 필요한 라이버러리들이 없는 경우 아래 명령어로 설치\n",
    "!pip install sentence-transformers xgboost scikit-learn pandas numpy joblib pyyaml tqdm\n",
    "!pip install tf-keras\n",
    "!pip install sentence-transformers\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "import joblib\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## 머신러닝 라이브러리\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xgboost as xgb\n",
    "\n",
    "## 로깅 설정\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c82b16f9-6612-417b-bccb-62dbbbcbfda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 프로젝트 디렉토리 구조가 준비되었습니다.\n",
      "   (폴더가 없다면 위의 주석을 해제하고 실행하세요)\n"
     ]
    }
   ],
   "source": [
    "# 폴더가 이미 만들어져 있다면 아래 코드는 실행하지 않아도 됩니다.\n",
    "# 필요시 주석을 해제하고 실행하세요.\n",
    "\n",
    "# def create_project_structure():\n",
    "#     \"\"\"프로젝트 디렉토리 구조를 생성합니다.\"\"\"\n",
    "#     \n",
    "#     directories = [\n",
    "#         'data/raw',\n",
    "#         'data/processed', \n",
    "#         'data/embeddings',\n",
    "#         'models/xgboost',\n",
    "#         'models/encoders',\n",
    "#         'src',\n",
    "#         'notebooks',\n",
    "#         'saved_models',\n",
    "#         'config',\n",
    "#         'logs'\n",
    "#     ]\n",
    "#     \n",
    "#     for directory in directories:\n",
    "#         Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "#     \n",
    "#     print(\"✅ 프로젝트 디렉토리 구조 생성 완료\")\n",
    "\n",
    "# create_project_structure()  # 필요시 주석 해제\n",
    "\n",
    "print(\"📁 프로젝트 디렉토리 구조가 준비되었습니다.\")\n",
    "print(\"   (폴더가 없다면 위의 주석을 해제하고 실행하세요)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ba1325-1757-4c23-95c2-0ae84b5f2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 설정 파일 생성\n",
    "\n",
    "config = {\n",
    "    'model' : {\n",
    "        'sbert_model' : 'snunlp/KR-SBERT-V40K-klueNLI-augSTS',\n",
    "        'embedding_dim' : 768,\n",
    "        'reduced_dim' : 128,\n",
    "        'dimensionality_reduction': 'PCA' ,\n",
    "        'xgboost_params' : {\n",
    "            'max_depth' : 6,\n",
    "            'learning_rate' : 0.1,\n",
    "            'n_estimators' : 100,\n",
    "            'random_state' : 42\n",
    "        }\n",
    "    },\n",
    "    'data' : {\n",
    "        'raw_file' : 'data/raw/gangwon_places_100.xlsx',\n",
    "        'processed_file' : 'data/processed/gangwon_places_100_processed.xlsx',\n",
    "        'embeddings_file' : 'data/embeddings/place_embeddings_pca128.npy'\n",
    "    },\n",
    "    'paths': {\n",
    "        'models' : 'models',\n",
    "        'encoders' : 'models/encoders',\n",
    "        'logs' : 'logs'\n",
    "    }\n",
    "}\n",
    "\n",
    "## config 폴더가 없으면 생성\n",
    "os.makedirs('config', exist_ok=True)\n",
    "\n",
    "## 설정 파일 저장\n",
    "with open('config/config.yaml', 'w', encoding='utf-8') as f: \n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac4009e-fadd-425f-9ac0-d59c3da4c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 전처리 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "## 데이터 전처리 함수 정의\n",
    "\n",
    "class DataPreprocessor: \n",
    "    \"\"\"데이터 전처리 클래스\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.season_encoder = None\n",
    "        self.nature_encoder = MultiLabelBinarizer()\n",
    "        self.vibe_encoder = MultiLabelBinarizer()\n",
    "        self.target_encoder = MultiLabelBinarizer()\n",
    "\n",
    "    def parse_multi_label_string(self, text: str) -> List[str]:\n",
    "        \"\"\"쉼표로 구분된 문자열을 리스트로 변환\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "\n",
    "        # 쉼표로 분리하고 공백 제거\n",
    "        items = [item.strip() for item in str(text).split(',')]\n",
    "        return [item for item in items if item] # 빈 문자열 제거\n",
    "\n",
    "    def _augment_single_row(self, row) -> str:\n",
    "        \"\"\"단일 행의 설명 텍스트 증강\"\"\"\n",
    "        original = str(row['short_description'])\n",
    "        \n",
    "        # 계절 정보\n",
    "        season_text = f\"이곳은 {row['season']}에 특히 아름답습니다.\"\n",
    "        \n",
    "        # 자연환경 정보\n",
    "        if row['nature_list']:\n",
    "            nature_text = f\"{', '.join(row['nature_list'])} 경관을 즐길 수 있습니다.\"\n",
    "        else:\n",
    "            nature_text = \"\"\n",
    "        \n",
    "        # 분위기 정보\n",
    "        if row['vibe_list']:\n",
    "            vibe_text = f\"{', '.join(row['vibe_list'])} 분위기로 좋습니다.\"\n",
    "        else:\n",
    "            vibe_text = \"\"\n",
    "        \n",
    "        # 대상 정보\n",
    "        if row['target_list']:\n",
    "            target_text = f\"{', '.join(row['target_list'])}에게 추천합니다.\"\n",
    "        else:\n",
    "            target_text = \"\"\n",
    "        \n",
    "        # 모든 정보 결합\n",
    "        augmented = f\"{original} {season_text} {nature_text} {vibe_text} {target_text}\"\n",
    "        \n",
    "        return augmented.strip()\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame: \n",
    "        \"\"\"데이터 전처리 메인 함수\"\"\"\n",
    "        # 복사본 생성\n",
    "        processed_df = df.copy()\n",
    "\n",
    "        # 필수 컬럼 확인\n",
    "        required_cols = ['name', 'season', 'nature', 'vibe', 'target', 'short_description']\n",
    "        missing_cols = [col for col in required_cols if col not in processed_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"필수 컬럼이 없습니다: {missing_cols}\")\n",
    "\n",
    "        # 결측치 처리\n",
    "        processed_df['short_description'] = processed_df['short_description'].fillna('')\n",
    "        processed_df['season'] = processed_df['season'].fillna('사계절')\n",
    "        processed_df['nature'] = processed_df['nature'].fillna('')\n",
    "        processed_df['vibe'] = processed_df['vibe'].fillna('')\n",
    "        processed_df['target'] = processed_df['target'].fillna('')\n",
    "\n",
    "        # 다중 라벨 파싱\n",
    "        processed_df['nature_list'] = processed_df['nature'].apply(self.parse_multi_label_string)\n",
    "        processed_df['vibe_list'] = processed_df['vibe'].apply(self.parse_multi_label_string)\n",
    "        processed_df['target_list'] = processed_df['target'].apply(self.parse_multi_label_string)\n",
    "\n",
    "        # 텍스트 정규화\n",
    "        processed_df['short_description'] = processed_df['short_description'].apply(\n",
    "        lambda x: re.sub(r'[^\\w\\s]', '', str(x)) if pd.notna(x) else ''\n",
    "        )\n",
    "        return processed_df\n",
    "\n",
    "        # ✨ 새로 추가: 데이터 증강\n",
    "        print(\"📝 데이터 증강 중...\")\n",
    "        processed_df['enhanced_description'] = processed_df.apply(\n",
    "            self._augment_single_row, axis=1\n",
    "        )\n",
    "        print(f\"✅ 데이터 증강 완료! ({len(processed_df)}개)\")\n",
    "        \n",
    "        return processed_df\n",
    "        \n",
    "    def fit_encoders(self, df: pd.DataFrame):\n",
    "        \"\"\"인코더들을 학습 데이터에 맞춤\"\"\"\n",
    "\n",
    "        # 계절은 단일 라벨이므로 LabelEncoder 대신 직접 처리\n",
    "        self.season_categories = sorted(df['season'].unique())\n",
    "\n",
    "        # 다중 라벨 인코더 학습\n",
    "        self.nature_encoder.fit(df['nature_list'])\n",
    "        self.vibe_encoder.fit(df['vibe_list'])\n",
    "        self.target_encoder.fit(df['target_list'])\n",
    "\n",
    "        print(f\"인코더 학습 완료\")\n",
    "        print(f\"   - 계절 카테고리: {self.season_categories}\")\n",
    "        print(f\"   - 자연환경 카테고리: {len(self.nature_encoder.classes_)}개\")\n",
    "        print(f\"   - 분위기 카테고리: {len(self.vibe_encoder.classes_)}개\")\n",
    "        print(f\"   - 대상 카테고리: {len(self.target_encoder.classes_)}개\")\n",
    "\n",
    "    def encode_labels(self, df: pd.DataFrame) -> Dict[str,np.ndarray]:\n",
    "        \"\"\"라벨들을 인코딩\"\"\"\n",
    "\n",
    "        # 계절 인코딩(원-핫 인코딩)\n",
    "        season_encoded = np.zeros((len(df), len(self.season_categories)))\n",
    "        for i, season in enumerate(df['season']):\n",
    "            if season in self.season_categories:\n",
    "                season_idx = self.season_categories.index(season)\n",
    "                season_encoded[i, season_idx] = 1\n",
    "\n",
    "        # 다중 라벨 인코등\n",
    "        nature_encoded = self.nature_encoder.transform(df['nature_list'])\n",
    "        vibe_encoded = self.vibe_encoder.transform(df['vibe_list'])\n",
    "        target_encoded = self.target_encoder.transform(df['target_list'])\n",
    "\n",
    "        return{\n",
    "            'season' : season_encoded,\n",
    "            'nature' : nature_encoded,\n",
    "            'vibe' : vibe_encoded,\n",
    "            'target' : target_encoded\n",
    "        }\n",
    "\n",
    "    def save_encoders(self, base_path: str):\n",
    "        \"\"\"인코더들을 저장\"\"\"\n",
    "        # 계절 카테고리 저장\n",
    "        joblib.dump(self.season_categories, f\"{base_path}/season_encoder.joblib\")\n",
    "\n",
    "        # 다중 라벨 인코더 저장\n",
    "        joblib.dump(self.nature_encoder, f\"{base_path}/nature_encoder.joblib\")\n",
    "        joblib.dump(self.vibe_encoder, f\"{base_path}/vibe_encoder.joblib\")\n",
    "        joblib.dump(self.target_encoder, f\"{base_path}/target_encoder.joblib\")\n",
    "\n",
    "        print(f\"인코더 저장 완료: {base_path}\")\n",
    "\n",
    "    def load_encoders(self, base_path: str):\n",
    "        \"\"\"인코더 로드\"\"\"\n",
    "\n",
    "        self.season_categories = joblib.load(f\"{base_path}/season_encoder.joblib\")\n",
    "        self.nature_encoder = joblib.load(f\"{base_path}/nature_encoder.joblib\")\n",
    "        self.vibe_encoder = joblib.load(f\"{base_path}/vibe_encoder.joblib\")\n",
    "        self.target_encoder = joblib.load(f\"{base_path}/target_encoder.joblib\")\n",
    "\n",
    "        print(f\"인코더 로드 완료: {base_path}\")\n",
    "\n",
    "print(f\"데이터 전처리 클래스 정의 완료\")\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e775a6e-6fdb-4c22-a1c7-024a7e48df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 생성 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "## 임베딩 생성 클래스 정의\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"SBERT 임베딩 생성 및 차원 축소 클래스\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.dimension_reducer = None\n",
    "        self.reduced_dim = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"SBERT 모델 로드\"\"\"\n",
    "        print(f\"SBERT 모델 로드 중: {self.model_name}\")\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        print(\"SBERT 모델 로드 완료\")\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"텍스트 리스트로부터 임베딩 생성\n",
    "         Args:\n",
    "            texts: 텍스트 리스트\n",
    "            use_enhanced: True이면 가중치 적용, False면 기본 방식\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "        print(f\"임베딩 생성 중... (총 {len(texts)}개 텍스트)\")\n",
    "  \n",
    "         #  개선: 가중치 적용 옵션\n",
    "        if use_enhanced:\n",
    "            print(\" 가중치 적용 임베딩 생성 모드\")\n",
    "            # 각 텍스트를 3번 반복해서 중요도 증가 (내부적으로만)\n",
    "            # 하지만 실제로는 normalize로 동일한 효과\n",
    "            embeddings = self.model.encode(\n",
    "                texts,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=True,  # L2 정규화로 품질 향상\n",
    "                show_progress_bar=True,\n",
    "                batch_size=16  # 배치 크기 최적화\n",
    "            )\n",
    "        else:\n",
    "            # 기존 방식\n",
    "            batch_size = 32\n",
    "            embeddings = []\n",
    "            for i in tqdm(range(0, len(texts), batch_size)):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                batch_embeddings = self.model.encode(batch_texts, convert_to_numpy=True)\n",
    "                embeddings.append(batch_embeddings)\n",
    "            embeddings = np.vstack(embeddings)\n",
    "\n",
    "        print(f\"임베딩 생성 완료: {embeddings.shape}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def fit_dimension_reducer(self, embeddings: np.ndarray, method: str = 'PCA',\n",
    "                              target_dim: int = 128):\n",
    "        \"\"\"차원 축소 모델 학습\"\"\"\n",
    "\n",
    "        self.reduced_dim = target_dim\n",
    "\n",
    "        if method =='PCA':\n",
    "            self.dimension_reducer = PCA(n_components=target_dim, random_state=42)\n",
    "        elif method =='TruncatedSVD':\n",
    "            self.dimension_reducer = TruncatedSVD(n_components=target_dim, random_state=42)\n",
    "        else: \n",
    "            raise ValueError(f\"지원하지 않는 차원 축소 방법: {method}\")\n",
    "\n",
    "        print(f\"{method}를 사용하여 {embeddings.shape[1]}차원 -> {target_dim}차원으로 축소\")\n",
    "        self.dimension_reducer.fit(embeddings)\n",
    "\n",
    "        # 설명 분산 비율 출력(PCA의 경우)\n",
    "        if method =='PCA':\n",
    "            explained_variance_ratio = self.dimension_reducer.explained_variance_ratio_\n",
    "            cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "            print(f\"설명 분산 비율: {cumulative_variance[-1]:.4f}\")\n",
    "\n",
    "        print(f\"차원 축소 모델 학습 완료\")\n",
    "\n",
    "    def reduce_dimensions(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"임베딩 차원 축소\"\"\"\n",
    "\n",
    "        if self.dimension_reducer is None:\n",
    "            raise ValueError(\"차원 축소 모델이 학습되지 않았습니다.\")\n",
    "\n",
    "        reduced_embeddings = self.dimension_reducer.transform(embeddings)\n",
    "        print(f\"차원 축소 완료: {embeddings.shape} -> {reduced_embeddings.shape}\")\n",
    "\n",
    "        return reduced_embeddings\n",
    "\n",
    "    def save_dimension_reducer(self, filepath: str):\n",
    "        \"\"\"차원 축소 모델 저장\"\"\"\n",
    "\n",
    "        model_data = {\n",
    "        'reducer': self.dimension_reducer,\n",
    "        'reduced_dim' : self.reduced_dim,\n",
    "        'model_name' : self.model_name\n",
    "        }\n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"차원 축소 모델 저장: {filepath}\")\n",
    "\n",
    "    def load_dimension_reducer(self, filepath: str):\n",
    "        \"\"\"차원 축소 모델 로드\"\"\"\n",
    "\n",
    "        model_data = joblib.load(filepath)\n",
    "        self.dimension_reducer = model_data['reducer']\n",
    "        self.reduced_dim = model_data['reduced_dim']\n",
    "        self.model_name = model_data['model_name']\n",
    "\n",
    "        print(f\"차원 축소 모델 로드: {filepath}\")\n",
    "\n",
    "print(\"임베딩 생성 클래스 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232b2813-e02a-4589-9a3e-25c354b01830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 학습 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "### XGBoost 학습 클래스 정의\n",
    "\n",
    "class XGBoostTrainer:\n",
    "    \"\"\"XGBoost 분류기 학습 클래스\"\"\"\n",
    "\n",
    "    def __init__(self, xgb_params: Dict):\n",
    "        self.xgb_params = xgb_params\n",
    "        self.models = {}\n",
    "        self.label_types = ['season', 'nature', 'vibe', 'target']\n",
    "\n",
    "    def train_models(self, feature: np.ndarray, labels: Dict[str, np.ndarray]):\n",
    "        \"\"\"모든 라벨 타입에 대해 분류기 학습\"\"\"\n",
    "\n",
    "        print(\"XGBoost 모델 학습 시작...\")\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            print(f\"\\n{label_type} 분류기 학습 중...\")\n",
    "\n",
    "            y = labels[label_type]\n",
    "\n",
    "            if label_type == 'season':\n",
    "                #단일 라벨: 원-핫에서 클래스 인덱스로 변환\n",
    "                y_single = np.argmax(y, axis=1)\n",
    "\n",
    "                model = xgb.XGBClassifier(**self.xgb_params)\n",
    "                model.fit(features, y_single)\n",
    "                           \n",
    "            else: \n",
    "                #다중 라벨: OneVsRestClassifier 사용\n",
    "                model = OneVsRestClassifier(\n",
    "                    xgb.XGBClassifier(**self.xgb_params)\n",
    "                )\n",
    "                model.fit(features, y)\n",
    "\n",
    "            self.models[label_type] = model\n",
    "            print(f\"모든 XGBoost 모델 학습 완료\")\n",
    "\n",
    "    def evaluate_models(self, features: np.ndarray, labels: Dict[str,np.ndarray]):\n",
    "        \"\"\"모델 성능 평가\"\"\"\n",
    "\n",
    "        print(\"\\n=== 모델 성능 평가===\")\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            print(f\"\\n[{label_type}] 성능 평가: \")\n",
    "\n",
    "\n",
    "            y_true = labels[label_type]\n",
    "            model = self.models[label_type]\n",
    "\n",
    "            if label_type == 'season':\n",
    "                # 단일 라벨 평가\n",
    "\n",
    "                y_true_single = np.argmax(y_true, axis=1)\n",
    "                y_pred = model.predict(features)\n",
    "\n",
    "                accuracy = accuracy_score(y_true_single, y_pred)\n",
    "                f1 = f1_score(y_true_single, y_pred, average='weighted')\n",
    "\n",
    "                print(f\"Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "            else: \n",
    "                # 다중 라벨 평가\n",
    "                y_pred = model.predict(features)\n",
    "\n",
    "                accuracy = accuracy_score(y_true, y_pred)\n",
    "                f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "                f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "                print(f\"Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"F1-Score (Micro): {f1_micro:.4f}\")\n",
    "                print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
    "                \n",
    "    def save_models(self,base_path: str):\n",
    "        \"\"\"모델들 저장\"\"\"\n",
    "        for label_type in self.label_types:\n",
    "            model_path = f\"{base_path}/xgboost/{label_type}_model.joblib\"\n",
    "            joblib.dump(self.models[label_type], model_path)\n",
    "            print(f\"{label_type} 모델 저장: {model_path}\")\n",
    "\n",
    "    def load_models(self,base_path: str):\n",
    "        \"\"\"모델들 로드\"\"\"\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            model_path = f\"{base_path}/xgboost/{label_type}_model.joblib\"\n",
    "            self.models[label_type] = joblib.load(model_path)\n",
    "            print(f\"{label_type} 모델 로드: {model_path}\")\n",
    "\n",
    "print(\"XGBoost 학습 클래스 정의 완료\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5e54007-c9a4-41e9-90c0-cb6ee76bd9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 수정된 추천 시스템 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "## 추천 시스템 클래스 정의\n",
    "\n",
    "# 새로운 셀에서 GangwonPlaceRecommender 클래스 재정의\n",
    "class GangwonPlaceRecommender:\n",
    "    \"\"\"강원도 관광지 추천 시스템 메인 클래스 (수정된 버전)\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = 'config/config.yaml'):\n",
    "        # 설정 로드\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        \n",
    "        # 컴포넌트 초기화\n",
    "        self.preprocessor = DataPreprocessor()\n",
    "        self.embedding_generator = EmbeddingGenerator(\n",
    "            self.config['model']['sbert_model']\n",
    "        )\n",
    "        self.xgb_trainer = XGBoostTrainer(\n",
    "            self.config['model']['xgboost_params']\n",
    "        )\n",
    "        \n",
    "        # 데이터 저장용\n",
    "        self.df = None\n",
    "        self.place_embeddings = None\n",
    "        self.place_names = None\n",
    "        \n",
    "        # 태그 매핑 (자유 문장 파싱용)\n",
    "        self.tag_mapping = {\n",
    "            'season': {\n",
    "                '봄': ['봄', '3월', '4월', '5월', '벚꽃', '꽃'],\n",
    "                '여름': ['여름', '6월', '7월', '8월', '바다', '해변', '시원', '물'],\n",
    "                '가을': ['가을', '9월', '10월', '11월', '단풍', '억새', '빨간'],\n",
    "                '겨울': ['겨울', '12월', '1월', '2월', '눈', '스키', '추운'],\n",
    "                '사계절': ['사계절', '연중', '언제나']\n",
    "            },\n",
    "            'nature': {\n",
    "                '산': ['산', '등산', '트레킹', '하이킹', '산책', '오르막'],\n",
    "                '바다': ['바다', '해변', '바닷가', '수영', '파도'],\n",
    "                '호수': ['호수', '연못', '물가', '저수지'],\n",
    "                '계곡': ['계곡', '시냇물', '개울', '물소리'],\n",
    "                '자연': ['자연', '숲', '나무', '풀', '식물'],\n",
    "                '도시': ['도시', '시내', '번화가', '상점']\n",
    "            },\n",
    "            'vibe': {\n",
    "                '감성': ['감성', '감성적', '로맨틱', '낭만', '예쁜'],\n",
    "                '활력': ['활력', '활기', '신나는', '즐거운', '재미'],\n",
    "                '휴식': ['휴식', '쉬는', '편안', '조용', '평온', '힐링'],\n",
    "                '산책': ['산책', '걷기', '거닐기', '천천히'],\n",
    "                '모험': ['모험', '스릴', '도전', '익스트림']\n",
    "            },\n",
    "            'target': {\n",
    "                '연인': ['연인', '커플', '남친', '여친', '애인'],\n",
    "                '가족': ['가족', '부모', '아이', '자녀', '아기'],\n",
    "                '친구': ['친구', '친구들', '동료', '같이'],\n",
    "                '혼자': ['혼자', '나만', '단독', '솔로']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def parse_user_input(self, user_input: Dict) -> Dict:\n",
    "        \"\"\"사용자 입력을 파싱하여 표준화된 형태로 변환\"\"\"\n",
    "        \n",
    "        parsed = {\n",
    "            'season': None,\n",
    "            'nature': [],\n",
    "            'vibe': [],\n",
    "            'target': []\n",
    "        }\n",
    "        \n",
    "        # 자유 문장 입력 처리\n",
    "        if 'free_text' in user_input:\n",
    "            text = user_input['free_text'].lower()\n",
    "            \n",
    "            # 각 태그 카테고리별로 매칭\n",
    "            for category, tag_dict in self.tag_mapping.items():\n",
    "                for tag, keywords in tag_dict.items():\n",
    "                    if any(keyword in text for keyword in keywords):\n",
    "                        if category == 'season':\n",
    "                            parsed['season'] = tag\n",
    "                        else:\n",
    "                            if tag not in parsed[category]:\n",
    "                                parsed[category].append(tag)\n",
    "        \n",
    "        # 직접 태그 입력 처리\n",
    "        else:\n",
    "            if 'season' in user_input:\n",
    "                parsed['season'] = user_input['season']\n",
    "            \n",
    "            for category in ['nature', 'vibe', 'target']:\n",
    "                if category in user_input:\n",
    "                    if isinstance(user_input[category], list):\n",
    "                        parsed[category] = user_input[category]\n",
    "                    else:\n",
    "                        parsed[category] = [user_input[category]]\n",
    "        \n",
    "        return parsed\n",
    "\n",
    "    def _calculate_advanced_tag_scores(self, user_input: Dict, df_row) -> float:\n",
    "        \"\"\"\n",
    "        ✨ 개선된 태그 매칭 스코어 (Jaccard + F1 결합)\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Season 매칭 (가중치 0.3)\n",
    "        if user_input.get('season') == df_row['season']:\n",
    "            score += 0.3\n",
    "        \n",
    "        # Nature 매칭 (가중치 0.25) - Jaccard + F1\n",
    "        if 'nature' in user_input and user_input['nature']:\n",
    "            user_set = set(user_input['nature'])\n",
    "            place_set = set(df_row['nature_list'])\n",
    "            \n",
    "            if user_set and place_set:\n",
    "                intersection = len(user_set & place_set)\n",
    "                union = len(user_set | place_set)\n",
    "                jaccard = intersection / union if union > 0 else 0\n",
    "                \n",
    "                precision = intersection / len(place_set) if place_set else 0\n",
    "                recall = intersection / len(user_set) if user_set else 0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                \n",
    "                score += 0.25 * (0.6 * jaccard + 0.4 * f1)\n",
    "        \n",
    "        # Vibe 매칭 (가중치 0.25) - Jaccard\n",
    "        if 'vibe' in user_input and user_input['vibe']:\n",
    "            user_set = set(user_input['vibe'])\n",
    "            place_set = set(df_row['vibe_list'])\n",
    "            \n",
    "            if user_set and place_set:\n",
    "                intersection = len(user_set & place_set)\n",
    "                union = len(user_set | place_set)\n",
    "                jaccard = intersection / union if union > 0 else 0\n",
    "                score += 0.25 * jaccard\n",
    "        \n",
    "        # Target 매칭 (가중치 0.2)\n",
    "        if 'target' in user_input and user_input['target']:\n",
    "            user_set = set(user_input['target'])\n",
    "            place_set = set(df_row['target_list'])\n",
    "            \n",
    "            if user_set and place_set:\n",
    "                intersection = len(user_set & place_set)\n",
    "                score += 0.2 * (intersection / len(user_set))\n",
    "        \n",
    "        return score\n",
    "        \n",
    "    def calculate_hybrid_score(self, user_input: Dict, \n",
    "                             similarity_weight: float = 0.6,\n",
    "                             tag_weight: float = 0.4) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"✨ 개선된 하이브리드 스코어 계산\"\"\"\n",
    "        \n",
    "        # 사용자 입력 파싱\n",
    "        parsed_input = self.parse_user_input(user_input)\n",
    "        \n",
    "        # 1. 텍스트 유사도 점수 계산\n",
    "        if 'free_text' in user_input:\n",
    "            query_text = user_input['free_text']\n",
    "        else:\n",
    "            # 태그를 문장으로 변환\n",
    "            query_parts = []\n",
    "            if parsed_input['season']:\n",
    "                query_parts.append(f\"{parsed_input['season']}에\")\n",
    "            if parsed_input['target']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['target'])}와\")\n",
    "            if parsed_input['nature']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['nature'])}에서\")\n",
    "            if parsed_input['vibe']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['vibe'])} 여행\")\n",
    "            \n",
    "            query_text = ' '.join(query_parts)\n",
    "        \n",
    "        # 쿼리 임베딩 생성\n",
    "        if self.embedding_generator.model is None:\n",
    "            self.embedding_generator.load_model()\n",
    "        \n",
    "        query_embedding = self.embedding_generator.model.encode([query_text])\n",
    "        \n",
    "        # 코사인 유사도 계산 - [0] 인덱스로 1차원 배열로 변환\n",
    "        similarity_scores = cosine_similarity(\n",
    "            query_embedding, \n",
    "            self.place_embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # 2. ✨ 개선된 태그 매칭 점수 계산\n",
    "        tag_scores = np.zeros(len(self.df))\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            tag_scores[idx] = self._calculate_advanced_tag_scores(parsed_input, row)\n",
    "        \n",
    "        # 정규화\n",
    "        if tag_scores.max() > 0:\n",
    "            tag_scores = tag_scores / tag_scores.max()\n",
    "        \n",
    "        # 3. 하이브리드 점수 계산\n",
    "        hybrid_scores = (\n",
    "            similarity_weight * similarity_scores +\n",
    "            tag_weight * tag_scores\n",
    "        )\n",
    "        \n",
    "        return hybrid_scores, similarity_scores, tag_scores\n",
    "    \n",
    "    def recommend_places(self, user_input: Dict, top_k: int = 10) -> Dict:\n",
    "        \"\"\"관광지 추천 메인 함수\"\"\"\n",
    "        \n",
    "        # 하이브리드 점수 계산\n",
    "        hybrid_scores, similarity_scores, tag_scores = self.calculate_hybrid_score(user_input)\n",
    "        \n",
    "        # 상위 k개 추천지 선택\n",
    "        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "        \n",
    "        # 추천 결과 구성\n",
    "        recommendations = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            place_info = {\n",
    "                'name': self.df.iloc[idx]['name'],\n",
    "                'season': self.df.iloc[idx]['season'],\n",
    "                'nature': self.df.iloc[idx]['nature_list'],\n",
    "                'vibe': self.df.iloc[idx]['vibe_list'],\n",
    "                'target': self.df.iloc[idx]['target_list'],\n",
    "                'description': self.df.iloc[idx]['short_description'],\n",
    "                'hybrid_score': float(hybrid_scores[idx]),\n",
    "                'similarity_score': float(similarity_scores[idx]),\n",
    "                'tag_score': float(tag_scores[idx])\n",
    "            }\n",
    "            recommendations.append(place_info)\n",
    "        \n",
    "        # 파싱된 사용자 입력 정보 추가\n",
    "        parsed_input = self.parse_user_input(user_input)\n",
    "        \n",
    "        result = {\n",
    "            'user_input': user_input,\n",
    "            'parsed_input': parsed_input,\n",
    "            'recommendations': recommendations,\n",
    "            'total_places': len(self.df)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"✅ 수정된 추천 시스템 클래스 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef561287-776f-4f82-9f5e-f2adba3a736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실제 csv 파일 로드 및 검증\n",
    "\n",
    "def load_and_validate_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"실제 CSV 파일 로드 및 검증\"\"\"\n",
    "\n",
    "    # 업로드된 CSV 파일 읽기\n",
    "    try: \n",
    "        #  파일이 업로드되어 있는지 확인하고 data/raw 폴더로 복사\n",
    "        if os.path.exists('gangwon_places_1000.xlsx'):\n",
    "            # 현재 디렉토리에 있는 파일을 data/raw 폴더로 복사\n",
    "            os.makedirs('data/raw', exist_ok=True)\n",
    "            import shutil\n",
    "            shutil.copy('gangwon_places_1000.xlsx', 'data/raw/gangwon_places_1000.xlsx')\n",
    "            print(\"CSV 파일을 data/raw 폴더로 복사 완료\")\n",
    "\n",
    "        # CSV 파이 로드\n",
    "        df = pd.read_csv('data/raw/gangwon_places_1000.xlsx', encoding='utf-8')\n",
    "        print(f\"✅ CSV 파일 로드 완료: {df.shape}\")\n",
    "\n",
    "        # 컬럼 정보 출력\n",
    "        print(f\"컬럼 정보: {df.columns.tolist()}\")\n",
    "\n",
    "        # 필수 컬럼 검증\n",
    "        required_columns = ['name', 'season', 'nature', 'vibe', 'target', 'short_description']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"⚠️  필수 컬럼 누락: {missing_columns}\")\n",
    "            print(\"데이터 구조를 확인하고 수정이 필요합니다.\")\n",
    "        else:\n",
    "            print(\"✅ 모든 필수 컬럼이 존재합니다.\")\n",
    "\n",
    "        # 데이터 타입 및 결측치 정보 출력\n",
    "        print(f\"\\n데이터 정보: \")\n",
    "        print(f\"- 총 행 수: {len(df)}\")\n",
    "        print(f\"- 결측치 현황: \")\n",
    "        for col in required_columns:\n",
    "            if col in df.columns: \n",
    "                missing_count = df[col].isnull().sum()\n",
    "                missing_pct = (missing_count / len(df)) * 100\n",
    "                print(f\" {col}: {missing_couint}개 {missing_pct:.1f}%)\")\n",
    "\n",
    "\n",
    "        # 샘플 데이터 확인\n",
    "        print(f\"\\n 샘플 데이터 (상위 3개): \")\n",
    "        for idx, row in df.head(3).iterrows():\n",
    "            print(f\"\\n{idx+1}. {row.get('name', 'N/A')}\")\n",
    "            print(f\"계절: {row.get('season', 'N/A')}\")\n",
    "            print(f\"자연 환경:  {row.get('nature', 'N/A')}\")\n",
    "            print(f\"분위기:  {row.get('vibe', 'N/A')}\")\n",
    "            print(f\"대상:  {row.get('vibe', 'N/A')}\")\n",
    "            print(f\"설명:  {row.get('short_description', 'N/A')[:50]}...\")\n",
    "\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"gangwon_places_1000.xlsx 파일을 찾을 수 없습니다.\")\n",
    "        print(\"파일을 현재 디렉토리에 업로드하거나 data/raw/ 폴더에 저장해주세요.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파일 로드 중 오류 발생: {str(e)}\")\n",
    "        return None\n",
    "    # 실제 CSV 파일 로드\n",
    "    print(\"== 실제 CSV 파일 로드 ===\")\n",
    "    df_loaded = load_and_validate_csv('data/raw/gangwon_places_1000.xlsx')\n",
    "\n",
    "    if df_loaded is not None:\n",
    "        print(\"\\n 실제 데이터 파일 로드 성공\")\n",
    "    else:\n",
    "        print(\"\\n 데이터 파일 로드 실패 - 프로그램을 종료합니다.\")\n",
    "        #실제 Jupyter 환경에서는 다음 셀 실행을 중단하거나 오류 처리를 추가할 수 있습니다.\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a52475d3-0a3c-43c4-8849-7588f03fb598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 실제 데이터 로드 및 전처리 ===\n",
      "📊 Excel 파일 로드 중: data/raw/gangwon_places_1000.xlsx\n",
      "✅ Excel 파일 로드 성공!\n",
      "\n",
      "원본 데이터: (1000, 13)\n",
      "컬럼: ['name', 'season', 'nature', 'vibe', 'target', 'fee', 'parking', 'address', 'open_time', 'latitude', 'longitude', 'full_address', 'short_description']\n",
      "\n",
      "📋 샘플 데이터 (첫 3행):\n",
      "\n",
      "1. 강릉 모래내 한과마을(갈골한과)\n",
      "   계절: 사계절\n",
      "   자연: 산, 호수\n",
      "   분위기: 액티비티, 역사\n",
      "   대상: 가족\n",
      "\n",
      "2. 국립 삼봉자연휴양림\n",
      "   계절: 봄\n",
      "   자연: 산, 자연, 호수\n",
      "   분위기: 산책, 액티비티, 힐링\n",
      "   대상: 가족\n",
      "\n",
      "3. 설악산국립공원(내설악)\n",
      "   계절: 여름\n",
      "   자연: 산, 자연\n",
      "   분위기: 산책\n",
      "   대상: nan\n",
      "\n",
      "📊 실제 데이터 정보:\n",
      "총 관광지 수: 1000\n",
      "전체 컬럼 수: 13\n",
      "\n",
      "- season 카테고리: 11개 종류\n",
      "  예시: ['사계절', '봄', '여름', '가을', '겨울']\n",
      "\n",
      "- nature 카테고리: 29개 종류\n",
      "  예시: ['산, 호수', '산, 자연, 호수', '산, 자연', '바다, 산, 자연', '바다, 산']\n",
      "\n",
      "- vibe 카테고리: 43개 종류\n",
      "  예시: ['액티비티, 역사', '산책, 액티비티, 힐링', '산책', '사진명소, 산책, 역사', '산책, 역사']\n",
      "\n",
      "- target 카테고리: 7개 종류\n",
      "  예시: ['가족', '친구', '연인', '연인, 친구', '가족, 친구']\n",
      "\n",
      "============================================================\n",
      "🔧 데이터 전처리 시작\n",
      "============================================================\n",
      "\n",
      "✅ 전처리 완료: (1000, 16)\n",
      "\n",
      "📋 전처리 결과 샘플 (상위 3개):\n",
      "\n",
      "1. 강릉 모래내 한과마을(갈골한과)\n",
      "   계절: 사계절\n",
      "   자연환경 (리스트): ['산', '호수']\n",
      "   분위기 (리스트): ['액티비티', '역사']\n",
      "   대상 (리스트): ['가족']\n",
      "   설명: 강릉 모래내 한과마을갈골한과은는 사계절에 특히 아름다워 산 경관이 뛰어나며 액티비티 분위기...\n",
      "\n",
      "2. 국립 삼봉자연휴양림\n",
      "   계절: 봄\n",
      "   자연환경 (리스트): ['산', '자연', '호수']\n",
      "   분위기 (리스트): ['산책', '액티비티', '힐링']\n",
      "   대상 (리스트): ['가족']\n",
      "   설명: 국립 삼봉자연휴양림은는 봄에 특히 아름다워 산 경관이 뛰어나며 산책 분위기로 가족에게 추천...\n",
      "\n",
      "3. 설악산국립공원(내설악)\n",
      "   계절: 여름\n",
      "   자연환경 (리스트): ['산', '자연']\n",
      "   분위기 (리스트): ['산책']\n",
      "   대상 (리스트): []\n",
      "   설명: 설악산국립공원내설악은는 여름에 특히 아름다워 산 경관이 뛰어나며 산책 분위기로 모두에게 추...\n",
      "\n",
      "============================================================\n",
      "🎓 인코더 학습 시작\n",
      "============================================================\n",
      "인코더 학습 완료\n",
      "   - 계절 카테고리: ['가을', '겨울', '봄', '봄, 가을', '봄, 가을, 겨울', '봄, 가을, 사계절', '봄, 여름, 가을', '봄, 여름, 가을, 사계절', '사계절', '여름', '여름, 사계절']\n",
      "   - 자연환경 카테고리: 6개\n",
      "   - 분위기 카테고리: 8개\n",
      "   - 대상 카테고리: 3개\n",
      "\n",
      "🔢 라벨 인코딩 중...\n",
      "\n",
      "✅ 인코딩 결과:\n",
      "   - season: (1000, 11)\n",
      "   - nature: (1000, 6)\n",
      "   - vibe: (1000, 8)\n",
      "   - target: (1000, 3)\n",
      "\n",
      "💾 전처리된 데이터 저장 중...\n",
      "✅ 저장 완료: data/processed/gangwon_places_100_processed.csv\n",
      "\n",
      "============================================================\n",
      "🎉 데이터 로드 및 전처리 완료!\n",
      "============================================================\n",
      "✅ 총 1000개 관광지 데이터 준비 완료\n",
      "✅ 로드된 파일: data/raw/gangwon_places_1000.xlsx\n",
      "✅ 인코더 학습 완료: 4개 카테고리\n"
     ]
    }
   ],
   "source": [
    "# ## 전체 파이프라인 실행 - 데이터 로드 및 전처리\n",
    "\n",
    "# # 추천 시스템 인스턴스 생성\n",
    "# recommender = GangwonPlaceRecommender()\n",
    "\n",
    "# # 실제 데이터 로드(업로드된 CSV 파일 사용)\n",
    "# print(\"=== 실제 데이터 로드 및 전처리===\")\n",
    "\n",
    "# # 업로드된 파일을 data/raw로 복사 (파일이 현재 디렉토리에 있는 경우)\n",
    "# if os.path.exists('gangwon_places_100.xlsx'):\n",
    "#     import shutil\n",
    "#     shutil.copy('gangwon_places_100.csv', 'data/raw/gangwon_places_100.xlsx')\n",
    "#     print(\"✅ 업로드된 CSV 파일을 data/raw로 복사 완료\")\n",
    "\n",
    "# # CSV 파일 로드\n",
    "# df = pd.read_csv('data/raw/gangwon_places_100.xlsx', encoding='utf-8')\n",
    "# print(f\"원본 데이터: {df.shape}\")\n",
    "# print(f\"컬럼: {df.columns.tolist()}\")\n",
    "\n",
    "# # 추가 컬럼 정보 출력\n",
    "# print(f\"\\n 실제 데이터 정보:\")\n",
    "# print(f\"총 관광지 수: {len(df)}\")\n",
    "# print(f\"전체 컬럼 수: {len(df.columns)}\")\n",
    "\n",
    "# # 각 카테고리별 고유값 확인\n",
    "# categorical_columns = ['season', 'nature', 'vibe', 'target']\n",
    "# for col in categorical_columns:\n",
    "#     if col in df.columns:\n",
    "#         unique_values = df[col].dropna().unique()\n",
    "#         print(f\"-{col} 카테고리: {len(unique_values)}개 종류\")\n",
    "#         print(f\" 예시: {list(unique_values)[:5]}\")\n",
    "\n",
    "# # 데이터 전처리\n",
    "# processed_df = recommender.preprocessor.preprocess_data(df)\n",
    "# print(f\"\\n 전처리 된 데이터: {processed_df.shape}\")\n",
    "\n",
    "# # 전처리 결과 확인\n",
    "# print(f\"\\n 전처리 결과 샘플 (상위 3개):\")\n",
    "# for idx,row in processed_df.head(3).iterrows():\n",
    "#     print(f\"\\n{idx+1}. {row['name']}\")\n",
    "#     print(f\"   계절: {row['season']}\")\n",
    "#     print(f\"   자연환경 (리스트): {row['nature_list']}\")\n",
    "#     print(f\"   분위기 (리스트): {row['vibe_list']}\")\n",
    "#     print(f\"   대상 (리스트): {row['target_list']}\")\n",
    "#     print(f\"   설명: {row['short_description'][:50]}...\")\n",
    "\n",
    "# # 인코더 학습\n",
    "# recommender.preprocessor.fit_encoders(processed_df)\n",
    "\n",
    "# # 라벨 인코딩\n",
    "# encoded_labels = recommender.preprocessor.encode_labels(processed_df)\n",
    "\n",
    "# # 인코딩 결과 확인\n",
    "# print(f\"\\n 인코딩 결과:\") \n",
    "\n",
    "# # 전처리된 데이터 저장\n",
    "# processed_df.to_csv('data/processed/gangwon_places_100_processed.xlsx', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# # 추천 시스템에 데이터 저장\n",
    "# recommender.df = processed_df\n",
    "# recommender.place_names = processed_df['name'].tolist()\n",
    "\n",
    "## 전체 파이프라인 실행 - 데이터 로드 및 전처리\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 추천 시스템 인스턴스 생성\n",
    "recommender = GangwonPlaceRecommender()\n",
    "\n",
    "print(\"=== 실제 데이터 로드 및 전처리 ===\")\n",
    "\n",
    "# 디렉토리 확인\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Excel 파일 로드 (data/raw/gangwon_places_100.xlsx)\n",
    "file_path = 'data/raw/gangwon_places_1000.xlsx'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"파일을 찾을 수 없습니다: {file_path}\")\n",
    "\n",
    "print(f\"📊 Excel 파일 로드 중: {file_path}\")\n",
    "df = pd.read_excel(file_path)\n",
    "print(f\"✅ Excel 파일 로드 성공!\")\n",
    "\n",
    "print(f\"\\n원본 데이터: {df.shape}\")\n",
    "print(f\"컬럼: {df.columns.tolist()}\")\n",
    "\n",
    "# 샘플 데이터 확인\n",
    "print(f\"\\n📋 샘플 데이터 (첫 3행):\")\n",
    "for idx, row in df.head(3).iterrows():\n",
    "    print(f\"\\n{idx+1}. {row['name']}\")\n",
    "    print(f\"   계절: {row['season']}\")\n",
    "    print(f\"   자연: {row['nature']}\")\n",
    "    print(f\"   분위기: {row['vibe']}\")\n",
    "    print(f\"   대상: {row['target']}\")\n",
    "\n",
    "# 데이터 정보\n",
    "print(f\"\\n📊 실제 데이터 정보:\")\n",
    "print(f\"총 관광지 수: {len(df)}\")\n",
    "print(f\"전체 컬럼 수: {len(df.columns)}\")\n",
    "\n",
    "# 각 카테고리별 고유값 확인\n",
    "categorical_columns = ['season', 'nature', 'vibe', 'target']\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        unique_values = df[col].dropna().unique()\n",
    "        print(f\"\\n- {col} 카테고리: {len(unique_values)}개 종류\")\n",
    "        print(f\"  예시: {list(unique_values)[:5]}\")\n",
    "\n",
    "# 데이터 전처리\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔧 데이터 전처리 시작\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "processed_df = recommender.preprocessor.preprocess_data(df)\n",
    "print(f\"\\n✅ 전처리 완료: {processed_df.shape}\")\n",
    "\n",
    "# 전처리 결과 확인\n",
    "print(f\"\\n📋 전처리 결과 샘플 (상위 3개):\")\n",
    "for idx, row in processed_df.head(3).iterrows():\n",
    "    print(f\"\\n{idx+1}. {row['name']}\")\n",
    "    print(f\"   계절: {row['season']}\")\n",
    "    print(f\"   자연환경 (리스트): {row['nature_list']}\")\n",
    "    print(f\"   분위기 (리스트): {row['vibe_list']}\")\n",
    "    print(f\"   대상 (리스트): {row['target_list']}\")\n",
    "    print(f\"   설명: {row['short_description'][:50]}...\")\n",
    "\n",
    "# 인코더 학습\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎓 인코더 학습 시작\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommender.preprocessor.fit_encoders(processed_df)\n",
    "\n",
    "# 라벨 인코딩\n",
    "print(\"\\n🔢 라벨 인코딩 중...\")\n",
    "encoded_labels = recommender.preprocessor.encode_labels(processed_df)\n",
    "\n",
    "# 인코딩 결과 확인\n",
    "print(f\"\\n✅ 인코딩 결과:\")\n",
    "for key, value in encoded_labels.items():\n",
    "    print(f\"   - {key}: {value.shape}\")\n",
    "\n",
    "# 전처리된 데이터 저장\n",
    "print(\"\\n💾 전처리된 데이터 저장 중...\")\n",
    "processed_df.to_csv('data/processed/gangwon_places_100_processed.csv', \n",
    "                    index=False, \n",
    "                    encoding='utf-8-sig')\n",
    "print(\"✅ 저장 완료: data/processed/gangwon_places_100_processed.csv\")\n",
    "\n",
    "# 추천 시스템에 데이터 저장\n",
    "recommender.df = processed_df\n",
    "recommender.place_names = processed_df['name'].tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 데이터 로드 및 전처리 완료!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✅ 총 {len(recommender.df)}개 관광지 데이터 준비 완료\")\n",
    "print(f\"✅ 로드된 파일: {file_path}\")\n",
    "print(f\"✅ 인코더 학습 완료: {len(encoded_labels)}개 카테고리\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92183aa0-ffe8-4d25-b760-6c19a5e77179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 현재 컬럼 목록:\n",
      "['name', 'season', 'nature', 'vibe', 'target', 'fee', 'parking', 'address', 'open_time', 'latitude', 'longitude', 'full_address', 'short_description', 'nature_list', 'vibe_list', 'target_list']\n",
      "\n",
      "총 16개 컬럼\n"
     ]
    }
   ],
   "source": [
    "# 현재 DataFrame의 컬럼 확인\n",
    "print(\"📋 현재 컬럼 목록:\")\n",
    "print(processed_df.columns.tolist())\n",
    "print(f\"\\n총 {len(processed_df.columns)}개 컬럼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85166c33-9132-433e-975c-d5a6cecc4db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataPreprocessor 메서드 목록:\n",
      "['encode_labels', 'fit_encoders', 'load_encoders', 'parse_multi_label_string', 'preprocess_data', 'save_encoders']\n",
      "✅ _augment_single_row 메서드 존재\n"
     ]
    }
   ],
   "source": [
    "# DataPreprocessor에 augment 메서드가 있는지 확인\n",
    "import inspect\n",
    "\n",
    "print(\"DataPreprocessor 메서드 목록:\")\n",
    "methods = [m for m in dir(DataPreprocessor) if not m.startswith('_')]\n",
    "print(methods)\n",
    "\n",
    "# _augment_single_row 메서드 확인\n",
    "if hasattr(DataPreprocessor, '_augment_single_row'):\n",
    "    print(\"✅ _augment_single_row 메서드 존재\")\n",
    "else:\n",
    "    print(\"❌ _augment_single_row 메서드 없음 - 클래스 재정의 필요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c52ae69c-d465-4196-acac-111ee6df6c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🤖 SBERT 임베딩 생성 (개선 버전)\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'enhanced_description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'enhanced_description'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# ✨ 개선: enhanced_description 사용\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m texts \u001b[38;5;241m=\u001b[39m processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menhanced_description\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📝 텍스트 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📝 샘플 길이: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 글자\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'enhanced_description'"
     ]
    }
   ],
   "source": [
    "# ## SBERT 임베딩 생성(768차원 유지)\n",
    "# print(\"\\n SBERT 임베딩 생성 및 차원 축소\")\n",
    "\n",
    "# # 텍스트 리스트 준비\n",
    "# texts = processed_df['short_description'].tolist()\n",
    "\n",
    "# # SBERT 임베딩 생성\n",
    "# embeddings = recommender.embedding_generator.generate_embeddings(texts)\n",
    "\n",
    "# print(f\"📊 임베딩 형태: {embeddings.shape}\")\n",
    "# print(f\"💾 메모리 사용량: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# \"\"\"\n",
    "# # 차원 축소 모델 학습\n",
    "# recommender.embedding_generator.fit_dimension_reducer(\n",
    "#     embeddings,\n",
    "#     method = recommender.config['model']['dimensionality_reduction'],\n",
    "#     target_dim = recommender.config['model']['reduced_dim']\n",
    "# )\n",
    "# # 차원 축소 적용\n",
    "# reduced_embeddings = recommender.embedding_generator.reduce_dimensions(embeddings)\n",
    "# \"\"\"\n",
    "# # 차원 축소 없이 원본 768차원 사용\n",
    "# os.makedirs('data/embeddings', exist_ok=True)\n",
    "# np.save('data/embeddings/place_embeddings_full768.npy', embeddings)\n",
    "\n",
    "# # # 임베딩 저장 \n",
    "# # np.save('data/embeddings/place_embeddings_pca128.npy', reduced_embeddings)\n",
    "\n",
    "# # 추천 시스템에 임베딩 저장\n",
    "# recommender.place_embeddings = embeddings\n",
    "\n",
    "\n",
    "# print(\"✅ 768차원 임베딩 생성 및 저장 완료\")\n",
    "# print(f\"   파일 저장\")\n",
    "\n",
    "## SBERT 임베딩 생성(768차원 유지) - 개선 버전\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🤖 SBERT 임베딩 생성 (개선 버전)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ✨ 개선: enhanced_description 사용\n",
    "texts = processed_df['enhanced_description'].tolist()\n",
    "print(f\"📝 텍스트 수: {len(texts)}\")\n",
    "print(f\"📝 샘플 길이: {len(texts[0])} 글자\")\n",
    "\n",
    "# ✨ 개선: use_enhanced=True 옵션으로 품질 향상\n",
    "embeddings = recommender.embedding_generator.generate_embeddings(\n",
    "    texts, \n",
    "    use_enhanced=True  # 정규화 + 최적화된 배치 처리\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 임베딩 정보:\")\n",
    "print(f\"   - 형태: {embeddings.shape}\")\n",
    "print(f\"   - 차원: {embeddings.shape[1]}D (768차원 유지)\")\n",
    "print(f\"   - 메모리: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"   - 데이터 타입: {embeddings.dtype}\")\n",
    "\n",
    "\"\"\"\n",
    "# 차원 축소는 현재 사용하지 않음 (성능 저하 방지)\n",
    "# 필요시 아래 코드 활성화:\n",
    "recommender.embedding_generator.fit_dimension_reducer(\n",
    "    embeddings,\n",
    "    method = recommender.config['model']['dimensionality_reduction'],\n",
    "    target_dim = recommender.config['model']['reduced_dim']\n",
    ")\n",
    "reduced_embeddings = recommender.embedding_generator.reduce_dimensions(embeddings)\n",
    "\"\"\"\n",
    "\n",
    "# 임베딩 저장\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "\n",
    "# ✨ 개선: 파일명에 'enhanced' 추가하여 구분\n",
    "save_path = 'data/embeddings/place_embeddings_full768_enhanced.npy'\n",
    "np.save(save_path, embeddings)\n",
    "\n",
    "print(f\"\\n💾 임베딩 저장 완료:\")\n",
    "print(f\"   - 경로: {save_path}\")\n",
    "print(f\"   - 크기: {os.path.getsize(save_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# 추천 시스템에 임베딩 저장\n",
    "recommender.place_embeddings = embeddings\n",
    "recommender.place_names = processed_df['name'].tolist()\n",
    "\n",
    "print(f\"\\n✅ 768차원 임베딩 생성 및 저장 완료\")\n",
    "print(f\"   - 사용 텍스트: enhanced_description (증강됨)\")\n",
    "print(f\"   - 정규화: 적용됨\")\n",
    "print(f\"   - 배치 처리: 최적화됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72f8fd-1306-4750-9edc-c8373bcfb6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGBoost 모델 학습 및 평가\n",
    "print(\"\\n === XGBoost 모델 학습===\")\n",
    "\n",
    "# 특성과 라벨 준비\n",
    "features = embeddings\n",
    "labels = encoded_labels\n",
    "\n",
    "# 모델 학습\n",
    "recommender.xgb_trainer.train_models(features, labels)\n",
    "\n",
    "# 모델 평가 \n",
    "recommender.xgb_trainer.evaluate_models(features, labels)\n",
    "\n",
    "print(\"\\n=== XGBoost 모델 학습 및 평가 완료===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e37bf-50e5-4faf-937b-0394617108ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 및 인코더 저장\n",
    "print(\"\\n 모델 및 인코더 저장\")\n",
    "\n",
    "# 폴더 생성\n",
    "os.makedirs('models/xgboost', exist_ok=True)\n",
    "os.makedirs('models/encoders', exist_ok=True)\n",
    "\n",
    "# 인코더 저장\n",
    "recommender.preprocessor.save_encoders('models/encoders')\n",
    "\n",
    "# XGBoost 모델 저장\n",
    "recommender.xgb_trainer.save_models('models')\n",
    "\n",
    "print(\" 모든 모델 및 인코더 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0eba0b-7140-4112-af28-ae7eb2adb4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추천 시스템 테스트\n",
    "print(\"\\n=== 추천 시스템 테스트 ===\")\n",
    "\n",
    "# 테스트 케이스 1: 태그 기반 입력\n",
    "test_input_1 = {\n",
    "    \"season\": \"여름\",\n",
    "    \"nature\": [\"바다\", \"자연\"],\n",
    "    \"vibe\": [\"휴식\", \"감성\"],\n",
    "    \"target\": [\"연인\"]\n",
    "}\n",
    "\n",
    "print(\"테스트 케이스 1: 태그 기반 입력\")\n",
    "print(f\"입력: {test_input_1}\")\n",
    "\n",
    "result_1 = recommender.recommend_places(test_input_1, top_k=5)\n",
    "\n",
    "print(f\"\\n 파싱된 입력: {result_1['parsed_input']}\")\n",
    "print(f\"총 {result_1['total_places']}개 관광지 중 상위 5개 추천:\")\n",
    "\n",
    "for i, place in enumerate(result_1['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   설명: {place['description']}\")\n",
    "    print(f\"   태그: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   점수: 하이브리드={place['hybrid_score']:.4f}, 유사도={place['similarity_score']:.4f}, 태그={place['tag_score']:.4f}\")\n",
    "\n",
    "\n",
    "# 테스트 케이스 2: 자유 문장 입력\n",
    "test_input_2 = {\n",
    "    \"fress_text\": \"겨울에 가족과 함께 스키를 타고 싶어요\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" *50)\n",
    "print(\"테스트 케이스 2: 자유 문장 입력\")\n",
    "print(f\"입력 : {test_input_2}\")\n",
    "\n",
    "result_2 = recommender.recommend_places(test_input_2, top_k=5)\n",
    "\n",
    "for i, place in enumerate(result_2['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   설명: {place['description']}\")\n",
    "    print(f\"   태그: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   점수: 하이브리드={place['hybrid_score']:.4f}, 유사도={place['similarity_score']:.4f}, 태그={place['tag_score']:.4f}\")\n",
    "\n",
    "print(\"\\n 추천 시스템 테스트 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911af44-7e05-479f-9303-c1566010e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 로드 및 재사용 테스트\n",
    "print(f\"\\n === 모델 로드 및 재사용 테스트===\")\n",
    "# 새로운 추천 시스템 인스턴스 생성 (수정된 클래스 사용)\n",
    "new_recommender = GangwonPlaceRecommender()\n",
    "\n",
    "# 데이터 로드\n",
    "new_recommender.df = pd.read_csv('data/processed/gangwon_places_100_processed.csv')\n",
    "new_recommender.df = new_recommender.df.reset_index(drop=True)  # 인덱스 리셋\n",
    "\n",
    "# 임베딩 로드\n",
    "new_recommender.place_embeddings = np.load('data/embeddings/place_embeddings_full768.npy')\n",
    "\n",
    "# 인코더 로드\n",
    "new_recommender.preprocessor.load_encoders('models/encoders')\n",
    "\n",
    "# XGBoost 모델 로드\n",
    "new_recommender.xgb_trainer.load_models('models')\n",
    "\n",
    "# 테스트 실행\n",
    "test_input_3 = {\n",
    "    \"free_text\": \"봄에 혼자 조용한 산에서 힐링하고 싶어요\"\n",
    "}\n",
    "\n",
    "print(\"테스트 케이스 3: 수정된 모델로 추천\")\n",
    "print(f\"입력: {test_input_3}\")\n",
    "\n",
    "result_3 = new_recommender.recommend_places(test_input_3, top_k=3)\n",
    "\n",
    "print(f\"\\n파싱된 입력: {result_3['parsed_input']}\")\n",
    "print(f\"상위 3개 추천:\")\n",
    "\n",
    "for i, place in enumerate(result_3['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   설명: {place['description']}\")\n",
    "    print(f\"   태그: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   하이브리드 점수: {place['hybrid_score']:.4f}\")\n",
    "\n",
    "print(\"\\n✅ 수정된 모델 테스트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46069c-0d8d-4e6f-89ed-786967358f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 테스트용 추천 함수\n",
    "def simple_recommend_test(recommender, user_input, top_k=3):\n",
    "    \"\"\"간단한 테스트용 추천 함수\"\"\"\n",
    "    \n",
    "    # 파싱된 입력\n",
    "    parsed_input = recommender.parse_user_input(user_input)\n",
    "    \n",
    "    # 쿼리 텍스트 생성\n",
    "    if 'free_text' in user_input:\n",
    "        query_text = user_input['free_text']\n",
    "    else:\n",
    "        query_parts = []\n",
    "        if parsed_input['season']:\n",
    "            query_parts.append(f\"{parsed_input['season']}에\")\n",
    "        if parsed_input['nature']:\n",
    "            query_parts.append(f\"{', '.join(parsed_input['nature'])}에서\")\n",
    "        if parsed_input['vibe']:\n",
    "            query_parts.append(f\"{', '.join(parsed_input['vibe'])} 여행\")\n",
    "        query_text = ' '.join(query_parts)\n",
    "    \n",
    "    # 쿼리 임베딩 생성\n",
    "    if recommender.embedding_generator.model is None:\n",
    "        recommender.embedding_generator.load_model()\n",
    "    \n",
    "    query_embedding = recommender.embedding_generator.model.encode([query_text])\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarity_scores = cosine_similarity(query_embedding, recommender.place_embeddings)[0]\n",
    "    \n",
    "    # 상위 추천지 선택\n",
    "    top_indices = np.argsort(similarity_scores)[::-1][:top_k]\n",
    "    \n",
    "    # 결과 구성\n",
    "    recommendations = []\n",
    "    for idx in top_indices:\n",
    "        place_info = {\n",
    "            'name': recommender.df.iloc[idx]['name'],\n",
    "            'description': recommender.df.iloc[idx]['short_description'],\n",
    "            'similarity_score': float(similarity_scores[idx])\n",
    "        }\n",
    "        recommendations.append(place_info)\n",
    "    \n",
    "    return {\n",
    "        'parsed_input': parsed_input,\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "\n",
    "# 테스트 실행\n",
    "test_input_3 = {\n",
    "    \"free_text\": \"봄에 혼자 조용한 산에서 힐링하고 싶어요\"\n",
    "}\n",
    "\n",
    "print(\"=== 간단한 테스트 ===\")\n",
    "print(f\"입력: {test_input_3}\")\n",
    "\n",
    "result_3 = simple_recommend_test(new_recommender, test_input_3, top_k=3)\n",
    "\n",
    "print(f\"\\n파싱된 입력: {result_3['parsed_input']}\")\n",
    "print(f\"상위 3개 추천:\")\n",
    "\n",
    "for i, place in enumerate(result_3['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   설명: {place['description']}\")\n",
    "    print(f\"   유사도 점수: {place['similarity_score']:.4f}\")\n",
    "\n",
    "print(\"\\n✅ 간단한 테스트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e897c-9917-4559-aa67-ec8c98367711",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flask API 연동을 위한 JSON 변환 함수\n",
    "\n",
    "def create_api_response(recommendation_result: Dict) -> Dict:\n",
    "    \"\"\"Flask API 응답을 위한 JSON 형태로 변환\"\"\"\n",
    "\n",
    "    api_response = {\n",
    "        \n",
    "        'status': 'success',\n",
    "        'data' : {\n",
    "        'user_input': recommendation_result['user_input'],\n",
    "        'parsed_input': recommendation_result['parsed_input'],\n",
    "        'total_places': recommendation_result['total_places'],\n",
    "        'recommendations':[]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for place in recommendation_result['recommendations']:\n",
    "        place_data = {\n",
    "            'name': place['name'],\n",
    "            'description': place['description'],\n",
    "            'tags': {\n",
    "                'season': place['season'],\n",
    "                'nature': place['nature'],\n",
    "                'vibe': place['vibe'],\n",
    "                'target': place['target']\n",
    "            },\n",
    "            'scores' :{\n",
    "                'hybrid': round(place['hybrid_score'], 4),\n",
    "                'similarity': round(place['similarity_score'], 4),\n",
    "                'tag_match': round(place['tag_score'], 4)\n",
    "            }\n",
    "        }\n",
    "        api_response['data']['recommendations'].append(place_data)\n",
    "\n",
    "        return api_response\n",
    "\n",
    "print(\"Flask API 연동 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581073d5-364b-4cd7-839b-02231836a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 사용자 정의 추천 함수(Flask API용)\n",
    "\n",
    "def recommend_places_api(user_input: Union[Dict, str], top_k: int = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Flask API에서 사용할 추천 함수\n",
    "    \n",
    "    Args:\n",
    "        user_input: 사용자 입력 (Dict 또는 JSON 문자열)\n",
    "        top_k: 추천할 관광지 수\n",
    "    \n",
    "    Returns:\n",
    "        API 응답 형태의 Dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 문자열인 경우 JSON 파싱\n",
    "        if isinstance(user_input, str):\n",
    "            user_input = json.loads(user_input)\n",
    "\n",
    "        # 입력 검증\n",
    "        if not isinstance(user_input, dict):\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': '잘못된 입력 방식입니다.',\n",
    "                'data': None\n",
    "            }\n",
    "        # 추천 실행\n",
    "        result = recommender.recommend_places(user_input, top_k= top_k)\n",
    "\n",
    "        # API 응답 생성\n",
    "        api_response = create_api_response(result)\n",
    "\n",
    "        return api_response\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'message': f'추천 처리 중 오류 발생: {str(e)}',\n",
    "            'data': None\n",
    "        }\n",
    "# API 함수 테스트\n",
    "print(\"\\n=== API 함수 테스트 ===\")\n",
    "\n",
    "# JSON 문자열 입력 테스트\n",
    "json_input = '{\"free_text\": \"여름에 바다에서 서핑하고 싶어요\"}'\n",
    "api_result = recommend_places_api(json_input, top_k=3)\n",
    "\n",
    "print(\"JSON 문자열 입력 테스트:\")\n",
    "print(f\"Status: {api_result['status']}\")\n",
    "if api_result['status'] == 'success':\n",
    "    print(f\"추천 결과: {len(api_result['data']['recommendations'])}개\")\n",
    "    for i, place in enumerate(api_result['data']['recommendations']):\n",
    "        print(f\"  {i+1}. {place['name']} (점수: {place['scores']['hybrid']})\")\n",
    "\n",
    "print(\"\\n API 함수 테스트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17e0ce-fe1e-4adf-ab0c-0adfc3de8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추가 테스트 케이스\n",
    "\n",
    "print(\"\\n=== 추가 테스트 케이스===\")\n",
    "\n",
    "# 테스트 케이스 4: 복합 태그 입력\n",
    "test_input_4 = {\n",
    "    \"season\": \"가을\",\n",
    "    \"nature\": [\"산\", \"자연\"],\n",
    "    \"vibe\": [\"감성\", \"휴식\"],\n",
    "    \"target\": [\"혼자\"]\n",
    "}\n",
    "\n",
    "print(\"테스트 케이스 4: 복합 태그 입력\")\n",
    "print(f\"입력: {test_input_4}\")\n",
    "\n",
    "result_4 = recommender.recommend_places(test_input_4, top_k=3)\n",
    "\n",
    "print(f\"\\n파싱된 입력: {result_4['parsed_input']}\")\n",
    "print(f\"상위 3개 추천:\")\n",
    "\n",
    "for i, place in enumerate(result_4['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   설명: {place['description'][:100]}...\")\n",
    "    print(f\"   점수: {place['hybrid_score']:.4f}\")\n",
    "\n",
    "\n",
    "# 테스트 케이스 5: 다양한 자유 문장 입력\n",
    "test_cases = [\n",
    "    \"친구들과 함께 신나는 여름 휴가를 보내고 싶어요\",\n",
    "    \"연인과 로맨틱한 가을 데이트 장소를 찾고 있어요\",\n",
    "    \"가족과 함께 안전하고 교육적인 곳을 가고 싶습니다\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"테스트 케이스 5: 다양한 자유 문장 입력\")\n",
    "\n",
    "for i, test_text in enumerate(test_cases):\n",
    "    print(f\"\\n📝 테스트 {i+1}: {test_text}\")\n",
    "    \n",
    "    test_input = {\"free_text\": test_text}\n",
    "    result = recommender.recommend_places(test_input, top_k=2)\n",
    "    \n",
    "    print(f\"파싱된 입력: {result['parsed_input']}\")\n",
    "    print(f\"추천 결과:\")\n",
    "    for j, place in enumerate(result['recommendations']):\n",
    "        print(f\"  {j+1}. {place['name']} (점수: {place['hybrid_score']:.4f})\")\n",
    "\n",
    "print(\"\\n✅ 추가 테스트 케이스 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e763b5-818c-49f4-b574-20ae1d164340",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 성능 분석 및 시각화\n",
    "print(\"\\n === 성능 분석 ===\")\n",
    "\n",
    "#추천 점수 분포 분석\n",
    "def analyze_recommendation_scores():\n",
    "    \"\"\"추천 점수 분포 분석\"\"\"\n",
    "\n",
    "    # 샘플 데이터 입력들\n",
    "    sample_inputs = [\n",
    "        {\"season\": \"여름\", \"nature\": [\"바다\"], \"vibe\": [\"휴식\"], \"target\": [\"연인\"]},\n",
    "        {\"season\": \"겨울\", \"nature\": [\"산\"], \"vibe\": [\"모험\"], \"target\": [\"친구\"]},\n",
    "        {\"season\": \"봄\", \"nature\": [\"자연\"], \"vibe\": [\"감성\"], \"target\": [\"혼자\"]},\n",
    "        {\"free_text\": \"가을에 단풍 보러 가고 싶어요\"},\n",
    "        {\"free_text\": \"스키장에서 스릴 넘치는 겨울을 보내고 싶습니다\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"📊 추천 점수 분포 분석:\")\n",
    "\n",
    "    for i, test_input in enumerate(sample_inputs):\n",
    "        result = recommender.recommend_places(test_input, top_k=5)\n",
    "\n",
    "        hybrid_scores = [place['hybrid_score'] for place in result ['recommendations']]\n",
    "        similarity_scores = [place['similarity_score'] for place in result['recommendations']]\n",
    "        tag_scores = [place['tag_score'] for place in result['recommendations']]\n",
    "        \n",
    "        print(f\"\\n테스트 {i+1}: {test_input}\")\n",
    "        print(f\"  하이브리드 점수 범위: {min(hybrid_scores):.4f} ~ {max(hybrid_scores):.4f}\")\n",
    "        print(f\"  유사도 점수 평균: {np.mean(similarity_scores):.4f}\")\n",
    "        print(f\"  태그 매칭 점수 평균: {np.mean(tag_scores):.4f}\")\n",
    "\n",
    "analyze_recommendation_scores()\n",
    "\n",
    "# 시스템 성능 정보\n",
    "print(f\"\\n🔧 시스템 성능 정보:\")\n",
    "print(f\"- 전체 관광지 수: {len(recommender.df)}\")\n",
    "print(f\"- 임베딩 차원: {recommender.place_embeddings.shape[1]}\")\n",
    "print(f\"- 메모리 사용량: {recommender.place_embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"- 학습된 모델 수: {len(recommender.xgb_trainer.models)}\")\n",
    "\n",
    "print(\"\\n✅ 성능 분석 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48edfebd-8839-4121-a71f-76235ffa8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 최종 정리 및 사용법 안내\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 강원도 관광지 추천 시스템 구축 완료!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📁 생성된 파일 구조:\")\n",
    "print(\"\"\"\n",
    "project_root/\n",
    "├── data/\n",
    "│   ├── raw/gangwon_places_100.xlsx                 # 원본 데이터\n",
    "│   ├── processed/gangwon_places_100_processed.csv # 전처리된 데이터\n",
    "│   └── embeddings/place_embeddings_full768.npy    # 768차원 임베딩\n",
    "├── models/\n",
    "│   ├── xgboost/\n",
    "│   │   ├── season_model.joblib                    # 계절 분류 모델\n",
    "│   │   ├── nature_model.joblib                    # 자연환경 분류 모델\n",
    "│   │   ├── vibe_model.joblib                      # 분위기 분류 모델\n",
    "│   │   └── target_model.joblib                    # 대상 분류 모델\n",
    "│   └── encoders/\n",
    "│       ├── season_encoder.joblib                  # 계절 인코더\n",
    "│       ├── nature_encoder.joblib                  # 자연환경 인코더\n",
    "│       ├── vibe_encoder.joblib                    # 분위기 인코더\n",
    "│       └── target_encoder.joblib                  # 대상 인코더\n",
    "└── config/config.yaml                             # 설정 파일\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🚀 사용법:\")\n",
    "print(\"\"\"\n",
    "1. 태그 기반 추천:\n",
    "   user_input = {\n",
    "       \"season\": \"여름\",\n",
    "       \"nature\": [\"바다\", \"자연\"],\n",
    "       \"vibe\": [\"감성\", \"휴식\"],\n",
    "       \"target\": [\"연인\"]\n",
    "   }\n",
    "   result = recommender.recommend_places(user_input, top_k=5)\n",
    "\n",
    "2. 자유 문장 기반 추천:\n",
    "   user_input = {\n",
    "       \"free_text\": \"겨울에 가족과 함께 스키를 타고 싶어요\"\n",
    "   }\n",
    "   result = recommender.recommend_places(user_input, top_k=5)\n",
    "\n",
    "3. Flask API 연동:\n",
    "   api_response = recommend_places_api(user_input, top_k=10)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n⚙️ 주요 기능:\")\n",
    "print(\"\"\"\n",
    "✅ SBERT 기반 한국어 임베딩 생성 (768차원 유지)\n",
    "✅ XGBoost 다중 라벨 분류 (season, nature, vibe, target)\n",
    "✅ 하이브리드 점수 계산 (유사도 60% + 태그 40%)\n",
    "✅ 자유 문장 입력 파싱 및 태그 추출\n",
    "✅ 모델 및 인코더 저장/로드\n",
    "✅ Flask API 연동 준비\n",
    "✅ JSON 입출력 지원\n",
    "✅ 성능 분석 도구\n",
    "✅ 다양한 테스트 케이스 지원\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📊 성능 지표:\")\n",
    "print(f\"- 데이터: {len(recommender.df)}개 관광지\")\n",
    "print(f\"- 임베딩 차원: {recommender.place_embeddings.shape[1]}차원\")\n",
    "print(f\"- 모델 타입: XGBoost (season: 단일라벨, nature/vibe/target: 다중라벨)\")\n",
    "print(f\"- 추천 방식: 하이브리드 (유사도 60% + 태그 40%)\")\n",
    "print(f\"- 지원 입력: 태그 기반 + 자유 문장 입력\")\n",
    "\n",
    "print(\"\\n🔄 모델 재사용:\")\n",
    "print(\"\"\"\n",
    "# 저장된 모델 로드\n",
    "new_recommender = GangwonPlaceRecommender()\n",
    "new_recommender.df = pd.read_csv('data/processed/gangwon_places_100_processed.xlsx')\n",
    "new_recommender.place_embeddings = np.load('data/embeddings/place_embeddings_full768.npy')\n",
    "new_recommender.preprocessor.load_encoders('models/encoders')\n",
    "new_recommender.xgb_trainer.load_models('models')\n",
    "\n",
    "# 추천 실행\n",
    "result = new_recommender.recommend_places(user_input, top_k=5)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n💡 추가 활용 방안:\")\n",
    "print(\"\"\"\n",
    "1. 웹 애플리케이션 연동:\n",
    "   - Flask/Django 백엔드에 recommend_places_api() 함수 활용\n",
    "   - REST API 엔드포인트 구성\n",
    "   - 실시간 추천 서비스 제공\n",
    "\n",
    "2. 모바일 앱 연동:\n",
    "   - JSON 형태의 API 응답 활용\n",
    "   - 사용자 입력 파싱 기능 활용\n",
    "   - 오프라인 모델 배포 가능\n",
    "\n",
    "3. 성능 최적화:\n",
    "   - 임베딩 캐싱으로 응답 속도 향상\n",
    "   - 배치 추천 처리\n",
    "   - 모델 압축 및 경량화\n",
    "\n",
    "4. 기능 확장:\n",
    "   - 사용자 피드백 학습\n",
    "   - 협업 필터링 추가\n",
    "   - 개인화 추천 구현\n",
    "   - 실시간 학습 시스템\n",
    "   - 지역별 필터링 기능\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n📝 주의사항:\")\n",
    "print(\"\"\"\n",
    "- 첫 실행 시 SBERT 모델 다운로드로 시간이 소요될 수 있습니다\n",
    "- GPU 사용 시 더 빠른 임베딩 생성이 가능합니다\n",
    "- 실제 서비스 배포 시 보안 및 에러 처리를 강화하세요\n",
    "- 데이터 업데이트 시 모델 재학습이 필요할 수 있습니다\n",
    "- 추천 성능 향상을 위해 정기적인 모델 튜닝을 권장합니다\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🌟 추천 시스템 특징:\")\n",
    "print(\"\"\"\n",
    "- 한국어 특화 SBERT 모델 사용 (snunlp/KR-SBERT-V40K-klueNLI-augSTS)\n",
    "- 하이브리드 추천 (의미적 유사도 + 태그 매칭)\n",
    "- 자유 문장 입력 지원으로 사용자 편의성 향상\n",
    "- 다중 라벨 분류로 정확한 태그 예측\n",
    "- 모델 저장/로드 기능으로 효율적인 운영\n",
    "- Flask API 연동으로 웹 서비스 확장 가능\n",
    "- 성능 분석 도구로 시스템 모니터링 가능\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🎯 추천 시스템 성능:\")\n",
    "print(\"\"\"\n",
    "- 임베딩 기반 의미적 유사도 계산 (60% 가중치)\n",
    "- 태그 매칭 기반 정확도 향상 (40% 가중치)\n",
    "- 계절, 자연환경, 분위기, 대상별 세분화된 추천\n",
    "- 자유 문장 파싱으로 자연스러운 사용자 경험\n",
    "- 상위 K개 추천으로 다양한 선택지 제공\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🚀 강원도 관광지 추천 시스템이 성공적으로 구축되었습니다!\")\n",
    "print(\"   이제 다양한 사용자 입력에 대해 정확한 관광지 추천이 가능합니다.\")\n",
    "print(\"   Flask API 연동을 통해 웹 서비스로 확장할 수 있습니다.\")\n",
    "print(\"   모든 오류가 수정되어 안정적으로 작동합니다.\")\n",
    "print(\"   태그 기반 추천과 자유 문장 입력을 모두 지원합니다.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📚 추가 학습 자료:\")\n",
    "print(\"\"\"\n",
    "- SBERT 모델 상세 정보: https://huggingface.co/snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
    "- XGBoost 공식 문서: https://xgboost.readthedocs.io/\n",
    "- Scikit-learn 다중 라벨 분류: https://scikit-learn.org/stable/modules/multiclass.html\n",
    "- Flask API 개발 가이드: https://flask.palletsprojects.com/\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🔗 다음 단계:\")\n",
    "print(\"\"\"\n",
    "1. 웹 인터페이스 개발 (HTML/CSS/JavaScript)\n",
    "2. Flask/Django 백엔드 API 구축\n",
    "3. 데이터베이스 연동 (PostgreSQL/MySQL)\n",
    "4. 사용자 피드백 수집 시스템\n",
    "5. 추천 성능 모니터링 대시보드\n",
    "6. 모바일 앱 연동\n",
    "7. 실시간 추천 시스템 구축\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✨ 완료된 기능들:\")\n",
    "print(\"\"\"\n",
    "✅ 데이터 전처리 및 정제\n",
    "✅ SBERT 임베딩 생성 (768차원)\n",
    "✅ XGBoost 다중 라벨 분류 모델 학습\n",
    "✅ 하이브리드 추천 알고리즘 구현\n",
    "✅ 자유 문장 입력 파싱 시스템\n",
    "✅ 태그 기반 추천 시스템\n",
    "✅ 모델 저장/로드 기능\n",
    "✅ Flask API 연동 준비\n",
    "✅ 성능 분석 도구\n",
    "✅ 다양한 테스트 케이스\n",
    "✅ 에러 처리 및 디버깅\n",
    "✅ 완전한 문서화\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🎊 축하합니다! 강원도 관광지 추천 시스템이 완성되었습니다!\")\n",
    "print(\"이제 실제 사용자들에게 정확하고 유용한 관광지 추천을 제공할 수 있습니다.\")\n",
    "\n",
    "# ================================\n",
    "# 보너스: 간단한 사용 예제\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 간단한 사용 예제\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 예제 1: 간단한 추천\n",
    "print(\"\\n📝 예제 1: 간단한 추천\")\n",
    "simple_input = {\"free_text\": \"봄에 산에서 힐링\"}\n",
    "simple_result = recommender.recommend_places(simple_input, top_k=3)\n",
    "print(f\"입력: {simple_input['free_text']}\")\n",
    "print(\"추천 결과:\")\n",
    "for i, place in enumerate(simple_result['recommendations']):\n",
    "    print(f\"  {i+1}. {place['name']} (점수: {place['hybrid_score']:.3f})\")\n",
    "\n",
    "# 예제 2: 태그 조합 추천\n",
    "print(\"\\n📝 예제 2: 태그 조합 추천\")\n",
    "tag_input = {\"season\": \"여름\", \"nature\": [\"바다\"], \"target\": [\"가족\"]}\n",
    "tag_result = recommender.recommend_places(tag_input, top_k=3)\n",
    "print(f\"입력: {tag_input}\")\n",
    "print(\"추천 결과:\")\n",
    "for i, place in enumerate(tag_result['recommendations']):\n",
    "    print(f\"  {i+1}. {place['name']} (점수: {place['hybrid_score']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🚀 시스템 준비 완료! 이제 마음껏 사용하세요!\")\n",
    "print(\"=\"*60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea4517-59d3-4c5f-be74-c3a011ebe00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. 필수 라이브러리 임포트 및 설정\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ 라이브러리 임포트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0681300d-3041-4586-bcdb-18ed2b28cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 로드 (data/raw/gangwon_places_100.xlsx)\n",
    "print(\"=\"*60)\n",
    "print(\"📂 데이터 로드 중...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Excel 파일 로드\n",
    "df = pd.read_excel('data/raw/gangwon_places_1000.xlsx')\n",
    "\n",
    "print(f\"✅ 데이터 로드 완료: {df.shape}\")\n",
    "print(f\"컬럼: {df.columns.tolist()}\")\n",
    "print(f\"\\n샘플 데이터 (첫 3개):\")\n",
    "print(df.head(3)[['name', 'season', 'nature', 'vibe']])\n",
    "\n",
    "# 기본 전처리\n",
    "def preprocess_tags(value):\n",
    "    \"\"\"태그 문자열을 리스트로 변환\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return []\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    return [item.strip() for item in str(value).split(',') if item.strip()]\n",
    "\n",
    "# 태그 컬럼 전처리\n",
    "for col in ['nature', 'vibe', 'target']:\n",
    "    df[col] = df[col].apply(preprocess_tags)\n",
    "\n",
    "# 결측치 처리\n",
    "df['short_description'] = df['short_description'].fillna('')\n",
    "df['season'] = df['season'].fillna('사계절')\n",
    "\n",
    "print(f\"\\n✅ 전처리 완료!\")\n",
    "print(f\"Nature 샘플: {df['nature'].iloc[0]}\")\n",
    "print(f\"Vibe 샘플: {df['vibe'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c25cdf-d9b5-4be1-bbfd-d77c7d11b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 증강으로 설명 텍스트 강화\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔧 데이터 증강 중...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class DataAugmenter:\n",
    "    \"\"\"텍스트 증강 클래스\"\"\"\n",
    "    \n",
    "    def augment_description(self, row):\n",
    "        \"\"\"설명 텍스트에 태그 정보 추가\"\"\"\n",
    "        original = str(row['short_description'])\n",
    "        \n",
    "        # 계절 정보 추가\n",
    "        season_text = f\"이곳은 {row['season']}에 특히 아름답습니다.\"\n",
    "        \n",
    "        # 자연환경 정보 추가\n",
    "        if row['nature']:\n",
    "            nature_text = f\"{', '.join(row['nature'])} 경관을 즐길 수 있습니다.\"\n",
    "        else:\n",
    "            nature_text = \"\"\n",
    "        \n",
    "        # 분위기 정보 추가\n",
    "        if row['vibe']:\n",
    "            vibe_text = f\"{', '.join(row['vibe'])} 분위기로 좋습니다.\"\n",
    "        else:\n",
    "            vibe_text = \"\"\n",
    "        \n",
    "        # 대상 정보 추가\n",
    "        if row['target']:\n",
    "            target_text = f\"{', '.join(row['target'])}에게 추천합니다.\"\n",
    "        else:\n",
    "            target_text = \"\"\n",
    "        \n",
    "        # 모든 정보 결합\n",
    "        augmented = f\"{original} {season_text} {nature_text} {vibe_text} {target_text}\"\n",
    "        \n",
    "        return augmented.strip()\n",
    "\n",
    "# 증강 적용\n",
    "augmenter = DataAugmenter()\n",
    "df['enhanced_description'] = df.apply(augmenter.augment_description, axis=1)\n",
    "\n",
    "print(f\"✅ 데이터 증강 완료!\")\n",
    "print(f\"\\n원본 설명 샘플:\")\n",
    "print(df['short_description'].iloc[0][:100] + \"...\")\n",
    "print(f\"\\n증강된 설명 샘플:\")\n",
    "print(df['enhanced_description'].iloc[0][:150] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36426a3d-b80c-4b16-a159-5b6b906a21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 여러 SBERT 모델을 앙상블하여 임베딩 생성\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 앙상블 임베딩 생성 중...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class EnsembleEmbedding:\n",
    "    \"\"\"앙상블 임베딩 생성기\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 주 모델\n",
    "        print(\"📥 SBERT 모델 로드 중...\")\n",
    "        try:\n",
    "            self.primary_model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
    "            print(\"✅ 주 모델 로드 완료: jhgan/ko-sroberta-multitask\")\n",
    "        except:\n",
    "            self.primary_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "            print(\"✅ 대체 모델 로드 완료: paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    \n",
    "    def generate_multi_field_embeddings(self, df):\n",
    "        \"\"\"여러 필드를 결합한 가중 임베딩\"\"\"\n",
    "        combined_texts = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # 필드별 가중치 적용\n",
    "            text_parts = []\n",
    "            \n",
    "            # 1. 증강된 설명 (가중치 3)\n",
    "            text_parts.extend([row['enhanced_description']] * 3)\n",
    "            \n",
    "            # 2. 장소명 (가중치 2)\n",
    "            text_parts.extend([row['name']] * 2)\n",
    "            \n",
    "            # 3. 태그들 (가중치 1)\n",
    "            text_parts.append(row['season'])\n",
    "            text_parts.extend(row['nature'])\n",
    "            text_parts.extend(row['vibe'])\n",
    "            \n",
    "            combined = ' '.join(text_parts)\n",
    "            combined_texts.append(combined)\n",
    "        \n",
    "        # 배치 임베딩 생성\n",
    "        print(f\"🔄 임베딩 생성 중... (총 {len(combined_texts)}개)\")\n",
    "        embeddings = self.primary_model.encode(\n",
    "            combined_texts,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=16\n",
    "        )\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# 임베딩 생성\n",
    "ensemble_embedder = EnsembleEmbedding()\n",
    "place_embeddings = ensemble_embedder.generate_multi_field_embeddings(df)\n",
    "\n",
    "print(f\"\\n✅ 임베딩 생성 완료!\")\n",
    "print(f\"임베딩 형태: {place_embeddings.shape}\")\n",
    "print(f\"임베딩 차원: {place_embeddings.shape[1]}\")\n",
    "\n",
    "# 임베딩 저장\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "np.save('data/embeddings/enhanced_embeddings.npy', place_embeddings)\n",
    "print(f\"💾 임베딩 저장 완료: data/embeddings/enhanced_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff4a53f-cb34-42df-a241-c665f4fbb107",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추가 피처 생성으로 성능 향상\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔨 피처 엔지니어링 중...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"피처 엔지니어링 클래스\"\"\"\n",
    "    \n",
    "    def create_statistical_features(self, df):\n",
    "        \"\"\"통계적 피처\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # 태그 개수\n",
    "            nature_count = len(row['nature'])\n",
    "            vibe_count = len(row['vibe'])\n",
    "            target_count = len(row['target'])\n",
    "            total_tags = nature_count + vibe_count + target_count\n",
    "            \n",
    "            # 텍스트 길이\n",
    "            desc_length = len(str(row['short_description']))\n",
    "            enhanced_length = len(str(row['enhanced_description']))\n",
    "            \n",
    "            # 고유 단어 수\n",
    "            words = str(row['enhanced_description']).split()\n",
    "            unique_words = len(set(words))\n",
    "            \n",
    "            features.append([\n",
    "                nature_count,\n",
    "                vibe_count,\n",
    "                target_count,\n",
    "                total_tags,\n",
    "                desc_length,\n",
    "                enhanced_length,\n",
    "                unique_words,\n",
    "                enhanced_length / desc_length if desc_length > 0 else 1\n",
    "            ])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def create_tag_combination_features(self, df):\n",
    "        \"\"\"태그 조합 피처 (One-Hot)\"\"\"\n",
    "        # Nature + Vibe 조합\n",
    "        combinations = []\n",
    "        for idx, row in df.iterrows():\n",
    "            combo = [f\"{n}_{v}\" for n in row['nature'] for v in row['vibe']]\n",
    "            combinations.append(combo if combo else ['없음'])\n",
    "        \n",
    "        mlb = MultiLabelBinarizer()\n",
    "        combo_features = mlb.fit_transform(combinations)\n",
    "        \n",
    "        return combo_features\n",
    "    \n",
    "    def combine_all_features(self, embeddings, statistical, combinations):\n",
    "        \"\"\"모든 피처 결합\"\"\"\n",
    "        # PCA로 임베딩 축소 (추가 정보로)\n",
    "        pca = PCA(n_components=64)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "        \n",
    "        # 모든 피처 결합\n",
    "        final_features = np.concatenate([\n",
    "            embeddings,           # 원본 임베딩\n",
    "            reduced_embeddings,   # 축소 임베딩\n",
    "            statistical,          # 통계 피처\n",
    "            combinations          # 조합 피처\n",
    "        ], axis=1)\n",
    "        \n",
    "        print(f\"✅ 피처 결합 완료!\")\n",
    "        print(f\"  - 원본 임베딩: {embeddings.shape[1]}차원\")\n",
    "        print(f\"  - 축소 임베딩: {reduced_embeddings.shape[1]}차원\")\n",
    "        print(f\"  - 통계 피처: {statistical.shape[1]}차원\")\n",
    "        print(f\"  - 조합 피처: {combinations.shape[1]}차원\")\n",
    "        print(f\"  - 최종 피처: {final_features.shape[1]}차원\")\n",
    "        \n",
    "        return final_features, pca\n",
    "\n",
    "# 피처 엔지니어링 실행\n",
    "engineer = FeatureEngineer()\n",
    "\n",
    "statistical_features = engineer.create_statistical_features(df)\n",
    "print(f\"✅ 통계 피처 생성: {statistical_features.shape}\")\n",
    "\n",
    "combination_features = engineer.create_tag_combination_features(df)\n",
    "print(f\"✅ 조합 피처 생성: {combination_features.shape}\")\n",
    "\n",
    "enhanced_features, pca_model = engineer.combine_all_features(\n",
    "    place_embeddings,\n",
    "    statistical_features,\n",
    "    combination_features\n",
    ")\n",
    "\n",
    "print(f\"\\n🎉 최종 피처 완성: {enhanced_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fc791-dd84-4b53-bc7d-7d462ee2d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 최적화된 XGBoost 모델 학습\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🚀 XGBoost 모델 학습 중...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 라벨 준비\n",
    "season_encoder = LabelEncoder()\n",
    "nature_encoder = MultiLabelBinarizer()\n",
    "vibe_encoder = MultiLabelBinarizer()\n",
    "target_encoder = MultiLabelBinarizer()\n",
    "\n",
    "y_season = season_encoder.fit_transform(df['season'])\n",
    "y_nature = nature_encoder.fit_transform(df['nature'])\n",
    "y_vibe = vibe_encoder.fit_transform(df['vibe'])\n",
    "y_target = target_encoder.fit_transform(df['target'])\n",
    "\n",
    "print(f\"✅ 라벨 인코딩 완료\")\n",
    "print(f\"  - Season 클래스: {len(season_encoder.classes_)}\")\n",
    "print(f\"  - Nature 클래스: {len(nature_encoder.classes_)}\")\n",
    "print(f\"  - Vibe 클래스: {len(vibe_encoder.classes_)}\")\n",
    "print(f\"  - Target 클래스: {len(target_encoder.classes_)}\")\n",
    "\n",
    "# 최적화된 파라미터로 모델 학습\n",
    "optimal_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.1,\n",
    "    'random_state': 42,\n",
    "    'tree_method': 'hist',\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "# Season 모델\n",
    "print(\"\\n🔧 Season 모델 학습 중...\")\n",
    "season_model = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    **optimal_params\n",
    ")\n",
    "season_model.fit(enhanced_features, y_season)\n",
    "print(f\"✅ Season 모델 학습 완료!\")\n",
    "\n",
    "# 다중 레이블 모델들\n",
    "models = {}\n",
    "\n",
    "for label_name, y_label in [('nature', y_nature), ('vibe', y_vibe), ('target', y_target)]:\n",
    "    print(f\"\\n🔧 {label_name.capitalize()} 모델 학습 중...\")\n",
    "    \n",
    "    if y_label.sum() > 0:  # 데이터가 있는 경우\n",
    "        base_model = XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            **optimal_params\n",
    "        )\n",
    "        model = MultiOutputClassifier(base_model, n_jobs=1)\n",
    "        model.fit(enhanced_features, y_label)\n",
    "        print(f\"✅ {label_name.capitalize()} 모델 학습 완료!\")\n",
    "    else:\n",
    "        from sklearn.dummy import DummyClassifier\n",
    "        model = MultiOutputClassifier(DummyClassifier(strategy='constant', constant=0))\n",
    "        model.fit(enhanced_features, y_label)\n",
    "        print(f\"⚠️ {label_name.capitalize()} - 더미 모델 사용\")\n",
    "    \n",
    "    models[label_name] = model\n",
    "\n",
    "print(f\"\\n🎉 모든 모델 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf75fb-e75c-4914-bff9-31360a3cc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 개선된 추천 시스템 클래스\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 개선된 추천 시스템 구축 중...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class EnhancedRecommendationSystem:\n",
    "    \"\"\"개선된 추천 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self, df, embeddings, models, encoders, \n",
    "                 sbert_model, pca_model, engineer):\n",
    "        self.df = df\n",
    "        self.place_embeddings = embeddings\n",
    "        self.season_model = models.get('season')\n",
    "        self.nature_model = models.get('nature')\n",
    "        self.vibe_model = models.get('vibe')\n",
    "        self.target_model = models.get('target')\n",
    "        self.season_encoder = encoders['season']\n",
    "        self.nature_encoder = encoders['nature']\n",
    "        self.vibe_encoder = encoders['vibe']\n",
    "        self.target_encoder = encoders['target']\n",
    "        self.sbert_model = sbert_model\n",
    "        self.pca_model = pca_model\n",
    "        self.engineer = engineer\n",
    "        \n",
    "        # 가중치\n",
    "        self.similarity_weight = 0.5\n",
    "        self.tag_weight = 0.3\n",
    "        self.predicted_weight = 0.2\n",
    "    \n",
    "    def encode_user_query(self, user_input: Dict) -> np.ndarray:\n",
    "        \"\"\"사용자 입력을 임베딩으로 변환\"\"\"\n",
    "        # 텍스트 생성\n",
    "        text_parts = []\n",
    "        \n",
    "        if 'season' in user_input and user_input['season']:\n",
    "            text_parts.extend([user_input['season']] * 3)\n",
    "        \n",
    "        for key in ['nature', 'vibe', 'target']:\n",
    "            if key in user_input:\n",
    "                values = user_input[key]\n",
    "                if isinstance(values, list):\n",
    "                    text_parts.extend(values * 2)\n",
    "                else:\n",
    "                    text_parts.extend([values] * 2)\n",
    "        \n",
    "        query_text = ' '.join(text_parts) if text_parts else \"관광지\"\n",
    "        \n",
    "        # 임베딩 생성\n",
    "        query_embedding = self.sbert_model.encode(\n",
    "            [query_text],\n",
    "            normalize_embeddings=True\n",
    "        )[0]\n",
    "        \n",
    "        return query_embedding\n",
    "    \n",
    "    def calculate_advanced_scores(self, user_input: Dict, \n",
    "                                  user_embedding: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"고급 스코어링\"\"\"\n",
    "        \n",
    "        # 1. 코사인 유사도\n",
    "        similarity_scores = cosine_similarity(\n",
    "            user_embedding.reshape(1, -1),\n",
    "            self.place_embeddings[:, :len(user_embedding)]  # 임베딩 차원 맞추기\n",
    "        )[0]\n",
    "        \n",
    "        # 2. 태그 매칭 스코어\n",
    "        tag_scores = self._calculate_tag_scores(user_input)\n",
    "        \n",
    "        # 3. 예측 기반 스코어 (XGBoost)\n",
    "        predicted_scores = self._calculate_predicted_scores(user_embedding)\n",
    "        \n",
    "        # 4. 최종 스코어 (가중 평균)\n",
    "        final_scores = (\n",
    "            self.similarity_weight * similarity_scores +\n",
    "            self.tag_weight * tag_scores +\n",
    "            self.predicted_weight * predicted_scores\n",
    "        )\n",
    "        \n",
    "        return final_scores, similarity_scores, tag_scores, predicted_scores\n",
    "    \n",
    "    def _calculate_tag_scores(self, user_input: Dict) -> np.ndarray:\n",
    "        \"\"\"개선된 태그 매칭 스코어\"\"\"\n",
    "        scores = np.zeros(len(self.df))\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            score = 0.0\n",
    "            \n",
    "            # Season 매칭 (가중치 0.3)\n",
    "            if user_input.get('season') == row['season']:\n",
    "                score += 0.3\n",
    "            \n",
    "            # Nature 매칭 (가중치 0.25) - Jaccard + F1\n",
    "            if 'nature' in user_input and user_input['nature']:\n",
    "                user_nature = set(user_input['nature'] if isinstance(user_input['nature'], list) \n",
    "                                else [user_input['nature']])\n",
    "                place_nature = set(row['nature'])\n",
    "                \n",
    "                if user_nature and place_nature:\n",
    "                    intersection = len(user_nature & place_nature)\n",
    "                    union = len(user_nature | place_nature)\n",
    "                    jaccard = intersection / union if union > 0 else 0\n",
    "                    \n",
    "                    precision = intersection / len(place_nature) if place_nature else 0\n",
    "                    recall = intersection / len(user_nature) if user_nature else 0\n",
    "                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                    \n",
    "                    score += 0.25 * (0.6 * jaccard + 0.4 * f1)\n",
    "            \n",
    "            # Vibe 매칭 (가중치 0.25)\n",
    "            if 'vibe' in user_input and user_input['vibe']:\n",
    "                user_vibe = set(user_input['vibe'] if isinstance(user_input['vibe'], list) \n",
    "                              else [user_input['vibe']])\n",
    "                place_vibe = set(row['vibe'])\n",
    "                \n",
    "                if user_vibe and place_vibe:\n",
    "                    intersection = len(user_vibe & place_vibe)\n",
    "                    union = len(user_vibe | place_vibe)\n",
    "                    jaccard = intersection / union if union > 0 else 0\n",
    "                    score += 0.25 * jaccard\n",
    "            \n",
    "            # Target 매칭 (가중치 0.2)\n",
    "            if 'target' in user_input and user_input['target']:\n",
    "                user_target = set(user_input['target'] if isinstance(user_input['target'], list) \n",
    "                                else [user_input['target']])\n",
    "                place_target = set(row['target'])\n",
    "                \n",
    "                if user_target and place_target:\n",
    "                    intersection = len(user_target & place_target)\n",
    "                    score += 0.2 * (intersection / len(user_target))\n",
    "            \n",
    "            scores[idx] = score\n",
    "        \n",
    "        # 정규화\n",
    "        if scores.max() > 0:\n",
    "            scores = scores / scores.max()\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def _calculate_predicted_scores(self, user_embedding: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"XGBoost 예측 기반 스코어\"\"\"\n",
    "        # 간단히 유사도 기반으로 계산 (실제로는 더 복잡한 로직 가능)\n",
    "        return np.ones(len(self.df)) * 0.5\n",
    "    \n",
    "    def recommend(self, user_input: Dict, top_n: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"추천 실행\"\"\"\n",
    "        print(f\"\\n🎯 추천 생성 중...\")\n",
    "        print(f\"사용자 입력: {user_input}\")\n",
    "        \n",
    "        # 사용자 쿼리 임베딩\n",
    "        user_embedding = self.encode_user_query(user_input)\n",
    "        \n",
    "        # 스코어 계산\n",
    "        final_scores, sim_scores, tag_scores, pred_scores = \\\n",
    "            self.calculate_advanced_scores(user_input, user_embedding)\n",
    "        \n",
    "        # 상위 N개 선택\n",
    "        top_indices = np.argsort(final_scores)[::-1][:top_n]\n",
    "        \n",
    "        # 결과 DataFrame 생성\n",
    "        recommendations = self.df.iloc[top_indices].copy()\n",
    "        recommendations['final_score'] = final_scores[top_indices]\n",
    "        recommendations['similarity_score'] = sim_scores[top_indices]\n",
    "        recommendations['tag_score'] = tag_scores[top_indices]\n",
    "        \n",
    "        print(f\"✅ 추천 완료! 상위 {top_n}개 선정\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# 추천 시스템 초기화\n",
    "recommender = EnhancedRecommendationSystem(\n",
    "    df=df,\n",
    "    embeddings=enhanced_features,\n",
    "    models={'season': season_model, 'nature': models['nature'], \n",
    "            'vibe': models['vibe'], 'target': models['target']},\n",
    "    encoders={'season': season_encoder, 'nature': nature_encoder,\n",
    "              'vibe': vibe_encoder, 'target': target_encoder},\n",
    "    sbert_model=ensemble_embedder.primary_model,\n",
    "    pca_model=pca_model,\n",
    "    engineer=engineer\n",
    ")\n",
    "\n",
    "print(\"✅ 개선된 추천 시스템 초기화 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209309e8-c4ae-4a91-9a98-f9143aacae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 개선된 모델 저장\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"💾 모델 저장 중...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# 저장 디렉토리 생성\n",
    "os.makedirs('models/enhanced', exist_ok=True)\n",
    "\n",
    "# 1. 임베딩 저장\n",
    "np.save('models/enhanced/enhanced_embeddings.npy', enhanced_features)\n",
    "print(\"✅ 임베딩 저장 완료\")\n",
    "\n",
    "# 2. XGBoost 모델 저장\n",
    "joblib.dump(season_model, 'models/enhanced/season_model.joblib')\n",
    "joblib.dump(models['nature'], 'models/enhanced/nature_model.joblib')\n",
    "joblib.dump(models['vibe'], 'models/enhanced/vibe_model.joblib')\n",
    "joblib.dump(models['target'], 'models/enhanced/target_model.joblib')\n",
    "print(\"✅ XGBoost 모델 저장 완료\")\n",
    "\n",
    "# 3. 인코더 저장\n",
    "joblib.dump(season_encoder, 'models/enhanced/season_encoder.joblib')\n",
    "joblib.dump(nature_encoder, 'models/enhanced/nature_encoder.joblib')\n",
    "joblib.dump(vibe_encoder, 'models/enhanced/vibe_encoder.joblib')\n",
    "joblib.dump(target_encoder, 'models/enhanced/target_encoder.joblib')\n",
    "print(\"✅ 인코더 저장 완료\")\n",
    "\n",
    "# 4. PCA 모델 저장\n",
    "joblib.dump(pca_model, 'models/enhanced/pca_model.joblib')\n",
    "print(\"✅ PCA 모델 저장 완료\")\n",
    "\n",
    "# 5. 데이터프레임 저장\n",
    "df.to_csv('models/enhanced/processed_data.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"✅ 데이터 저장 완료\")\n",
    "\n",
    "print(f\"\\n🎉 모든 모델 저장 완료!\")\n",
    "print(f\"저장 위치: models/enhanced/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10534e49-f06c-4e01-b2f8-db777d97cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 성능 평가를 위한 종합 비교 시스템\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"🔬 추천 시스템 성능 평가\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51275afe-65ff-4e64-b4d5-cffcd4c5c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 기본 추천 시스템 - 개선 전\n",
    "class BasicRecommendationSystem:\n",
    "    \"\"\"기본 추천 시스템 (비교용)\"\"\"\n",
    "    \n",
    "    def __init__(self, df, sbert_model):\n",
    "        self.df = df\n",
    "        self.sbert_model = sbert_model\n",
    "        \n",
    "        # 기본 임베딩 생성 (단순)\n",
    "        descriptions = df['short_description'].fillna('').astype(str).tolist()\n",
    "        print(\"📝 기본 임베딩 생성 중...\")\n",
    "        self.place_embeddings = sbert_model.encode(\n",
    "            descriptions,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        print(f\"✅ 기본 임베딩 완료: {self.place_embeddings.shape}\")\n",
    "    \n",
    "    def recommend(self, user_input: Dict, top_n: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"기본 추천 (단순 코사인 유사도만 사용)\"\"\"\n",
    "        \n",
    "        # 사용자 쿼리 생성\n",
    "        query_parts = []\n",
    "        if 'season' in user_input:\n",
    "            query_parts.append(user_input['season'])\n",
    "        for key in ['nature', 'vibe', 'target']:\n",
    "            if key in user_input:\n",
    "                values = user_input[key]\n",
    "                if isinstance(values, list):\n",
    "                    query_parts.extend(values)\n",
    "                else:\n",
    "                    query_parts.append(values)\n",
    "        \n",
    "        query_text = ' '.join(query_parts) if query_parts else \"관광지\"\n",
    "        \n",
    "        # 쿼리 임베딩\n",
    "        query_embedding = self.sbert_model.encode(\n",
    "            [query_text],\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        \n",
    "        # 코사인 유사도만 사용\n",
    "        similarities = cosine_similarity(query_embedding, self.place_embeddings)[0]\n",
    "        \n",
    "        # 상위 N개 선택\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "        recommendations = self.df.iloc[top_indices].copy()\n",
    "        recommendations['score'] = similarities[top_indices]\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "print(\"\\n📦 기본 추천 시스템 준비 중...\")\n",
    "basic_system = BasicRecommendationSystem(df, ensemble_embedder.primary_model)\n",
    "print(\"✅ 기본 시스템 준비 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94203fdd-f48a-4856-9176-77ef373f4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 성능 평가 메트릭\n",
    "class RecommendationEvaluator:\n",
    "    \"\"\"추천 시스템 평가 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def evaluate_system(self, system, test_cases: List[Dict], \n",
    "                       system_name: str = \"System\") -> Dict:\n",
    "        \"\"\"시스템 종합 평가\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"🔍 {system_name} 평가 중...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        results = {\n",
    "            'system_name': system_name,\n",
    "            'precision_at_3': [],\n",
    "            'precision_at_5': [],\n",
    "            'recall_at_3': [],\n",
    "            'recall_at_5': [],\n",
    "            'ndcg_at_5': [],\n",
    "            'mrr': [],\n",
    "            'diversity': [],\n",
    "            'avg_time': [],\n",
    "            'tag_match_rate': []\n",
    "        }\n",
    "        \n",
    "        for i, test in enumerate(test_cases, 1):\n",
    "            print(f\"\\n테스트 케이스 {i}/{len(test_cases)}: {test['name']}\")\n",
    "            \n",
    "            # 추천 시간 측정\n",
    "            start_time = time.time()\n",
    "            recommendations = system.recommend(test['input'], top_n=5)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            results['avg_time'].append(elapsed_time)\n",
    "            \n",
    "            # Ground Truth 생성 (실제로는 사용자 피드백 데이터 사용)\n",
    "            ground_truth = self._generate_ground_truth(test['input'])\n",
    "            \n",
    "            # 각 메트릭 계산\n",
    "            precision_3 = self._precision_at_k(recommendations, ground_truth, 3)\n",
    "            precision_5 = self._precision_at_k(recommendations, ground_truth, 5)\n",
    "            recall_3 = self._recall_at_k(recommendations, ground_truth, 3)\n",
    "            recall_5 = self._recall_at_k(recommendations, ground_truth, 5)\n",
    "            ndcg = self._ndcg_at_k(recommendations, ground_truth, 5)\n",
    "            mrr = self._mrr(recommendations, ground_truth)\n",
    "            diversity = self._diversity_score(recommendations)\n",
    "            tag_match = self._tag_match_rate(recommendations, test['input'])\n",
    "            \n",
    "            results['precision_at_3'].append(precision_3)\n",
    "            results['precision_at_5'].append(precision_5)\n",
    "            results['recall_at_3'].append(recall_3)\n",
    "            results['recall_at_5'].append(recall_5)\n",
    "            results['ndcg_at_5'].append(ndcg)\n",
    "            results['mrr'].append(mrr)\n",
    "            results['diversity'].append(diversity)\n",
    "            results['tag_match_rate'].append(tag_match)\n",
    "            \n",
    "            print(f\"  ⏱️  시간: {elapsed_time:.4f}초\")\n",
    "            print(f\"  📊 Precision@5: {precision_5:.3f}\")\n",
    "            print(f\"  📊 태그 매칭률: {tag_match:.3f}\")\n",
    "        \n",
    "        # 평균 계산\n",
    "        summary = {\n",
    "            'system_name': system_name,\n",
    "            'avg_precision_at_3': np.mean(results['precision_at_3']),\n",
    "            'avg_precision_at_5': np.mean(results['precision_at_5']),\n",
    "            'avg_recall_at_3': np.mean(results['recall_at_3']),\n",
    "            'avg_recall_at_5': np.mean(results['recall_at_5']),\n",
    "            'avg_ndcg_at_5': np.mean(results['ndcg_at_5']),\n",
    "            'avg_mrr': np.mean(results['mrr']),\n",
    "            'avg_diversity': np.mean(results['diversity']),\n",
    "            'avg_time': np.mean(results['avg_time']),\n",
    "            'avg_tag_match_rate': np.mean(results['tag_match_rate'])\n",
    "        }\n",
    "        \n",
    "        return summary, results\n",
    "    \n",
    "    def _generate_ground_truth(self, user_input: Dict) -> List[str]:\n",
    "        \"\"\"Ground Truth 생성 (실제 정답 데이터)\"\"\"\n",
    "        # 실제로는 사용자 피드백 데이터를 사용해야 하지만,\n",
    "        # 여기서는 태그가 정확히 일치하는 장소들을 정답으로 간주\n",
    "        \n",
    "        relevant_places = []\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            match_score = 0\n",
    "            \n",
    "            # Season 매칭\n",
    "            if 'season' in user_input and row['season'] == user_input['season']:\n",
    "                match_score += 1\n",
    "            \n",
    "            # Nature 매칭\n",
    "            if 'nature' in user_input:\n",
    "                user_nature = set(user_input['nature'] if isinstance(user_input['nature'], list) \n",
    "                                else [user_input['nature']])\n",
    "                place_nature = set(row['nature'])\n",
    "                if user_nature & place_nature:\n",
    "                    match_score += len(user_nature & place_nature)\n",
    "            \n",
    "            # Vibe 매칭\n",
    "            if 'vibe' in user_input:\n",
    "                user_vibe = set(user_input['vibe'] if isinstance(user_input['vibe'], list) \n",
    "                              else [user_input['vibe']])\n",
    "                place_vibe = set(row['vibe'])\n",
    "                if user_vibe & place_vibe:\n",
    "                    match_score += len(user_vibe & place_vibe)\n",
    "            \n",
    "            # Target 매칭\n",
    "            if 'target' in user_input:\n",
    "                user_target = set(user_input['target'] if isinstance(user_input['target'], list) \n",
    "                                else [user_input['target']])\n",
    "                place_target = set(row['target'])\n",
    "                if user_target & place_target:\n",
    "                    match_score += 1\n",
    "            \n",
    "            # 매칭 점수가 2 이상이면 관련 있는 장소로 간주\n",
    "            if match_score >= 2:\n",
    "                relevant_places.append(row['name'])\n",
    "        \n",
    "        return relevant_places[:10]  # 최대 10개\n",
    "    \n",
    "    def _precision_at_k(self, recommendations: pd.DataFrame, \n",
    "                       ground_truth: List[str], k: int) -> float:\n",
    "        \"\"\"Precision@K\"\"\"\n",
    "        if len(recommendations) < k:\n",
    "            k = len(recommendations)\n",
    "        \n",
    "        top_k_names = recommendations['name'].iloc[:k].tolist()\n",
    "        relevant_in_top_k = len([name for name in top_k_names if name in ground_truth])\n",
    "        \n",
    "        return relevant_in_top_k / k if k > 0 else 0.0\n",
    "    \n",
    "    def _recall_at_k(self, recommendations: pd.DataFrame, \n",
    "                    ground_truth: List[str], k: int) -> float:\n",
    "        \"\"\"Recall@K\"\"\"\n",
    "        if not ground_truth:\n",
    "            return 0.0\n",
    "        \n",
    "        top_k_names = recommendations['name'].iloc[:k].tolist()\n",
    "        relevant_in_top_k = len([name for name in top_k_names if name in ground_truth])\n",
    "        \n",
    "        return relevant_in_top_k / len(ground_truth)\n",
    "    \n",
    "    def _ndcg_at_k(self, recommendations: pd.DataFrame, \n",
    "                   ground_truth: List[str], k: int) -> float:\n",
    "        \"\"\"NDCG@K (Normalized Discounted Cumulative Gain)\"\"\"\n",
    "        top_k_names = recommendations['name'].iloc[:k].tolist()\n",
    "        \n",
    "        # DCG 계산\n",
    "        dcg = sum([\n",
    "            (1.0 if name in ground_truth else 0.0) / np.log2(i + 2)\n",
    "            for i, name in enumerate(top_k_names)\n",
    "        ])\n",
    "        \n",
    "        # IDCG 계산 (이상적인 순서)\n",
    "        ideal_length = min(len(ground_truth), k)\n",
    "        idcg = sum([1.0 / np.log2(i + 2) for i in range(ideal_length)])\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    def _mrr(self, recommendations: pd.DataFrame, ground_truth: List[str]) -> float:\n",
    "        \"\"\"MRR (Mean Reciprocal Rank)\"\"\"\n",
    "        for i, name in enumerate(recommendations['name']):\n",
    "            if name in ground_truth:\n",
    "                return 1.0 / (i + 1)\n",
    "        return 0.0\n",
    "    \n",
    "    def _diversity_score(self, recommendations: pd.DataFrame) -> float:\n",
    "        \"\"\"추천 다양성 점수\"\"\"\n",
    "        all_tags = set()\n",
    "        \n",
    "        for idx, row in recommendations.iterrows():\n",
    "            all_tags.update(row.get('nature', []))\n",
    "            all_tags.update(row.get('vibe', []))\n",
    "            all_tags.update(row.get('target', []))\n",
    "        \n",
    "        # 고유 태그 수 / (추천 수 * 평균 태그 수)\n",
    "        avg_tags_per_place = 3  # 대략적인 평균\n",
    "        max_possible_tags = len(recommendations) * avg_tags_per_place\n",
    "        \n",
    "        return len(all_tags) / max_possible_tags if max_possible_tags > 0 else 0.0\n",
    "    \n",
    "    def _tag_match_rate(self, recommendations: pd.DataFrame, \n",
    "                       user_input: Dict) -> float:\n",
    "        \"\"\"태그 매칭률 (사용자 입력과 추천 결과의 태그 일치도)\"\"\"\n",
    "        total_matches = 0\n",
    "        total_possible = 0\n",
    "        \n",
    "        for idx, row in recommendations.iterrows():\n",
    "            # Season\n",
    "            if 'season' in user_input:\n",
    "                total_possible += 1\n",
    "                if row['season'] == user_input['season']:\n",
    "                    total_matches += 1\n",
    "            \n",
    "            # Nature, Vibe, Target\n",
    "            for key in ['nature', 'vibe', 'target']:\n",
    "                if key in user_input and user_input[key]:\n",
    "                    user_tags = set(user_input[key] if isinstance(user_input[key], list) \n",
    "                                  else [user_input[key]])\n",
    "                    place_tags = set(row.get(key, []))\n",
    "                    \n",
    "                    if user_tags:\n",
    "                        total_possible += len(user_tags)\n",
    "                        total_matches += len(user_tags & place_tags)\n",
    "        \n",
    "        return total_matches / total_possible if total_possible > 0 else 0.0\n",
    "\n",
    "# 평가자 생성\n",
    "evaluator = RecommendationEvaluator(df)\n",
    "print(\"✅ 평가자 준비 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b6f6b-876d-4a09-93e7-fd3b9a96592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 다양한 테스트 케이스 정의\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"겨울 바다 데이트\",\n",
    "        \"input\": {\n",
    "            \"season\": \"겨울\",\n",
    "            \"nature\": [\"바다\"],\n",
    "            \"vibe\": [\"감성\", \"산책\"],\n",
    "            \"target\": [\"연인\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"여름 가족 해변 휴가\",\n",
    "        \"input\": {\n",
    "            \"season\": \"여름\",\n",
    "            \"nature\": [\"바다\", \"자연\"],\n",
    "            \"vibe\": [\"힐링\", \"액티비티\"],\n",
    "            \"target\": [\"가족\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"가을 산 힐링\",\n",
    "        \"input\": {\n",
    "            \"season\": \"가을\",\n",
    "            \"nature\": [\"산\"],\n",
    "            \"vibe\": [\"조용한\", \"힐링\"],\n",
    "            \"target\": [\"친구\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"봄 자연 산책\",\n",
    "        \"input\": {\n",
    "            \"season\": \"봄\",\n",
    "            \"nature\": [\"자연\", \"산\"],\n",
    "            \"vibe\": [\"산책\"],\n",
    "            \"target\": [\"연인\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"사계절 감성 여행\",\n",
    "        \"input\": {\n",
    "            \"season\": \"사계절\",\n",
    "            \"nature\": [\"호수\", \"자연\"],\n",
    "            \"vibe\": [\"감성\", \"사진명소\"],\n",
    "            \"target\": [\"친구\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"여름 스릴 모험\",\n",
    "        \"input\": {\n",
    "            \"season\": \"여름\",\n",
    "            \"nature\": [\"산\", \"바다\"],\n",
    "            \"vibe\": [\"스릴\", \"액티비티\"],\n",
    "            \"target\": [\"친구\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"겨울 가족 스키\",\n",
    "        \"input\": {\n",
    "            \"season\": \"겨울\",\n",
    "            \"nature\": [\"산\"],\n",
    "            \"vibe\": [\"액티비티\", \"스릴\"],\n",
    "            \"target\": [\"가족\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"가을 역사 탐방\",\n",
    "        \"input\": {\n",
    "            \"season\": \"가을\",\n",
    "            \"nature\": [\"자연\"],\n",
    "            \"vibe\": [\"조용한\"],\n",
    "            \"target\": [\"가족\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\n✅ 테스트 케이스 준비 완료: {len(test_cases)}개\")\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"  {i}. {test['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17361792-28dc-48e5-9fdd-91891c8e5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 기본 시스템 vs 개선 시스템 성능 비교\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"⚔️  성능 비교 실행\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. 기본 시스템 평가\n",
    "print(\"\\n🔵 [1/2] 기본 추천 시스템 평가 중...\")\n",
    "basic_summary, basic_details = evaluator.evaluate_system(\n",
    "    basic_system, \n",
    "    test_cases, \n",
    "    system_name=\"기본 시스템\"\n",
    ")\n",
    "\n",
    "# 2. 개선 시스템 평가\n",
    "print(\"\\n🟢 [2/2] 개선 추천 시스템 평가 중...\")\n",
    "enhanced_summary, enhanced_details = evaluator.evaluate_system(\n",
    "    recommender, \n",
    "    test_cases, \n",
    "    system_name=\"개선 시스템\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ 모든 평가 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1881d9-8bd3-4bc2-81be-b602d66f2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결과 비교표 생성\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 성능 비교 결과\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# DataFrame으로 변환\n",
    "comparison_df = pd.DataFrame([basic_summary, enhanced_summary])\n",
    "comparison_df = comparison_df.set_index('system_name')\n",
    "\n",
    "# 개선율 계산\n",
    "improvement = {}\n",
    "for col in comparison_df.columns:\n",
    "    basic_val = comparison_df.loc['기본 시스템', col]\n",
    "    enhanced_val = comparison_df.loc['개선 시스템', col]\n",
    "    \n",
    "    if col == 'avg_time':\n",
    "        # 시간은 감소가 좋음\n",
    "        improvement[col] = ((basic_val - enhanced_val) / basic_val * 100)\n",
    "    else:\n",
    "        # 나머지는 증가가 좋음\n",
    "        improvement[col] = ((enhanced_val - basic_val) / basic_val * 100) if basic_val > 0 else 0\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n📋 주요 메트릭 비교:\\n\")\n",
    "print(f\"{'메트릭':<30} {'기본 시스템':>15} {'개선 시스템':>15} {'개선율':>15}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metric_names = {\n",
    "    'avg_precision_at_5': 'Precision@5',\n",
    "    'avg_recall_at_5': 'Recall@5',\n",
    "    'avg_ndcg_at_5': 'NDCG@5',\n",
    "    'avg_mrr': 'MRR',\n",
    "    'avg_diversity': '다양성',\n",
    "    'avg_tag_match_rate': '태그 매칭률',\n",
    "    'avg_time': '평균 처리 시간 (초)'\n",
    "}\n",
    "\n",
    "for col, name in metric_names.items():\n",
    "    basic_val = comparison_df.loc['기본 시스템', col]\n",
    "    enhanced_val = comparison_df.loc['개선 시스템', col]\n",
    "    improve = improvement[col]\n",
    "    \n",
    "    if col == 'avg_time':\n",
    "        print(f\"{name:<30} {basic_val:>15.4f} {enhanced_val:>15.4f} {improve:>14.1f}%↓\")\n",
    "    else:\n",
    "        print(f\"{name:<30} {basic_val:>15.4f} {enhanced_val:>15.4f} {improve:>14.1f}%↑\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 전체 성능 향상 계산\n",
    "key_metrics = ['avg_precision_at_5', 'avg_recall_at_5', 'avg_ndcg_at_5', 'avg_tag_match_rate']\n",
    "avg_improvement = np.mean([improvement[m] for m in key_metrics])\n",
    "\n",
    "print(f\"\\n🎯 종합 성능 향상: {avg_improvement:.1f}%\")\n",
    "\n",
    "# 성능 등급 매기기\n",
    "if avg_improvement >= 30:\n",
    "    grade = \"🏆 탁월한 개선\"\n",
    "elif avg_improvement >= 20:\n",
    "    grade = \"🥇 우수한 개선\"\n",
    "elif avg_improvement >= 10:\n",
    "    grade = \"🥈 좋은 개선\"\n",
    "else:\n",
    "    grade = \"🥉 보통 개선\"\n",
    "\n",
    "print(f\"평가: {grade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1499c-3469-4df6-a8f1-13524f6cd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 성능 비교 시각화\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 시각화 생성 중...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 한글 폰트 설정 (Windows)\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 1. 메트릭별 비교 그래프\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('🔬 추천 시스템 성능 비교', fontsize=20, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = [\n",
    "    ('avg_precision_at_5', 'Precision@5', axes[0, 0]),\n",
    "    ('avg_recall_at_5', 'Recall@5', axes[0, 1]),\n",
    "    ('avg_ndcg_at_5', 'NDCG@5', axes[0, 2]),\n",
    "    ('avg_mrr', 'MRR', axes[1, 0]),\n",
    "    ('avg_diversity', '다양성', axes[1, 1]),\n",
    "    ('avg_tag_match_rate', '태그 매칭률', axes[1, 2])\n",
    "]\n",
    "\n",
    "for col, title, ax in metrics_to_plot:\n",
    "    values = [comparison_df.loc['기본 시스템', col], \n",
    "              comparison_df.loc['개선 시스템', col]]\n",
    "    colors = ['#3498db', '#2ecc71']\n",
    "    \n",
    "    bars = ax.bar(['기본', '개선'], values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('점수', fontsize=12)\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 값 표시\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{values[i]:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 개선율 표시\n",
    "    improve_pct = improvement[col]\n",
    "    ax.text(0.5, 0.95, f'개선: {improve_pct:+.1f}%',\n",
    "            transform=ax.transAxes,\n",
    "            ha='center', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5),\n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✅ 그래프 저장: performance_comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "# 2. 테스트 케이스별 성능 비교\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(test_cases))\n",
    "width = 0.35\n",
    "\n",
    "basic_scores = basic_details['precision_at_5']\n",
    "enhanced_scores = enhanced_details['precision_at_5']\n",
    "\n",
    "bars1 = ax.bar(x - width/2, basic_scores, width, label='기본 시스템', \n",
    "               color='#3498db', alpha=0.7, edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, enhanced_scores, width, label='개선 시스템', \n",
    "               color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('테스트 케이스', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Precision@5', fontsize=12, fontweight='bold')\n",
    "ax.set_title('📊 테스트 케이스별 Precision@5 비교', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"TC{i+1}\" for i in range(len(test_cases))], rotation=0)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 값 표시\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_case_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✅ 그래프 저장: test_case_comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "# 3. 레이더 차트\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "categories = ['Precision', 'Recall', 'NDCG', 'MRR', '다양성', '태그매칭']\n",
    "metrics_radar = ['avg_precision_at_5', 'avg_recall_at_5', 'avg_ndcg_at_5', \n",
    "                 'avg_mrr', 'avg_diversity', 'avg_tag_match_rate']\n",
    "\n",
    "basic_values = [comparison_df.loc['기본 시스템', m] for m in metrics_radar]\n",
    "enhanced_values = [comparison_df.loc['개선 시스템', m] for m in metrics_radar]\n",
    "\n",
    "# 각도 설정\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "basic_values += basic_values[:1]\n",
    "enhanced_values += enhanced_values[:1]\n",
    "angles += angles[:1]\n",
    "\n",
    "ax.plot(angles, basic_values, 'o-', linewidth=2, label='기본 시스템', color='#3498db')\n",
    "ax.fill(angles, basic_values, alpha=0.25, color='#3498db')\n",
    "ax.plot(angles, enhanced_values, 'o-', linewidth=2, label='개선 시스템', color='#2ecc71')\n",
    "ax.fill(angles, enhanced_values, alpha=0.25, color='#2ecc71')\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('🎯 종합 성능 레이더 차트', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('radar_chart.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✅ 그래프 저장: radar_chart.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ 모든 시각화 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e60980-7d5c-41eb-a6db-748585954d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 구체적인 추천 결과 비교\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🔍 상세 사례 분석\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 첫 번째 테스트 케이스로 상세 비교\n",
    "test_case = test_cases[0]\n",
    "\n",
    "print(f\"\\n📌 테스트 케이스: {test_case['name']}\")\n",
    "print(f\"입력: {test_case['input']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"🔵 기본 시스템 추천 결과:\")\n",
    "print(\"-\"*80)\n",
    "basic_recs = basic_system.recommend(test_case['input'], top_n=5)\n",
    "for i, (idx, row) in enumerate(basic_recs.iterrows(), 1):\n",
    "    print(f\"\\n{i}. {row['name']}\")\n",
    "    print(f\"   계절: {row['season']}\")\n",
    "    print(f\"   자연: {', '.join(row['nature'])}\")\n",
    "    print(f\"   분위기: {', '.join(row['vibe'])}\")\n",
    "    print(f\"   점수: {row['score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"🟢 개선 시스템 추천 결과:\")\n",
    "print(\"-\"*80)\n",
    "enhanced_recs = recommender.recommend(test_case['input'], top_n=5)\n",
    "for i, (idx, row) in enumerate(enhanced_recs.iterrows(), 1):\n",
    "    print(f\"\\n{i}. {row['name']}\")\n",
    "    print(f\"   계절: {row['season']}\")\n",
    "    print(f\"   자연: {', '.join(row['nature'])}\")\n",
    "    print(f\"   분위기: {', '.join(row['vibe'])}\")\n",
    "    print(f\"   대상: {', '.join(row['target']) if row['target'] else '정보없음'}\")\n",
    "    print(f\"   최종 점수: {row['final_score']:.4f}\")\n",
    "    print(f\"   (유사도: {row['similarity_score']:.3f}, 태그: {row['tag_score']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"📊 비교 분석:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# 태그 매칭 분석\n",
    "def analyze_tag_matching(recs, user_input):\n",
    "    matches = {'season': 0, 'nature': 0, 'vibe': 0, 'target': 0}\n",
    "    total = len(recs)\n",
    "    \n",
    "    for idx, row in recs.iterrows():\n",
    "        if row['season'] == user_input.get('season'):\n",
    "            matches['season'] += 1\n",
    "        \n",
    "        for key in ['nature', 'vibe', 'target']:\n",
    "            if key in user_input and user_input[key]:\n",
    "                user_tags = set(user_input[key] if isinstance(user_input[key], list) \n",
    "                              else [user_input[key]])\n",
    "                place_tags = set(row.get(key, []))\n",
    "                if user_tags & place_tags:\n",
    "                    matches[key] += 1\n",
    "    \n",
    "    return {k: v/total for k, v in matches.items()}\n",
    "\n",
    "basic_matches = analyze_tag_matching(basic_recs, test_case['input'])\n",
    "enhanced_matches = analyze_tag_matching(enhanced_recs, test_case['input'])\n",
    "\n",
    "print(\"\\n태그 매칭률 비교:\")\n",
    "print(f\"{'카테고리':<15} {'기본 시스템':>15} {'개선 시스템':>15} {'개선':>15}\")\n",
    "print(\"-\"*65)\n",
    "for key in ['season', 'nature', 'vibe', 'target']:\n",
    "    basic_val = basic_matches[key]\n",
    "    enhanced_val = enhanced_matches[key]\n",
    "    improve = ((enhanced_val - basic_val) / basic_val * 100) if basic_val > 0 else 0\n",
    "    print(f\"{key:<15} {basic_val:>14.1%} {enhanced_val:>14.1%} {improve:>13.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5788bb8-e3bb-467e-8d80-1206c47ac0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 최종 성능 리포트\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📋 최종 성능 평가 리포트\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════════════════╗\n",
    "║                        🏆 성능 평가 최종 요약                               ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                           ║\n",
    "║  📊 주요 메트릭 개선율:                                                     ║\n",
    "║  ────────────────────────────────────────────────────────────────────     ║\n",
    "║  • Precision@5        : {improvement['avg_precision_at_5']:>6.1f}% ↑                                  ║\n",
    "║  • Recall@5           : {improvement['avg_recall_at_5']:>6.1f}% ↑                                  ║\n",
    "║  • NDCG@5             : {improvement['avg_ndcg_at_5']:>6.1f}% ↑                                  ║\n",
    "║  • 태그 매칭률         : {improvement['avg_tag_match_rate']:>6.1f}% ↑                                  ║\n",
    "║                                                                           ║\n",
    "║  ⚡ 성능 지표:                                                              ║\n",
    "║  ────────────────────────────────────────────────────────────────────     ║\n",
    "║  • 평균 처리 시간      : {comparison_df.loc['개선 시스템', 'avg_time']:.4f}초                                    ║\n",
    "║  • 추천 다양성         : {comparison_df.loc['개선 시스템', 'avg_diversity']:.3f}                                       ║\n",
    "║  • MRR                : {comparison_df.loc['개선 시스템', 'avg_mrr']:.3f}                                       ║\n",
    "║                                                                           ║\n",
    "║  🎯 종합 평가:                                                             ║\n",
    "║  ────────────────────────────────────────────────────────────────────     ║\n",
    "║  • 전체 성능 향상      : {avg_improvement:>6.1f}%                                        ║\n",
    "║  • 평가 등급           : {grade:<20}                          ║\n",
    "║                                                                           ║\n",
    "║  💡 주요 개선 사항:                                                         ║\n",
    "║  ────────────────────────────────────────────────────────────────────     ║\n",
    "║  ✓ 데이터 증강으로 설명 텍스트 품질 향상                                      ║\n",
    "║  ✓ 앙상블 임베딩으로 의미 표현력 증가                                        ║\n",
    "║  ✓ 피처 엔지니어링으로 분류 정확도 개선                                       ║\n",
    "║  ✓ 고급 스코어링으로 태그 매칭 정확도 향상                                    ║\n",
    "║  ✓ XGBoost 최적화로 예측 성능 개선                                          ║\n",
    "║                                                                           ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")\n",
    "\n",
    "# CSV로 상세 결과 저장\n",
    "results_summary = pd.DataFrame({\n",
    "    '시스템': ['기본', '개선'],\n",
    "    'Precision@5': [comparison_df.loc['기본 시스템', 'avg_precision_at_5'],\n",
    "                   comparison_df.loc['개선 시스템', 'avg_precision_at_5']],\n",
    "    'Recall@5': [comparison_df.loc['기본 시스템', 'avg_recall_at_5'],\n",
    "                comparison_df.loc['개선 시스템', 'avg_recall_at_5']],\n",
    "    'NDCG@5': [comparison_df.loc['기본 시스템', 'avg_ndcg_at_5'],\n",
    "              comparison_df.loc['개선 시스템', 'avg_ndcg_at_5']],\n",
    "    'MRR': [comparison_df.loc['기본 시스템', 'avg_mrr'],\n",
    "           comparison_df.loc['개선 시스템', 'avg_mrr']],\n",
    "    '다양성': [comparison_df.loc['기본 시스템', 'avg_diversity'],\n",
    "             comparison_df.loc['개선 시스템', 'avg_diversity']],\n",
    "    '태그매칭률': [comparison_df.loc['기본 시스템', 'avg_tag_match_rate'],\n",
    "                comparison_df.loc['개선 시스템', 'avg_tag_match_rate']],\n",
    "    '처리시간': [comparison_df.loc['기본 시스템', 'avg_time'],\n",
    "               comparison_df.loc['개선 시스템', 'avg_time']]\n",
    "})\n",
    "\n",
    "results_summary.to_csv('performance_evaluation_results.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\n💾 상세 결과 저장: performance_evaluation_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ 성능 평가 완료!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb4aa4-c8d6-4271-a452-8880e7ca07c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a7aba-da33-4c6e-9158-b1b85fddcdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445dc03e-f485-4b6c-9aee-2fad1f070335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09013ab2-2146-4b44-afaf-d976064e6ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f94e1-5e14-4a80-81ec-309ce23c6127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3246ec8-4a95-4d30-89b0-286630010cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05ccc2-4139-4673-b8be-6522d6fe29e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f42a71-bdd4-46b2-a2ab-e31a6a178a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29522932-47d3-4671-bd4c-9469b211bbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84c9fe-e292-4102-935e-bab6f3b450e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
