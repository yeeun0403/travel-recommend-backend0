{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76b5130-236d-4f97-bee8-2c1e15b081bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: requests in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (80.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "WARNING:tensorflow:From C:\\Users\\tjdwl\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò Î∞è ÏûÑÌè¨Ìä∏\n",
    "## ÌïÑÏöîÌïú ÎùºÏù¥Î≤ÑÎü¨Î¶¨Îì§Ïù¥ ÏóÜÎäî Í≤ΩÏö∞ ÏïÑÎûò Î™ÖÎ†πÏñ¥Î°ú ÏÑ§Ïπò\n",
    "!pip install sentence-transformers xgboost scikit-learn pandas numpy joblib pyyaml tqdm\n",
    "!pip install tf-keras\n",
    "!pip install sentence-transformers\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "import joblib\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## Î®∏Ïã†Îü¨Îãù ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xgboost as xgb\n",
    "\n",
    "## Î°úÍπÖ ÏÑ§Ï†ï\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c82b16f9-6612-417b-bccb-62dbbbcbfda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ ÌîÑÎ°úÏ†ùÌä∏ ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞Í∞Ä Ï§ÄÎπÑÎêòÏóàÏäµÎãàÎã§.\n",
      "   (Ìè¥ÎçîÍ∞Ä ÏóÜÎã§Î©¥ ÏúÑÏùò Ï£ºÏÑùÏùÑ Ìï¥Ï†úÌïòÍ≥† Ïã§ÌñâÌïòÏÑ∏Ïöî)\n"
     ]
    }
   ],
   "source": [
    "# Ìè¥ÎçîÍ∞Ä Ïù¥ÎØ∏ ÎßåÎì§Ïñ¥Ï†∏ ÏûàÎã§Î©¥ ÏïÑÎûò ÏΩîÎìúÎäî Ïã§ÌñâÌïòÏßÄ ÏïäÏïÑÎèÑ Îê©ÎãàÎã§.\n",
    "# ÌïÑÏöîÏãú Ï£ºÏÑùÏùÑ Ìï¥Ï†úÌïòÍ≥† Ïã§ÌñâÌïòÏÑ∏Ïöî.\n",
    "\n",
    "# def create_project_structure():\n",
    "#     \"\"\"ÌîÑÎ°úÏ†ùÌä∏ ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.\"\"\"\n",
    "#     \n",
    "#     directories = [\n",
    "#         'data/raw',\n",
    "#         'data/processed', \n",
    "#         'data/embeddings',\n",
    "#         'models/xgboost',\n",
    "#         'models/encoders',\n",
    "#         'src',\n",
    "#         'notebooks',\n",
    "#         'saved_models',\n",
    "#         'config',\n",
    "#         'logs'\n",
    "#     ]\n",
    "#     \n",
    "#     for directory in directories:\n",
    "#         Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "#     \n",
    "#     print(\"‚úÖ ÌîÑÎ°úÏ†ùÌä∏ ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞ ÏÉùÏÑ± ÏôÑÎ£å\")\n",
    "\n",
    "# create_project_structure()  # ÌïÑÏöîÏãú Ï£ºÏÑù Ìï¥Ï†ú\n",
    "\n",
    "print(\"üìÅ ÌîÑÎ°úÏ†ùÌä∏ ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞Í∞Ä Ï§ÄÎπÑÎêòÏóàÏäµÎãàÎã§.\")\n",
    "print(\"   (Ìè¥ÎçîÍ∞Ä ÏóÜÎã§Î©¥ ÏúÑÏùò Ï£ºÏÑùÏùÑ Ìï¥Ï†úÌïòÍ≥† Ïã§ÌñâÌïòÏÑ∏Ïöî)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ba1325-1757-4c23-95c2-0ae84b5f2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ÏÑ§Ï†ï ÌååÏùº ÏÉùÏÑ±\n",
    "\n",
    "config = {\n",
    "    'model' : {\n",
    "        'sbert_model' : 'snunlp/KR-SBERT-V40K-klueNLI-augSTS',\n",
    "        'embedding_dim' : 768,\n",
    "        'reduced_dim' : 128,\n",
    "        'dimensionality_reduction': 'PCA' ,\n",
    "        'xgboost_params' : {\n",
    "            'max_depth' : 6,\n",
    "            'learning_rate' : 0.1,\n",
    "            'n_estimators' : 100,\n",
    "            'random_state' : 42\n",
    "        }\n",
    "    },\n",
    "    'data' : {\n",
    "        'raw_file' : 'data/raw/gangwon_places_100.xlsx',\n",
    "        'processed_file' : 'data/processed/gangwon_places_100_processed.xlsx',\n",
    "        'embeddings_file' : 'data/embeddings/place_embeddings_pca128.npy'\n",
    "    },\n",
    "    'paths': {\n",
    "        'models' : 'models',\n",
    "        'encoders' : 'models/encoders',\n",
    "        'logs' : 'logs'\n",
    "    }\n",
    "}\n",
    "\n",
    "## config Ìè¥ÎçîÍ∞Ä ÏóÜÏúºÎ©¥ ÏÉùÏÑ±\n",
    "os.makedirs('config', exist_ok=True)\n",
    "\n",
    "## ÏÑ§Ï†ï ÌååÏùº Ï†ÄÏû•\n",
    "with open('config/config.yaml', 'w', encoding='utf-8') as f: \n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac4009e-fadd-425f-9ac0-d59c3da4c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ ÌÅ¥ÎûòÏä§ Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "## Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò Ï†ïÏùò\n",
    "\n",
    "class DataPreprocessor: \n",
    "    \"\"\"Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ ÌÅ¥ÎûòÏä§\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.season_encoder = None\n",
    "        self.nature_encoder = MultiLabelBinarizer()\n",
    "        self.vibe_encoder = MultiLabelBinarizer()\n",
    "        self.target_encoder = MultiLabelBinarizer()\n",
    "\n",
    "    def parse_multi_label_string(self, text: str) -> List[str]:\n",
    "        \"\"\"ÏâºÌëúÎ°ú Íµ¨Î∂ÑÎêú Î¨∏ÏûêÏó¥ÏùÑ Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôò\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "\n",
    "        # ÏâºÌëúÎ°ú Î∂ÑÎ¶¨ÌïòÍ≥† Í≥µÎ∞± Ï†úÍ±∞\n",
    "        items = [item.strip() for item in str(text).split(',')]\n",
    "        return [item for item in items if item] # Îπà Î¨∏ÏûêÏó¥ Ï†úÍ±∞\n",
    "\n",
    "    def _augment_single_row(self, row) -> str:\n",
    "        \"\"\"Îã®Ïùº ÌñâÏùò ÏÑ§Î™Ö ÌÖçÏä§Ìä∏ Ï¶ùÍ∞ï\"\"\"\n",
    "        original = str(row['short_description'])\n",
    "        \n",
    "        # Í≥ÑÏ†à Ï†ïÎ≥¥\n",
    "        season_text = f\"Ïù¥Í≥≥ÏùÄ {row['season']}Ïóê ÌäπÌûà ÏïÑÎ¶ÑÎãµÏäµÎãàÎã§.\"\n",
    "        \n",
    "        # ÏûêÏó∞ÌôòÍ≤Ω Ï†ïÎ≥¥\n",
    "        if row['nature_list']:\n",
    "            nature_text = f\"{', '.join(row['nature_list'])} Í≤ΩÍ¥ÄÏùÑ Ï¶êÍ∏∏ Ïàò ÏûàÏäµÎãàÎã§.\"\n",
    "        else:\n",
    "            nature_text = \"\"\n",
    "        \n",
    "        # Î∂ÑÏúÑÍ∏∞ Ï†ïÎ≥¥\n",
    "        if row['vibe_list']:\n",
    "            vibe_text = f\"{', '.join(row['vibe_list'])} Î∂ÑÏúÑÍ∏∞Î°ú Ï¢ãÏäµÎãàÎã§.\"\n",
    "        else:\n",
    "            vibe_text = \"\"\n",
    "        \n",
    "        # ÎåÄÏÉÅ Ï†ïÎ≥¥\n",
    "        if row['target_list']:\n",
    "            target_text = f\"{', '.join(row['target_list'])}ÏóêÍ≤å Ï∂îÏ≤úÌï©ÎãàÎã§.\"\n",
    "        else:\n",
    "            target_text = \"\"\n",
    "        \n",
    "        # Î™®Îì† Ï†ïÎ≥¥ Í≤∞Ìï©\n",
    "        augmented = f\"{original} {season_text} {nature_text} {vibe_text} {target_text}\"\n",
    "        \n",
    "        return augmented.strip()\n",
    "    \n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame: \n",
    "        \"\"\"Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Î©îÏù∏ Ìï®Ïàò\"\"\"\n",
    "        # Î≥µÏÇ¨Î≥∏ ÏÉùÏÑ±\n",
    "        processed_df = df.copy()\n",
    "\n",
    "        # ÌïÑÏàò Ïª¨Îüº ÌôïÏù∏\n",
    "        required_cols = ['name', 'season', 'nature', 'vibe', 'target', 'short_description']\n",
    "        missing_cols = [col for col in required_cols if col not in processed_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"ÌïÑÏàò Ïª¨ÎüºÏù¥ ÏóÜÏäµÎãàÎã§: {missing_cols}\")\n",
    "\n",
    "        # Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨\n",
    "        processed_df['short_description'] = processed_df['short_description'].fillna('')\n",
    "        processed_df['season'] = processed_df['season'].fillna('ÏÇ¨Í≥ÑÏ†à')\n",
    "        processed_df['nature'] = processed_df['nature'].fillna('')\n",
    "        processed_df['vibe'] = processed_df['vibe'].fillna('')\n",
    "        processed_df['target'] = processed_df['target'].fillna('')\n",
    "\n",
    "        # Îã§Ï§ë ÎùºÎ≤® ÌååÏã±\n",
    "        processed_df['nature_list'] = processed_df['nature'].apply(self.parse_multi_label_string)\n",
    "        processed_df['vibe_list'] = processed_df['vibe'].apply(self.parse_multi_label_string)\n",
    "        processed_df['target_list'] = processed_df['target'].apply(self.parse_multi_label_string)\n",
    "\n",
    "        # ÌÖçÏä§Ìä∏ Ï†ïÍ∑úÌôî\n",
    "        processed_df['short_description'] = processed_df['short_description'].apply(\n",
    "        lambda x: re.sub(r'[^\\w\\s]', '', str(x)) if pd.notna(x) else ''\n",
    "        )\n",
    "        return processed_df\n",
    "\n",
    "        # ‚ú® ÏÉàÎ°ú Ï∂îÍ∞Ä: Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï\n",
    "        print(\"üìù Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï Ï§ë...\")\n",
    "        processed_df['enhanced_description'] = processed_df.apply(\n",
    "            self._augment_single_row, axis=1\n",
    "        )\n",
    "        print(f\"‚úÖ Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï ÏôÑÎ£å! ({len(processed_df)}Í∞ú)\")\n",
    "        \n",
    "        return processed_df\n",
    "        \n",
    "    def fit_encoders(self, df: pd.DataFrame):\n",
    "        \"\"\"Ïù∏ÏΩîÎçîÎì§ÏùÑ ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Ïóê ÎßûÏ∂§\"\"\"\n",
    "\n",
    "        # Í≥ÑÏ†àÏùÄ Îã®Ïùº ÎùºÎ≤®Ïù¥ÎØÄÎ°ú LabelEncoder ÎåÄÏã† ÏßÅÏ†ë Ï≤òÎ¶¨\n",
    "        self.season_categories = sorted(df['season'].unique())\n",
    "\n",
    "        # Îã§Ï§ë ÎùºÎ≤® Ïù∏ÏΩîÎçî ÌïôÏäµ\n",
    "        self.nature_encoder.fit(df['nature_list'])\n",
    "        self.vibe_encoder.fit(df['vibe_list'])\n",
    "        self.target_encoder.fit(df['target_list'])\n",
    "\n",
    "        print(f\"Ïù∏ÏΩîÎçî ÌïôÏäµ ÏôÑÎ£å\")\n",
    "        print(f\"   - Í≥ÑÏ†à Ïπ¥ÌÖåÍ≥†Î¶¨: {self.season_categories}\")\n",
    "        print(f\"   - ÏûêÏó∞ÌôòÍ≤Ω Ïπ¥ÌÖåÍ≥†Î¶¨: {len(self.nature_encoder.classes_)}Í∞ú\")\n",
    "        print(f\"   - Î∂ÑÏúÑÍ∏∞ Ïπ¥ÌÖåÍ≥†Î¶¨: {len(self.vibe_encoder.classes_)}Í∞ú\")\n",
    "        print(f\"   - ÎåÄÏÉÅ Ïπ¥ÌÖåÍ≥†Î¶¨: {len(self.target_encoder.classes_)}Í∞ú\")\n",
    "\n",
    "    def encode_labels(self, df: pd.DataFrame) -> Dict[str,np.ndarray]:\n",
    "        \"\"\"ÎùºÎ≤®Îì§ÏùÑ Ïù∏ÏΩîÎî©\"\"\"\n",
    "\n",
    "        # Í≥ÑÏ†à Ïù∏ÏΩîÎî©(Ïõê-Ìï´ Ïù∏ÏΩîÎî©)\n",
    "        season_encoded = np.zeros((len(df), len(self.season_categories)))\n",
    "        for i, season in enumerate(df['season']):\n",
    "            if season in self.season_categories:\n",
    "                season_idx = self.season_categories.index(season)\n",
    "                season_encoded[i, season_idx] = 1\n",
    "\n",
    "        # Îã§Ï§ë ÎùºÎ≤® Ïù∏ÏΩîÎì±\n",
    "        nature_encoded = self.nature_encoder.transform(df['nature_list'])\n",
    "        vibe_encoded = self.vibe_encoder.transform(df['vibe_list'])\n",
    "        target_encoded = self.target_encoder.transform(df['target_list'])\n",
    "\n",
    "        return{\n",
    "            'season' : season_encoded,\n",
    "            'nature' : nature_encoded,\n",
    "            'vibe' : vibe_encoded,\n",
    "            'target' : target_encoded\n",
    "        }\n",
    "\n",
    "    def save_encoders(self, base_path: str):\n",
    "        \"\"\"Ïù∏ÏΩîÎçîÎì§ÏùÑ Ï†ÄÏû•\"\"\"\n",
    "        # Í≥ÑÏ†à Ïπ¥ÌÖåÍ≥†Î¶¨ Ï†ÄÏû•\n",
    "        joblib.dump(self.season_categories, f\"{base_path}/season_encoder.joblib\")\n",
    "\n",
    "        # Îã§Ï§ë ÎùºÎ≤® Ïù∏ÏΩîÎçî Ï†ÄÏû•\n",
    "        joblib.dump(self.nature_encoder, f\"{base_path}/nature_encoder.joblib\")\n",
    "        joblib.dump(self.vibe_encoder, f\"{base_path}/vibe_encoder.joblib\")\n",
    "        joblib.dump(self.target_encoder, f\"{base_path}/target_encoder.joblib\")\n",
    "\n",
    "        print(f\"Ïù∏ÏΩîÎçî Ï†ÄÏû• ÏôÑÎ£å: {base_path}\")\n",
    "\n",
    "    def load_encoders(self, base_path: str):\n",
    "        \"\"\"Ïù∏ÏΩîÎçî Î°úÎìú\"\"\"\n",
    "\n",
    "        self.season_categories = joblib.load(f\"{base_path}/season_encoder.joblib\")\n",
    "        self.nature_encoder = joblib.load(f\"{base_path}/nature_encoder.joblib\")\n",
    "        self.vibe_encoder = joblib.load(f\"{base_path}/vibe_encoder.joblib\")\n",
    "        self.target_encoder = joblib.load(f\"{base_path}/target_encoder.joblib\")\n",
    "\n",
    "        print(f\"Ïù∏ÏΩîÎçî Î°úÎìú ÏôÑÎ£å: {base_path}\")\n",
    "\n",
    "print(f\"Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ ÌÅ¥ÎûòÏä§ Ï†ïÏùò ÏôÑÎ£å\")\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e775a6e-6fdb-4c22-a1c7-024a7e48df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÏûÑÎ≤†Îî© ÏÉùÏÑ± ÌÅ¥ÎûòÏä§ Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "## ÏûÑÎ≤†Îî© ÏÉùÏÑ± ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"SBERT ÏûÑÎ≤†Îî© ÏÉùÏÑ± Î∞è Ï∞®Ïõê Ï∂ïÏÜå ÌÅ¥ÎûòÏä§\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.dimension_reducer = None\n",
    "        self.reduced_dim = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"SBERT Î™®Îç∏ Î°úÎìú\"\"\"\n",
    "        print(f\"SBERT Î™®Îç∏ Î°úÎìú Ï§ë: {self.model_name}\")\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        print(\"SBERT Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\")\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"ÌÖçÏä§Ìä∏ Î¶¨Ïä§Ìä∏Î°úÎ∂ÄÌÑ∞ ÏûÑÎ≤†Îî© ÏÉùÏÑ±\n",
    "         Args:\n",
    "            texts: ÌÖçÏä§Ìä∏ Î¶¨Ïä§Ìä∏\n",
    "            use_enhanced: TrueÏù¥Î©¥ Í∞ÄÏ§ëÏπò Ï†ÅÏö©, FalseÎ©¥ Í∏∞Î≥∏ Î∞©Ïãù\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "        print(f\"ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ï§ë... (Ï¥ù {len(texts)}Í∞ú ÌÖçÏä§Ìä∏)\")\n",
    "  \n",
    "         #  Í∞úÏÑ†: Í∞ÄÏ§ëÏπò Ï†ÅÏö© ÏòµÏÖò\n",
    "        if use_enhanced:\n",
    "            print(\" Í∞ÄÏ§ëÏπò Ï†ÅÏö© ÏûÑÎ≤†Îî© ÏÉùÏÑ± Î™®Îìú\")\n",
    "            # Í∞Å ÌÖçÏä§Ìä∏Î•º 3Î≤à Î∞òÎ≥µÌï¥ÏÑú Ï§ëÏöîÎèÑ Ï¶ùÍ∞Ä (ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°úÎßå)\n",
    "            # ÌïòÏßÄÎßå Ïã§Ï†úÎ°úÎäî normalizeÎ°ú ÎèôÏùºÌïú Ìö®Í≥º\n",
    "            embeddings = self.model.encode(\n",
    "                texts,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=True,  # L2 Ï†ïÍ∑úÌôîÎ°ú ÌíàÏßà Ìñ•ÏÉÅ\n",
    "                show_progress_bar=True,\n",
    "                batch_size=16  # Î∞∞Ïπò ÌÅ¨Í∏∞ ÏµúÏ†ÅÌôî\n",
    "            )\n",
    "        else:\n",
    "            # Í∏∞Ï°¥ Î∞©Ïãù\n",
    "            batch_size = 32\n",
    "            embeddings = []\n",
    "            for i in tqdm(range(0, len(texts), batch_size)):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                batch_embeddings = self.model.encode(batch_texts, convert_to_numpy=True)\n",
    "                embeddings.append(batch_embeddings)\n",
    "            embeddings = np.vstack(embeddings)\n",
    "\n",
    "        print(f\"ÏûÑÎ≤†Îî© ÏÉùÏÑ± ÏôÑÎ£å: {embeddings.shape}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def fit_dimension_reducer(self, embeddings: np.ndarray, method: str = 'PCA',\n",
    "                              target_dim: int = 128):\n",
    "        \"\"\"Ï∞®Ïõê Ï∂ïÏÜå Î™®Îç∏ ÌïôÏäµ\"\"\"\n",
    "\n",
    "        self.reduced_dim = target_dim\n",
    "\n",
    "        if method =='PCA':\n",
    "            self.dimension_reducer = PCA(n_components=target_dim, random_state=42)\n",
    "        elif method =='TruncatedSVD':\n",
    "            self.dimension_reducer = TruncatedSVD(n_components=target_dim, random_state=42)\n",
    "        else: \n",
    "            raise ValueError(f\"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Ï∞®Ïõê Ï∂ïÏÜå Î∞©Î≤ï: {method}\")\n",
    "\n",
    "        print(f\"{method}Î•º ÏÇ¨Ïö©ÌïòÏó¨ {embeddings.shape[1]}Ï∞®Ïõê -> {target_dim}Ï∞®ÏõêÏúºÎ°ú Ï∂ïÏÜå\")\n",
    "        self.dimension_reducer.fit(embeddings)\n",
    "\n",
    "        # ÏÑ§Î™Ö Î∂ÑÏÇ∞ ÎπÑÏú® Ï∂úÎ†•(PCAÏùò Í≤ΩÏö∞)\n",
    "        if method =='PCA':\n",
    "            explained_variance_ratio = self.dimension_reducer.explained_variance_ratio_\n",
    "            cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "            print(f\"ÏÑ§Î™Ö Î∂ÑÏÇ∞ ÎπÑÏú®: {cumulative_variance[-1]:.4f}\")\n",
    "\n",
    "        print(f\"Ï∞®Ïõê Ï∂ïÏÜå Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å\")\n",
    "\n",
    "    def reduce_dimensions(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ÏûÑÎ≤†Îî© Ï∞®Ïõê Ï∂ïÏÜå\"\"\"\n",
    "\n",
    "        if self.dimension_reducer is None:\n",
    "            raise ValueError(\"Ï∞®Ïõê Ï∂ïÏÜå Î™®Îç∏Ïù¥ ÌïôÏäµÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.\")\n",
    "\n",
    "        reduced_embeddings = self.dimension_reducer.transform(embeddings)\n",
    "        print(f\"Ï∞®Ïõê Ï∂ïÏÜå ÏôÑÎ£å: {embeddings.shape} -> {reduced_embeddings.shape}\")\n",
    "\n",
    "        return reduced_embeddings\n",
    "\n",
    "    def save_dimension_reducer(self, filepath: str):\n",
    "        \"\"\"Ï∞®Ïõê Ï∂ïÏÜå Î™®Îç∏ Ï†ÄÏû•\"\"\"\n",
    "\n",
    "        model_data = {\n",
    "        'reducer': self.dimension_reducer,\n",
    "        'reduced_dim' : self.reduced_dim,\n",
    "        'model_name' : self.model_name\n",
    "        }\n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"Ï∞®Ïõê Ï∂ïÏÜå Î™®Îç∏ Ï†ÄÏû•: {filepath}\")\n",
    "\n",
    "    def load_dimension_reducer(self, filepath: str):\n",
    "        \"\"\"Ï∞®Ïõê Ï∂ïÏÜå Î™®Îç∏ Î°úÎìú\"\"\"\n",
    "\n",
    "        model_data = joblib.load(filepath)\n",
    "        self.dimension_reducer = model_data['reducer']\n",
    "        self.reduced_dim = model_data['reduced_dim']\n",
    "        self.model_name = model_data['model_name']\n",
    "\n",
    "        print(f\"Ï∞®Ïõê Ï∂ïÏÜå Î™®Îç∏ Î°úÎìú: {filepath}\")\n",
    "\n",
    "print(\"ÏûÑÎ≤†Îî© ÏÉùÏÑ± ÌÅ¥ÎûòÏä§ Ï†ïÏùò ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232b2813-e02a-4589-9a3e-25c354b01830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost ÌïôÏäµ ÌÅ¥ÎûòÏä§ Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "### XGBoost ÌïôÏäµ ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
    "\n",
    "class XGBoostTrainer:\n",
    "    \"\"\"XGBoost Î∂ÑÎ•òÍ∏∞ ÌïôÏäµ ÌÅ¥ÎûòÏä§\"\"\"\n",
    "\n",
    "    def __init__(self, xgb_params: Dict):\n",
    "        self.xgb_params = xgb_params\n",
    "        self.models = {}\n",
    "        self.label_types = ['season', 'nature', 'vibe', 'target']\n",
    "\n",
    "    def train_models(self, feature: np.ndarray, labels: Dict[str, np.ndarray]):\n",
    "        \"\"\"Î™®Îì† ÎùºÎ≤® ÌÉÄÏûÖÏóê ÎåÄÌï¥ Î∂ÑÎ•òÍ∏∞ ÌïôÏäµ\"\"\"\n",
    "\n",
    "        print(\"XGBoost Î™®Îç∏ ÌïôÏäµ ÏãúÏûë...\")\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            print(f\"\\n{label_type} Î∂ÑÎ•òÍ∏∞ ÌïôÏäµ Ï§ë...\")\n",
    "\n",
    "            y = labels[label_type]\n",
    "\n",
    "            if label_type == 'season':\n",
    "                #Îã®Ïùº ÎùºÎ≤®: Ïõê-Ìï´ÏóêÏÑú ÌÅ¥ÎûòÏä§ Ïù∏Îç±Ïä§Î°ú Î≥ÄÌôò\n",
    "                y_single = np.argmax(y, axis=1)\n",
    "\n",
    "                model = xgb.XGBClassifier(**self.xgb_params)\n",
    "                model.fit(features, y_single)\n",
    "                           \n",
    "            else: \n",
    "                #Îã§Ï§ë ÎùºÎ≤®: OneVsRestClassifier ÏÇ¨Ïö©\n",
    "                model = OneVsRestClassifier(\n",
    "                    xgb.XGBClassifier(**self.xgb_params)\n",
    "                )\n",
    "                model.fit(features, y)\n",
    "\n",
    "            self.models[label_type] = model\n",
    "            print(f\"Î™®Îì† XGBoost Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å\")\n",
    "\n",
    "    def evaluate_models(self, features: np.ndarray, labels: Dict[str,np.ndarray]):\n",
    "        \"\"\"Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä\"\"\"\n",
    "\n",
    "        print(\"\\n=== Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä===\")\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            print(f\"\\n[{label_type}] ÏÑ±Îä• ÌèâÍ∞Ä: \")\n",
    "\n",
    "\n",
    "            y_true = labels[label_type]\n",
    "            model = self.models[label_type]\n",
    "\n",
    "            if label_type == 'season':\n",
    "                # Îã®Ïùº ÎùºÎ≤® ÌèâÍ∞Ä\n",
    "\n",
    "                y_true_single = np.argmax(y_true, axis=1)\n",
    "                y_pred = model.predict(features)\n",
    "\n",
    "                accuracy = accuracy_score(y_true_single, y_pred)\n",
    "                f1 = f1_score(y_true_single, y_pred, average='weighted')\n",
    "\n",
    "                print(f\"Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "            else: \n",
    "                # Îã§Ï§ë ÎùºÎ≤® ÌèâÍ∞Ä\n",
    "                y_pred = model.predict(features)\n",
    "\n",
    "                accuracy = accuracy_score(y_true, y_pred)\n",
    "                f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "                f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "                print(f\"Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"F1-Score (Micro): {f1_micro:.4f}\")\n",
    "                print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
    "                \n",
    "    def save_models(self,base_path: str):\n",
    "        \"\"\"Î™®Îç∏Îì§ Ï†ÄÏû•\"\"\"\n",
    "        for label_type in self.label_types:\n",
    "            model_path = f\"{base_path}/xgboost/{label_type}_model.joblib\"\n",
    "            joblib.dump(self.models[label_type], model_path)\n",
    "            print(f\"{label_type} Î™®Îç∏ Ï†ÄÏû•: {model_path}\")\n",
    "\n",
    "    def load_models(self,base_path: str):\n",
    "        \"\"\"Î™®Îç∏Îì§ Î°úÎìú\"\"\"\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            model_path = f\"{base_path}/xgboost/{label_type}_model.joblib\"\n",
    "            self.models[label_type] = joblib.load(model_path)\n",
    "            print(f\"{label_type} Î™®Îç∏ Î°úÎìú: {model_path}\")\n",
    "\n",
    "print(\"XGBoost ÌïôÏäµ ÌÅ¥ÎûòÏä§ Ï†ïÏùò ÏôÑÎ£å\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5e54007-c9a4-41e9-90c0-cb6ee76bd9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÏàòÏ†ïÎêú Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÌÅ¥ÎûòÏä§ Ï†ïÏùò ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "## Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
    "\n",
    "# ÏÉàÎ°úÏö¥ ÏÖÄÏóêÏÑú GangwonPlaceRecommender ÌÅ¥ÎûòÏä§ Ïû¨Ï†ïÏùò\n",
    "class GangwonPlaceRecommender:\n",
    "    \"\"\"Í∞ïÏõêÎèÑ Í¥ÄÍ¥ëÏßÄ Ï∂îÏ≤ú ÏãúÏä§ÌÖú Î©îÏù∏ ÌÅ¥ÎûòÏä§ (ÏàòÏ†ïÎêú Î≤ÑÏ†Ñ)\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = 'config/config.yaml'):\n",
    "        # ÏÑ§Ï†ï Î°úÎìú\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        \n",
    "        # Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî\n",
    "        self.preprocessor = DataPreprocessor()\n",
    "        self.embedding_generator = EmbeddingGenerator(\n",
    "            self.config['model']['sbert_model']\n",
    "        )\n",
    "        self.xgb_trainer = XGBoostTrainer(\n",
    "            self.config['model']['xgboost_params']\n",
    "        )\n",
    "        \n",
    "        # Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•Ïö©\n",
    "        self.df = None\n",
    "        self.place_embeddings = None\n",
    "        self.place_names = None\n",
    "        \n",
    "        # ÌÉúÍ∑∏ Îß§Ìïë (ÏûêÏú† Î¨∏Ïû• ÌååÏã±Ïö©)\n",
    "        self.tag_mapping = {\n",
    "            'season': {\n",
    "                'Î¥Ñ': ['Î¥Ñ', '3Ïõî', '4Ïõî', '5Ïõî', 'Î≤öÍΩÉ', 'ÍΩÉ'],\n",
    "                'Ïó¨Î¶Ñ': ['Ïó¨Î¶Ñ', '6Ïõî', '7Ïõî', '8Ïõî', 'Î∞îÎã§', 'Ìï¥Î≥Ä', 'ÏãúÏõê', 'Î¨º'],\n",
    "                'Í∞ÄÏùÑ': ['Í∞ÄÏùÑ', '9Ïõî', '10Ïõî', '11Ïõî', 'Îã®Ìíç', 'ÏñµÏÉà', 'Îπ®Í∞Ñ'],\n",
    "                'Í≤®Ïö∏': ['Í≤®Ïö∏', '12Ïõî', '1Ïõî', '2Ïõî', 'Îàà', 'Ïä§ÌÇ§', 'Ï∂îÏö¥'],\n",
    "                'ÏÇ¨Í≥ÑÏ†à': ['ÏÇ¨Í≥ÑÏ†à', 'Ïó∞Ï§ë', 'Ïñ∏Ï†úÎÇò']\n",
    "            },\n",
    "            'nature': {\n",
    "                'ÏÇ∞': ['ÏÇ∞', 'Îì±ÏÇ∞', 'Ìä∏Î†àÌÇπ', 'ÌïòÏù¥ÌÇπ', 'ÏÇ∞Ï±Ö', 'Ïò§Î•¥Îßâ'],\n",
    "                'Î∞îÎã§': ['Î∞îÎã§', 'Ìï¥Î≥Ä', 'Î∞îÎã∑Í∞Ä', 'ÏàòÏòÅ', 'ÌååÎèÑ'],\n",
    "                'Ìò∏Ïàò': ['Ìò∏Ïàò', 'Ïó∞Î™ª', 'Î¨ºÍ∞Ä', 'Ï†ÄÏàòÏßÄ'],\n",
    "                'Í≥ÑÍ≥°': ['Í≥ÑÍ≥°', 'ÏãúÎÉáÎ¨º', 'Í∞úÏö∏', 'Î¨ºÏÜåÎ¶¨'],\n",
    "                'ÏûêÏó∞': ['ÏûêÏó∞', 'Ïà≤', 'ÎÇòÎ¨¥', 'ÌíÄ', 'ÏãùÎ¨º'],\n",
    "                'ÎèÑÏãú': ['ÎèÑÏãú', 'ÏãúÎÇ¥', 'Î≤àÌôîÍ∞Ä', 'ÏÉÅÏ†ê']\n",
    "            },\n",
    "            'vibe': {\n",
    "                'Í∞êÏÑ±': ['Í∞êÏÑ±', 'Í∞êÏÑ±Ï†Å', 'Î°úÎß®Ìã±', 'ÎÇ≠Îßå', 'ÏòàÏÅú'],\n",
    "                'ÌôúÎ†•': ['ÌôúÎ†•', 'ÌôúÍ∏∞', 'Ïã†ÎÇòÎäî', 'Ï¶êÍ±∞Ïö¥', 'Ïû¨ÎØ∏'],\n",
    "                'Ìú¥Ïãù': ['Ìú¥Ïãù', 'Ïâ¨Îäî', 'Ìé∏Ïïà', 'Ï°∞Ïö©', 'ÌèâÏò®', 'ÌûêÎßÅ'],\n",
    "                'ÏÇ∞Ï±Ö': ['ÏÇ∞Ï±Ö', 'Í±∑Í∏∞', 'Í±∞ÎãêÍ∏∞', 'Ï≤úÏ≤úÌûà'],\n",
    "                'Î™®Ìóò': ['Î™®Ìóò', 'Ïä§Î¶¥', 'ÎèÑÏ†Ñ', 'ÏùµÏä§Ìä∏Î¶º']\n",
    "            },\n",
    "            'target': {\n",
    "                'Ïó∞Ïù∏': ['Ïó∞Ïù∏', 'Ïª§Ìîå', 'ÎÇ®Ïπú', 'Ïó¨Ïπú', 'Ïï†Ïù∏'],\n",
    "                'Í∞ÄÏ°±': ['Í∞ÄÏ°±', 'Î∂ÄÎ™®', 'ÏïÑÏù¥', 'ÏûêÎÖÄ', 'ÏïÑÍ∏∞'],\n",
    "                'ÏπúÍµ¨': ['ÏπúÍµ¨', 'ÏπúÍµ¨Îì§', 'ÎèôÎ£å', 'Í∞ôÏù¥'],\n",
    "                'ÌòºÏûê': ['ÌòºÏûê', 'ÎÇòÎßå', 'Îã®ÎèÖ', 'ÏÜîÎ°ú']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def parse_user_input(self, user_input: Dict) -> Dict:\n",
    "        \"\"\"ÏÇ¨Ïö©Ïûê ÏûÖÎ†•ÏùÑ ÌååÏã±ÌïòÏó¨ ÌëúÏ§ÄÌôîÎêú ÌòïÌÉúÎ°ú Î≥ÄÌôò\"\"\"\n",
    "        \n",
    "        parsed = {\n",
    "            'season': None,\n",
    "            'nature': [],\n",
    "            'vibe': [],\n",
    "            'target': []\n",
    "        }\n",
    "        \n",
    "        # ÏûêÏú† Î¨∏Ïû• ÏûÖÎ†• Ï≤òÎ¶¨\n",
    "        if 'free_text' in user_input:\n",
    "            text = user_input['free_text'].lower()\n",
    "            \n",
    "            # Í∞Å ÌÉúÍ∑∏ Ïπ¥ÌÖåÍ≥†Î¶¨Î≥ÑÎ°ú Îß§Ïπ≠\n",
    "            for category, tag_dict in self.tag_mapping.items():\n",
    "                for tag, keywords in tag_dict.items():\n",
    "                    if any(keyword in text for keyword in keywords):\n",
    "                        if category == 'season':\n",
    "                            parsed['season'] = tag\n",
    "                        else:\n",
    "                            if tag not in parsed[category]:\n",
    "                                parsed[category].append(tag)\n",
    "        \n",
    "        # ÏßÅÏ†ë ÌÉúÍ∑∏ ÏûÖÎ†• Ï≤òÎ¶¨\n",
    "        else:\n",
    "            if 'season' in user_input:\n",
    "                parsed['season'] = user_input['season']\n",
    "            \n",
    "            for category in ['nature', 'vibe', 'target']:\n",
    "                if category in user_input:\n",
    "                    if isinstance(user_input[category], list):\n",
    "                        parsed[category] = user_input[category]\n",
    "                    else:\n",
    "                        parsed[category] = [user_input[category]]\n",
    "        \n",
    "        return parsed\n",
    "\n",
    "    def _calculate_advanced_tag_scores(self, user_input: Dict, df_row) -> float:\n",
    "        \"\"\"\n",
    "        ‚ú® Í∞úÏÑ†Îêú ÌÉúÍ∑∏ Îß§Ïπ≠ Ïä§ÏΩîÏñ¥ (Jaccard + F1 Í≤∞Ìï©)\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Season Îß§Ïπ≠ (Í∞ÄÏ§ëÏπò 0.3)\n",
    "        if user_input.get('season') == df_row['season']:\n",
    "            score += 0.3\n",
    "        \n",
    "        # Nature Îß§Ïπ≠ (Í∞ÄÏ§ëÏπò 0.25) - Jaccard + F1\n",
    "        if 'nature' in user_input and user_input['nature']:\n",
    "            user_set = set(user_input['nature'])\n",
    "            place_set = set(df_row['nature_list'])\n",
    "            \n",
    "            if user_set and place_set:\n",
    "                intersection = len(user_set & place_set)\n",
    "                union = len(user_set | place_set)\n",
    "                jaccard = intersection / union if union > 0 else 0\n",
    "                \n",
    "                precision = intersection / len(place_set) if place_set else 0\n",
    "                recall = intersection / len(user_set) if user_set else 0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                \n",
    "                score += 0.25 * (0.6 * jaccard + 0.4 * f1)\n",
    "        \n",
    "        # Vibe Îß§Ïπ≠ (Í∞ÄÏ§ëÏπò 0.25) - Jaccard\n",
    "        if 'vibe' in user_input and user_input['vibe']:\n",
    "            user_set = set(user_input['vibe'])\n",
    "            place_set = set(df_row['vibe_list'])\n",
    "            \n",
    "            if user_set and place_set:\n",
    "                intersection = len(user_set & place_set)\n",
    "                union = len(user_set | place_set)\n",
    "                jaccard = intersection / union if union > 0 else 0\n",
    "                score += 0.25 * jaccard\n",
    "        \n",
    "        # Target Îß§Ïπ≠ (Í∞ÄÏ§ëÏπò 0.2)\n",
    "        if 'target' in user_input and user_input['target']:\n",
    "            user_set = set(user_input['target'])\n",
    "            place_set = set(df_row['target_list'])\n",
    "            \n",
    "            if user_set and place_set:\n",
    "                intersection = len(user_set & place_set)\n",
    "                score += 0.2 * (intersection / len(user_set))\n",
    "        \n",
    "        return score\n",
    "        \n",
    "    def calculate_hybrid_score(self, user_input: Dict, \n",
    "                             similarity_weight: float = 0.6,\n",
    "                             tag_weight: float = 0.4) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"‚ú® Í∞úÏÑ†Îêú ÌïòÏù¥Î∏åÎ¶¨Îìú Ïä§ÏΩîÏñ¥ Í≥ÑÏÇ∞\"\"\"\n",
    "        \n",
    "        # ÏÇ¨Ïö©Ïûê ÏûÖÎ†• ÌååÏã±\n",
    "        parsed_input = self.parse_user_input(user_input)\n",
    "        \n",
    "        # 1. ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ Ï†êÏàò Í≥ÑÏÇ∞\n",
    "        if 'free_text' in user_input:\n",
    "            query_text = user_input['free_text']\n",
    "        else:\n",
    "            # ÌÉúÍ∑∏Î•º Î¨∏Ïû•ÏúºÎ°ú Î≥ÄÌôò\n",
    "            query_parts = []\n",
    "            if parsed_input['season']:\n",
    "                query_parts.append(f\"{parsed_input['season']}Ïóê\")\n",
    "            if parsed_input['target']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['target'])}ÏôÄ\")\n",
    "            if parsed_input['nature']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['nature'])}ÏóêÏÑú\")\n",
    "            if parsed_input['vibe']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['vibe'])} Ïó¨Ìñâ\")\n",
    "            \n",
    "            query_text = ' '.join(query_parts)\n",
    "        \n",
    "        # ÏøºÎ¶¨ ÏûÑÎ≤†Îî© ÏÉùÏÑ±\n",
    "        if self.embedding_generator.model is None:\n",
    "            self.embedding_generator.load_model()\n",
    "        \n",
    "        query_embedding = self.embedding_generator.model.encode([query_text])\n",
    "        \n",
    "        # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ - [0] Ïù∏Îç±Ïä§Î°ú 1Ï∞®Ïõê Î∞∞Ïó¥Î°ú Î≥ÄÌôò\n",
    "        similarity_scores = cosine_similarity(\n",
    "            query_embedding, \n",
    "            self.place_embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # 2. ‚ú® Í∞úÏÑ†Îêú ÌÉúÍ∑∏ Îß§Ïπ≠ Ï†êÏàò Í≥ÑÏÇ∞\n",
    "        tag_scores = np.zeros(len(self.df))\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            tag_scores[idx] = self._calculate_advanced_tag_scores(parsed_input, row)\n",
    "        \n",
    "        # Ï†ïÍ∑úÌôî\n",
    "        if tag_scores.max() > 0:\n",
    "            tag_scores = tag_scores / tag_scores.max()\n",
    "        \n",
    "        # 3. ÌïòÏù¥Î∏åÎ¶¨Îìú Ï†êÏàò Í≥ÑÏÇ∞\n",
    "        hybrid_scores = (\n",
    "            similarity_weight * similarity_scores +\n",
    "            tag_weight * tag_scores\n",
    "        )\n",
    "        \n",
    "        return hybrid_scores, similarity_scores, tag_scores\n",
    "    \n",
    "    def recommend_places(self, user_input: Dict, top_k: int = 10) -> Dict:\n",
    "        \"\"\"Í¥ÄÍ¥ëÏßÄ Ï∂îÏ≤ú Î©îÏù∏ Ìï®Ïàò\"\"\"\n",
    "        \n",
    "        # ÌïòÏù¥Î∏åÎ¶¨Îìú Ï†êÏàò Í≥ÑÏÇ∞\n",
    "        hybrid_scores, similarity_scores, tag_scores = self.calculate_hybrid_score(user_input)\n",
    "        \n",
    "        # ÏÉÅÏúÑ kÍ∞ú Ï∂îÏ≤úÏßÄ ÏÑ†ÌÉù\n",
    "        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "        \n",
    "        # Ï∂îÏ≤ú Í≤∞Í≥º Íµ¨ÏÑ±\n",
    "        recommendations = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            place_info = {\n",
    "                'name': self.df.iloc[idx]['name'],\n",
    "                'season': self.df.iloc[idx]['season'],\n",
    "                'nature': self.df.iloc[idx]['nature_list'],\n",
    "                'vibe': self.df.iloc[idx]['vibe_list'],\n",
    "                'target': self.df.iloc[idx]['target_list'],\n",
    "                'description': self.df.iloc[idx]['short_description'],\n",
    "                'hybrid_score': float(hybrid_scores[idx]),\n",
    "                'similarity_score': float(similarity_scores[idx]),\n",
    "                'tag_score': float(tag_scores[idx])\n",
    "            }\n",
    "            recommendations.append(place_info)\n",
    "        \n",
    "        # ÌååÏã±Îêú ÏÇ¨Ïö©Ïûê ÏûÖÎ†• Ï†ïÎ≥¥ Ï∂îÍ∞Ä\n",
    "        parsed_input = self.parse_user_input(user_input)\n",
    "        \n",
    "        result = {\n",
    "            'user_input': user_input,\n",
    "            'parsed_input': parsed_input,\n",
    "            'recommendations': recommendations,\n",
    "            'total_places': len(self.df)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"‚úÖ ÏàòÏ†ïÎêú Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÌÅ¥ÎûòÏä§ Ï†ïÏùò ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef561287-776f-4f82-9f5e-f2adba3a736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ïã§Ï†ú csv ÌååÏùº Î°úÎìú Î∞è Í≤ÄÏ¶ù\n",
    "\n",
    "def load_and_validate_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Ïã§Ï†ú CSV ÌååÏùº Î°úÎìú Î∞è Í≤ÄÏ¶ù\"\"\"\n",
    "\n",
    "    # ÏóÖÎ°úÎìúÎêú CSV ÌååÏùº ÏùΩÍ∏∞\n",
    "    try: \n",
    "        #  ÌååÏùºÏù¥ ÏóÖÎ°úÎìúÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÍ≥† data/raw Ìè¥ÎçîÎ°ú Î≥µÏÇ¨\n",
    "        if os.path.exists('gangwon_places_1000.xlsx'):\n",
    "            # ÌòÑÏû¨ ÎîîÎ†âÌÜ†Î¶¨Ïóê ÏûàÎäî ÌååÏùºÏùÑ data/raw Ìè¥ÎçîÎ°ú Î≥µÏÇ¨\n",
    "            os.makedirs('data/raw', exist_ok=True)\n",
    "            import shutil\n",
    "            shutil.copy('gangwon_places_1000.xlsx', 'data/raw/gangwon_places_1000.xlsx')\n",
    "            print(\"CSV ÌååÏùºÏùÑ data/raw Ìè¥ÎçîÎ°ú Î≥µÏÇ¨ ÏôÑÎ£å\")\n",
    "\n",
    "        # CSV ÌååÏù¥ Î°úÎìú\n",
    "        df = pd.read_csv('data/raw/gangwon_places_1000.xlsx', encoding='utf-8')\n",
    "        print(f\"‚úÖ CSV ÌååÏùº Î°úÎìú ÏôÑÎ£å: {df.shape}\")\n",
    "\n",
    "        # Ïª¨Îüº Ï†ïÎ≥¥ Ï∂úÎ†•\n",
    "        print(f\"Ïª¨Îüº Ï†ïÎ≥¥: {df.columns.tolist()}\")\n",
    "\n",
    "        # ÌïÑÏàò Ïª¨Îüº Í≤ÄÏ¶ù\n",
    "        required_columns = ['name', 'season', 'nature', 'vibe', 'target', 'short_description']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"‚ö†Ô∏è  ÌïÑÏàò Ïª¨Îüº ÎàÑÎùΩ: {missing_columns}\")\n",
    "            print(\"Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Î•º ÌôïÏù∏ÌïòÍ≥† ÏàòÏ†ïÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.\")\n",
    "        else:\n",
    "            print(\"‚úÖ Î™®Îì† ÌïÑÏàò Ïª¨ÎüºÏù¥ Ï°¥Ïû¨Ìï©ÎãàÎã§.\")\n",
    "\n",
    "        # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î∞è Í≤∞Ï∏°Ïπò Ï†ïÎ≥¥ Ï∂úÎ†•\n",
    "        print(f\"\\nÎç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥: \")\n",
    "        print(f\"- Ï¥ù Ìñâ Ïàò: {len(df)}\")\n",
    "        print(f\"- Í≤∞Ï∏°Ïπò ÌòÑÌô©: \")\n",
    "        for col in required_columns:\n",
    "            if col in df.columns: \n",
    "                missing_count = df[col].isnull().sum()\n",
    "                missing_pct = (missing_count / len(df)) * 100\n",
    "                print(f\" {col}: {missing_couint}Í∞ú {missing_pct:.1f}%)\")\n",
    "\n",
    "\n",
    "        # ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏\n",
    "        print(f\"\\n ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ (ÏÉÅÏúÑ 3Í∞ú): \")\n",
    "        for idx, row in df.head(3).iterrows():\n",
    "            print(f\"\\n{idx+1}. {row.get('name', 'N/A')}\")\n",
    "            print(f\"Í≥ÑÏ†à: {row.get('season', 'N/A')}\")\n",
    "            print(f\"ÏûêÏó∞ ÌôòÍ≤Ω:  {row.get('nature', 'N/A')}\")\n",
    "            print(f\"Î∂ÑÏúÑÍ∏∞:  {row.get('vibe', 'N/A')}\")\n",
    "            print(f\"ÎåÄÏÉÅ:  {row.get('vibe', 'N/A')}\")\n",
    "            print(f\"ÏÑ§Î™Ö:  {row.get('short_description', 'N/A')[:50]}...\")\n",
    "\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"gangwon_places_1000.xlsx ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "        print(\"ÌååÏùºÏùÑ ÌòÑÏû¨ ÎîîÎ†âÌÜ†Î¶¨Ïóê ÏóÖÎ°úÎìúÌïòÍ±∞ÎÇò data/raw/ Ìè¥ÎçîÏóê Ï†ÄÏû•Ìï¥Ï£ºÏÑ∏Ïöî.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ÌååÏùº Î°úÎìú Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}\")\n",
    "        return None\n",
    "    # Ïã§Ï†ú CSV ÌååÏùº Î°úÎìú\n",
    "    print(\"== Ïã§Ï†ú CSV ÌååÏùº Î°úÎìú ===\")\n",
    "    df_loaded = load_and_validate_csv('data/raw/gangwon_places_1000.xlsx')\n",
    "\n",
    "    if df_loaded is not None:\n",
    "        print(\"\\n Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÌååÏùº Î°úÎìú ÏÑ±Í≥µ\")\n",
    "    else:\n",
    "        print(\"\\n Îç∞Ïù¥ÌÑ∞ ÌååÏùº Î°úÎìú Ïã§Ìå® - ÌîÑÎ°úÍ∑∏Îû®ÏùÑ Ï¢ÖÎ£åÌï©ÎãàÎã§.\")\n",
    "        #Ïã§Ï†ú Jupyter ÌôòÍ≤ΩÏóêÏÑúÎäî Îã§Ïùå ÏÖÄ Ïã§ÌñâÏùÑ Ï§ëÎã®ÌïòÍ±∞ÎÇò Ïò§Î•ò Ï≤òÎ¶¨Î•º Ï∂îÍ∞ÄÌï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a52475d3-0a3c-43c4-8849-7588f03fb598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨ ===\n",
      "üìä Excel ÌååÏùº Î°úÎìú Ï§ë: data/raw/gangwon_places_1000.xlsx\n",
      "‚úÖ Excel ÌååÏùº Î°úÎìú ÏÑ±Í≥µ!\n",
      "\n",
      "ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞: (1000, 13)\n",
      "Ïª¨Îüº: ['name', 'season', 'nature', 'vibe', 'target', 'fee', 'parking', 'address', 'open_time', 'latitude', 'longitude', 'full_address', 'short_description']\n",
      "\n",
      "üìã ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ (Ï≤´ 3Ìñâ):\n",
      "\n",
      "1. Í∞ïÎ¶â Î™®ÎûòÎÇ¥ ÌïúÍ≥ºÎßàÏùÑ(Í∞àÍ≥®ÌïúÍ≥º)\n",
      "   Í≥ÑÏ†à: ÏÇ¨Í≥ÑÏ†à\n",
      "   ÏûêÏó∞: ÏÇ∞, Ìò∏Ïàò\n",
      "   Î∂ÑÏúÑÍ∏∞: Ïï°Ìã∞ÎπÑÌã∞, Ïó≠ÏÇ¨\n",
      "   ÎåÄÏÉÅ: Í∞ÄÏ°±\n",
      "\n",
      "2. Íµ≠Î¶Ω ÏÇºÎ¥âÏûêÏó∞Ìú¥ÏñëÎ¶º\n",
      "   Í≥ÑÏ†à: Î¥Ñ\n",
      "   ÏûêÏó∞: ÏÇ∞, ÏûêÏó∞, Ìò∏Ïàò\n",
      "   Î∂ÑÏúÑÍ∏∞: ÏÇ∞Ï±Ö, Ïï°Ìã∞ÎπÑÌã∞, ÌûêÎßÅ\n",
      "   ÎåÄÏÉÅ: Í∞ÄÏ°±\n",
      "\n",
      "3. ÏÑ§ÏïÖÏÇ∞Íµ≠Î¶ΩÍ≥µÏõê(ÎÇ¥ÏÑ§ÏïÖ)\n",
      "   Í≥ÑÏ†à: Ïó¨Î¶Ñ\n",
      "   ÏûêÏó∞: ÏÇ∞, ÏûêÏó∞\n",
      "   Î∂ÑÏúÑÍ∏∞: ÏÇ∞Ï±Ö\n",
      "   ÎåÄÏÉÅ: nan\n",
      "\n",
      "üìä Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥:\n",
      "Ï¥ù Í¥ÄÍ¥ëÏßÄ Ïàò: 1000\n",
      "Ï†ÑÏ≤¥ Ïª¨Îüº Ïàò: 13\n",
      "\n",
      "- season Ïπ¥ÌÖåÍ≥†Î¶¨: 11Í∞ú Ï¢ÖÎ•ò\n",
      "  ÏòàÏãú: ['ÏÇ¨Í≥ÑÏ†à', 'Î¥Ñ', 'Ïó¨Î¶Ñ', 'Í∞ÄÏùÑ', 'Í≤®Ïö∏']\n",
      "\n",
      "- nature Ïπ¥ÌÖåÍ≥†Î¶¨: 29Í∞ú Ï¢ÖÎ•ò\n",
      "  ÏòàÏãú: ['ÏÇ∞, Ìò∏Ïàò', 'ÏÇ∞, ÏûêÏó∞, Ìò∏Ïàò', 'ÏÇ∞, ÏûêÏó∞', 'Î∞îÎã§, ÏÇ∞, ÏûêÏó∞', 'Î∞îÎã§, ÏÇ∞']\n",
      "\n",
      "- vibe Ïπ¥ÌÖåÍ≥†Î¶¨: 43Í∞ú Ï¢ÖÎ•ò\n",
      "  ÏòàÏãú: ['Ïï°Ìã∞ÎπÑÌã∞, Ïó≠ÏÇ¨', 'ÏÇ∞Ï±Ö, Ïï°Ìã∞ÎπÑÌã∞, ÌûêÎßÅ', 'ÏÇ∞Ï±Ö', 'ÏÇ¨ÏßÑÎ™ÖÏÜå, ÏÇ∞Ï±Ö, Ïó≠ÏÇ¨', 'ÏÇ∞Ï±Ö, Ïó≠ÏÇ¨']\n",
      "\n",
      "- target Ïπ¥ÌÖåÍ≥†Î¶¨: 7Í∞ú Ï¢ÖÎ•ò\n",
      "  ÏòàÏãú: ['Í∞ÄÏ°±', 'ÏπúÍµ¨', 'Ïó∞Ïù∏', 'Ïó∞Ïù∏, ÏπúÍµ¨', 'Í∞ÄÏ°±, ÏπúÍµ¨']\n",
      "\n",
      "============================================================\n",
      "üîß Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ ÏãúÏûë\n",
      "============================================================\n",
      "\n",
      "‚úÖ Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å: (1000, 16)\n",
      "\n",
      "üìã Ï†ÑÏ≤òÎ¶¨ Í≤∞Í≥º ÏÉòÌîå (ÏÉÅÏúÑ 3Í∞ú):\n",
      "\n",
      "1. Í∞ïÎ¶â Î™®ÎûòÎÇ¥ ÌïúÍ≥ºÎßàÏùÑ(Í∞àÍ≥®ÌïúÍ≥º)\n",
      "   Í≥ÑÏ†à: ÏÇ¨Í≥ÑÏ†à\n",
      "   ÏûêÏó∞ÌôòÍ≤Ω (Î¶¨Ïä§Ìä∏): ['ÏÇ∞', 'Ìò∏Ïàò']\n",
      "   Î∂ÑÏúÑÍ∏∞ (Î¶¨Ïä§Ìä∏): ['Ïï°Ìã∞ÎπÑÌã∞', 'Ïó≠ÏÇ¨']\n",
      "   ÎåÄÏÉÅ (Î¶¨Ïä§Ìä∏): ['Í∞ÄÏ°±']\n",
      "   ÏÑ§Î™Ö: Í∞ïÎ¶â Î™®ÎûòÎÇ¥ ÌïúÍ≥ºÎßàÏùÑÍ∞àÍ≥®ÌïúÍ≥ºÏùÄÎäî ÏÇ¨Í≥ÑÏ†àÏóê ÌäπÌûà ÏïÑÎ¶ÑÎã§Ïõå ÏÇ∞ Í≤ΩÍ¥ÄÏù¥ Îõ∞Ïñ¥ÎÇòÎ©∞ Ïï°Ìã∞ÎπÑÌã∞ Î∂ÑÏúÑÍ∏∞...\n",
      "\n",
      "2. Íµ≠Î¶Ω ÏÇºÎ¥âÏûêÏó∞Ìú¥ÏñëÎ¶º\n",
      "   Í≥ÑÏ†à: Î¥Ñ\n",
      "   ÏûêÏó∞ÌôòÍ≤Ω (Î¶¨Ïä§Ìä∏): ['ÏÇ∞', 'ÏûêÏó∞', 'Ìò∏Ïàò']\n",
      "   Î∂ÑÏúÑÍ∏∞ (Î¶¨Ïä§Ìä∏): ['ÏÇ∞Ï±Ö', 'Ïï°Ìã∞ÎπÑÌã∞', 'ÌûêÎßÅ']\n",
      "   ÎåÄÏÉÅ (Î¶¨Ïä§Ìä∏): ['Í∞ÄÏ°±']\n",
      "   ÏÑ§Î™Ö: Íµ≠Î¶Ω ÏÇºÎ¥âÏûêÏó∞Ìú¥ÏñëÎ¶ºÏùÄÎäî Î¥ÑÏóê ÌäπÌûà ÏïÑÎ¶ÑÎã§Ïõå ÏÇ∞ Í≤ΩÍ¥ÄÏù¥ Îõ∞Ïñ¥ÎÇòÎ©∞ ÏÇ∞Ï±Ö Î∂ÑÏúÑÍ∏∞Î°ú Í∞ÄÏ°±ÏóêÍ≤å Ï∂îÏ≤ú...\n",
      "\n",
      "3. ÏÑ§ÏïÖÏÇ∞Íµ≠Î¶ΩÍ≥µÏõê(ÎÇ¥ÏÑ§ÏïÖ)\n",
      "   Í≥ÑÏ†à: Ïó¨Î¶Ñ\n",
      "   ÏûêÏó∞ÌôòÍ≤Ω (Î¶¨Ïä§Ìä∏): ['ÏÇ∞', 'ÏûêÏó∞']\n",
      "   Î∂ÑÏúÑÍ∏∞ (Î¶¨Ïä§Ìä∏): ['ÏÇ∞Ï±Ö']\n",
      "   ÎåÄÏÉÅ (Î¶¨Ïä§Ìä∏): []\n",
      "   ÏÑ§Î™Ö: ÏÑ§ÏïÖÏÇ∞Íµ≠Î¶ΩÍ≥µÏõêÎÇ¥ÏÑ§ÏïÖÏùÄÎäî Ïó¨Î¶ÑÏóê ÌäπÌûà ÏïÑÎ¶ÑÎã§Ïõå ÏÇ∞ Í≤ΩÍ¥ÄÏù¥ Îõ∞Ïñ¥ÎÇòÎ©∞ ÏÇ∞Ï±Ö Î∂ÑÏúÑÍ∏∞Î°ú Î™®ÎëêÏóêÍ≤å Ï∂î...\n",
      "\n",
      "============================================================\n",
      "üéì Ïù∏ÏΩîÎçî ÌïôÏäµ ÏãúÏûë\n",
      "============================================================\n",
      "Ïù∏ÏΩîÎçî ÌïôÏäµ ÏôÑÎ£å\n",
      "   - Í≥ÑÏ†à Ïπ¥ÌÖåÍ≥†Î¶¨: ['Í∞ÄÏùÑ', 'Í≤®Ïö∏', 'Î¥Ñ', 'Î¥Ñ, Í∞ÄÏùÑ', 'Î¥Ñ, Í∞ÄÏùÑ, Í≤®Ïö∏', 'Î¥Ñ, Í∞ÄÏùÑ, ÏÇ¨Í≥ÑÏ†à', 'Î¥Ñ, Ïó¨Î¶Ñ, Í∞ÄÏùÑ', 'Î¥Ñ, Ïó¨Î¶Ñ, Í∞ÄÏùÑ, ÏÇ¨Í≥ÑÏ†à', 'ÏÇ¨Í≥ÑÏ†à', 'Ïó¨Î¶Ñ', 'Ïó¨Î¶Ñ, ÏÇ¨Í≥ÑÏ†à']\n",
      "   - ÏûêÏó∞ÌôòÍ≤Ω Ïπ¥ÌÖåÍ≥†Î¶¨: 6Í∞ú\n",
      "   - Î∂ÑÏúÑÍ∏∞ Ïπ¥ÌÖåÍ≥†Î¶¨: 8Í∞ú\n",
      "   - ÎåÄÏÉÅ Ïπ¥ÌÖåÍ≥†Î¶¨: 3Í∞ú\n",
      "\n",
      "üî¢ ÎùºÎ≤® Ïù∏ÏΩîÎî© Ï§ë...\n",
      "\n",
      "‚úÖ Ïù∏ÏΩîÎî© Í≤∞Í≥º:\n",
      "   - season: (1000, 11)\n",
      "   - nature: (1000, 6)\n",
      "   - vibe: (1000, 8)\n",
      "   - target: (1000, 3)\n",
      "\n",
      "üíæ Ï†ÑÏ≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ï§ë...\n",
      "‚úÖ Ï†ÄÏû• ÏôÑÎ£å: data/processed/gangwon_places_100_processed.csv\n",
      "\n",
      "============================================================\n",
      "üéâ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å!\n",
      "============================================================\n",
      "‚úÖ Ï¥ù 1000Í∞ú Í¥ÄÍ¥ëÏßÄ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ ÏôÑÎ£å\n",
      "‚úÖ Î°úÎìúÎêú ÌååÏùº: data/raw/gangwon_places_1000.xlsx\n",
      "‚úÖ Ïù∏ÏΩîÎçî ÌïôÏäµ ÏôÑÎ£å: 4Í∞ú Ïπ¥ÌÖåÍ≥†Î¶¨\n"
     ]
    }
   ],
   "source": [
    "# ## Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ - Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
    "\n",
    "# # Ï∂îÏ≤ú ÏãúÏä§ÌÖú Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±\n",
    "# recommender = GangwonPlaceRecommender()\n",
    "\n",
    "# # Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Î°úÎìú(ÏóÖÎ°úÎìúÎêú CSV ÌååÏùº ÏÇ¨Ïö©)\n",
    "# print(\"=== Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨===\")\n",
    "\n",
    "# # ÏóÖÎ°úÎìúÎêú ÌååÏùºÏùÑ data/rawÎ°ú Î≥µÏÇ¨ (ÌååÏùºÏù¥ ÌòÑÏû¨ ÎîîÎ†âÌÜ†Î¶¨Ïóê ÏûàÎäî Í≤ΩÏö∞)\n",
    "# if os.path.exists('gangwon_places_100.xlsx'):\n",
    "#     import shutil\n",
    "#     shutil.copy('gangwon_places_100.csv', 'data/raw/gangwon_places_100.xlsx')\n",
    "#     print(\"‚úÖ ÏóÖÎ°úÎìúÎêú CSV ÌååÏùºÏùÑ data/rawÎ°ú Î≥µÏÇ¨ ÏôÑÎ£å\")\n",
    "\n",
    "# # CSV ÌååÏùº Î°úÎìú\n",
    "# df = pd.read_csv('data/raw/gangwon_places_100.xlsx', encoding='utf-8')\n",
    "# print(f\"ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞: {df.shape}\")\n",
    "# print(f\"Ïª¨Îüº: {df.columns.tolist()}\")\n",
    "\n",
    "# # Ï∂îÍ∞Ä Ïª¨Îüº Ï†ïÎ≥¥ Ï∂úÎ†•\n",
    "# print(f\"\\n Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥:\")\n",
    "# print(f\"Ï¥ù Í¥ÄÍ¥ëÏßÄ Ïàò: {len(df)}\")\n",
    "# print(f\"Ï†ÑÏ≤¥ Ïª¨Îüº Ïàò: {len(df.columns)}\")\n",
    "\n",
    "# # Í∞Å Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Í≥†Ïú†Í∞í ÌôïÏù∏\n",
    "# categorical_columns = ['season', 'nature', 'vibe', 'target']\n",
    "# for col in categorical_columns:\n",
    "#     if col in df.columns:\n",
    "#         unique_values = df[col].dropna().unique()\n",
    "#         print(f\"-{col} Ïπ¥ÌÖåÍ≥†Î¶¨: {len(unique_values)}Í∞ú Ï¢ÖÎ•ò\")\n",
    "#         print(f\" ÏòàÏãú: {list(unique_values)[:5]}\")\n",
    "\n",
    "# # Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\n",
    "# processed_df = recommender.preprocessor.preprocess_data(df)\n",
    "# print(f\"\\n Ï†ÑÏ≤òÎ¶¨ Îêú Îç∞Ïù¥ÌÑ∞: {processed_df.shape}\")\n",
    "\n",
    "# # Ï†ÑÏ≤òÎ¶¨ Í≤∞Í≥º ÌôïÏù∏\n",
    "# print(f\"\\n Ï†ÑÏ≤òÎ¶¨ Í≤∞Í≥º ÏÉòÌîå (ÏÉÅÏúÑ 3Í∞ú):\")\n",
    "# for idx,row in processed_df.head(3).iterrows():\n",
    "#     print(f\"\\n{idx+1}. {row['name']}\")\n",
    "#     print(f\"   Í≥ÑÏ†à: {row['season']}\")\n",
    "#     print(f\"   ÏûêÏó∞ÌôòÍ≤Ω (Î¶¨Ïä§Ìä∏): {row['nature_list']}\")\n",
    "#     print(f\"   Î∂ÑÏúÑÍ∏∞ (Î¶¨Ïä§Ìä∏): {row['vibe_list']}\")\n",
    "#     print(f\"   ÎåÄÏÉÅ (Î¶¨Ïä§Ìä∏): {row['target_list']}\")\n",
    "#     print(f\"   ÏÑ§Î™Ö: {row['short_description'][:50]}...\")\n",
    "\n",
    "# # Ïù∏ÏΩîÎçî ÌïôÏäµ\n",
    "# recommender.preprocessor.fit_encoders(processed_df)\n",
    "\n",
    "# # ÎùºÎ≤® Ïù∏ÏΩîÎî©\n",
    "# encoded_labels = recommender.preprocessor.encode_labels(processed_df)\n",
    "\n",
    "# # Ïù∏ÏΩîÎî© Í≤∞Í≥º ÌôïÏù∏\n",
    "# print(f\"\\n Ïù∏ÏΩîÎî© Í≤∞Í≥º:\") \n",
    "\n",
    "# # Ï†ÑÏ≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•\n",
    "# processed_df.to_csv('data/processed/gangwon_places_100_processed.xlsx', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# # Ï∂îÏ≤ú ÏãúÏä§ÌÖúÏóê Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•\n",
    "# recommender.df = processed_df\n",
    "# recommender.place_names = processed_df['name'].tolist()\n",
    "\n",
    "## Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ - Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ï∂îÏ≤ú ÏãúÏä§ÌÖú Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±\n",
    "recommender = GangwonPlaceRecommender()\n",
    "\n",
    "print(\"=== Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨ ===\")\n",
    "\n",
    "# ÎîîÎ†âÌÜ†Î¶¨ ÌôïÏù∏\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Excel ÌååÏùº Î°úÎìú (data/raw/gangwon_places_100.xlsx)\n",
    "file_path = 'data/raw/gangwon_places_1000.xlsx'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {file_path}\")\n",
    "\n",
    "print(f\"üìä Excel ÌååÏùº Î°úÎìú Ï§ë: {file_path}\")\n",
    "df = pd.read_excel(file_path)\n",
    "print(f\"‚úÖ Excel ÌååÏùº Î°úÎìú ÏÑ±Í≥µ!\")\n",
    "\n",
    "print(f\"\\nÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞: {df.shape}\")\n",
    "print(f\"Ïª¨Îüº: {df.columns.tolist()}\")\n",
    "\n",
    "# ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏\n",
    "print(f\"\\nüìã ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ (Ï≤´ 3Ìñâ):\")\n",
    "for idx, row in df.head(3).iterrows():\n",
    "    print(f\"\\n{idx+1}. {row['name']}\")\n",
    "    print(f\"   Í≥ÑÏ†à: {row['season']}\")\n",
    "    print(f\"   ÏûêÏó∞: {row['nature']}\")\n",
    "    print(f\"   Î∂ÑÏúÑÍ∏∞: {row['vibe']}\")\n",
    "    print(f\"   ÎåÄÏÉÅ: {row['target']}\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥\n",
    "print(f\"\\nüìä Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥:\")\n",
    "print(f\"Ï¥ù Í¥ÄÍ¥ëÏßÄ Ïàò: {len(df)}\")\n",
    "print(f\"Ï†ÑÏ≤¥ Ïª¨Îüº Ïàò: {len(df.columns)}\")\n",
    "\n",
    "# Í∞Å Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Í≥†Ïú†Í∞í ÌôïÏù∏\n",
    "categorical_columns = ['season', 'nature', 'vibe', 'target']\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        unique_values = df[col].dropna().unique()\n",
    "        print(f\"\\n- {col} Ïπ¥ÌÖåÍ≥†Î¶¨: {len(unique_values)}Í∞ú Ï¢ÖÎ•ò\")\n",
    "        print(f\"  ÏòàÏãú: {list(unique_values)[:5]}\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ ÏãúÏûë\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "processed_df = recommender.preprocessor.preprocess_data(df)\n",
    "print(f\"\\n‚úÖ Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å: {processed_df.shape}\")\n",
    "\n",
    "# Ï†ÑÏ≤òÎ¶¨ Í≤∞Í≥º ÌôïÏù∏\n",
    "print(f\"\\nüìã Ï†ÑÏ≤òÎ¶¨ Í≤∞Í≥º ÏÉòÌîå (ÏÉÅÏúÑ 3Í∞ú):\")\n",
    "for idx, row in processed_df.head(3).iterrows():\n",
    "    print(f\"\\n{idx+1}. {row['name']}\")\n",
    "    print(f\"   Í≥ÑÏ†à: {row['season']}\")\n",
    "    print(f\"   ÏûêÏó∞ÌôòÍ≤Ω (Î¶¨Ïä§Ìä∏): {row['nature_list']}\")\n",
    "    print(f\"   Î∂ÑÏúÑÍ∏∞ (Î¶¨Ïä§Ìä∏): {row['vibe_list']}\")\n",
    "    print(f\"   ÎåÄÏÉÅ (Î¶¨Ïä§Ìä∏): {row['target_list']}\")\n",
    "    print(f\"   ÏÑ§Î™Ö: {row['short_description'][:50]}...\")\n",
    "\n",
    "# Ïù∏ÏΩîÎçî ÌïôÏäµ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéì Ïù∏ÏΩîÎçî ÌïôÏäµ ÏãúÏûë\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommender.preprocessor.fit_encoders(processed_df)\n",
    "\n",
    "# ÎùºÎ≤® Ïù∏ÏΩîÎî©\n",
    "print(\"\\nüî¢ ÎùºÎ≤® Ïù∏ÏΩîÎî© Ï§ë...\")\n",
    "encoded_labels = recommender.preprocessor.encode_labels(processed_df)\n",
    "\n",
    "# Ïù∏ÏΩîÎî© Í≤∞Í≥º ÌôïÏù∏\n",
    "print(f\"\\n‚úÖ Ïù∏ÏΩîÎî© Í≤∞Í≥º:\")\n",
    "for key, value in encoded_labels.items():\n",
    "    print(f\"   - {key}: {value.shape}\")\n",
    "\n",
    "# Ï†ÑÏ≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•\n",
    "print(\"\\nüíæ Ï†ÑÏ≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ï§ë...\")\n",
    "processed_df.to_csv('data/processed/gangwon_places_100_processed.csv', \n",
    "                    index=False, \n",
    "                    encoding='utf-8-sig')\n",
    "print(\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: data/processed/gangwon_places_100_processed.csv\")\n",
    "\n",
    "# Ï∂îÏ≤ú ÏãúÏä§ÌÖúÏóê Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•\n",
    "recommender.df = processed_df\n",
    "recommender.place_names = processed_df['name'].tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Ï¥ù {len(recommender.df)}Í∞ú Í¥ÄÍ¥ëÏßÄ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ ÏôÑÎ£å\")\n",
    "print(f\"‚úÖ Î°úÎìúÎêú ÌååÏùº: {file_path}\")\n",
    "print(f\"‚úÖ Ïù∏ÏΩîÎçî ÌïôÏäµ ÏôÑÎ£å: {len(encoded_labels)}Í∞ú Ïπ¥ÌÖåÍ≥†Î¶¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92183aa0-ffe8-4d25-b760-6c19a5e77179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã ÌòÑÏû¨ Ïª¨Îüº Î™©Î°ù:\n",
      "['name', 'season', 'nature', 'vibe', 'target', 'fee', 'parking', 'address', 'open_time', 'latitude', 'longitude', 'full_address', 'short_description', 'nature_list', 'vibe_list', 'target_list']\n",
      "\n",
      "Ï¥ù 16Í∞ú Ïª¨Îüº\n"
     ]
    }
   ],
   "source": [
    "# ÌòÑÏû¨ DataFrameÏùò Ïª¨Îüº ÌôïÏù∏\n",
    "print(\"üìã ÌòÑÏû¨ Ïª¨Îüº Î™©Î°ù:\")\n",
    "print(processed_df.columns.tolist())\n",
    "print(f\"\\nÏ¥ù {len(processed_df.columns)}Í∞ú Ïª¨Îüº\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85166c33-9132-433e-975c-d5a6cecc4db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataPreprocessor Î©îÏÑúÎìú Î™©Î°ù:\n",
      "['encode_labels', 'fit_encoders', 'load_encoders', 'parse_multi_label_string', 'preprocess_data', 'save_encoders']\n",
      "‚úÖ _augment_single_row Î©îÏÑúÎìú Ï°¥Ïû¨\n"
     ]
    }
   ],
   "source": [
    "# DataPreprocessorÏóê augment Î©îÏÑúÎìúÍ∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏\n",
    "import inspect\n",
    "\n",
    "print(\"DataPreprocessor Î©îÏÑúÎìú Î™©Î°ù:\")\n",
    "methods = [m for m in dir(DataPreprocessor) if not m.startswith('_')]\n",
    "print(methods)\n",
    "\n",
    "# _augment_single_row Î©îÏÑúÎìú ÌôïÏù∏\n",
    "if hasattr(DataPreprocessor, '_augment_single_row'):\n",
    "    print(\"‚úÖ _augment_single_row Î©îÏÑúÎìú Ï°¥Ïû¨\")\n",
    "else:\n",
    "    print(\"‚ùå _augment_single_row Î©îÏÑúÎìú ÏóÜÏùå - ÌÅ¥ÎûòÏä§ Ïû¨Ï†ïÏùò ÌïÑÏöî!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c52ae69c-d465-4196-acac-111ee6df6c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ü§ñ SBERT ÏûÑÎ≤†Îî© ÏÉùÏÑ± (Í∞úÏÑ† Î≤ÑÏ†Ñ)\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'enhanced_description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'enhanced_description'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# ‚ú® Í∞úÏÑ†: enhanced_description ÏÇ¨Ïö©\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m texts \u001b[38;5;241m=\u001b[39m processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menhanced_description\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìù ÌÖçÏä§Ìä∏ Ïàò: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìù ÏÉòÌîå Í∏∏Ïù¥: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Í∏ÄÏûê\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'enhanced_description'"
     ]
    }
   ],
   "source": [
    "# ## SBERT ÏûÑÎ≤†Îî© ÏÉùÏÑ±(768Ï∞®Ïõê Ïú†ÏßÄ)\n",
    "# print(\"\\n SBERT ÏûÑÎ≤†Îî© ÏÉùÏÑ± Î∞è Ï∞®Ïõê Ï∂ïÏÜå\")\n",
    "\n",
    "# # ÌÖçÏä§Ìä∏ Î¶¨Ïä§Ìä∏ Ï§ÄÎπÑ\n",
    "# texts = processed_df['short_description'].tolist()\n",
    "\n",
    "# # SBERT ÏûÑÎ≤†Îî© ÏÉùÏÑ±\n",
    "# embeddings = recommender.embedding_generator.generate_embeddings(texts)\n",
    "\n",
    "# print(f\"üìä ÏûÑÎ≤†Îî© ÌòïÌÉú: {embeddings.shape}\")\n",
    "# print(f\"üíæ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# \"\"\"\n",
    "# # Ï∞®Ïõê Ï∂ïÏÜå Î™®Îç∏ ÌïôÏäµ\n",
    "# recommender.embedding_generator.fit_dimension_reducer(\n",
    "#     embeddings,\n",
    "#     method = recommender.config['model']['dimensionality_reduction'],\n",
    "#     target_dim = recommender.config['model']['reduced_dim']\n",
    "# )\n",
    "# # Ï∞®Ïõê Ï∂ïÏÜå Ï†ÅÏö©\n",
    "# reduced_embeddings = recommender.embedding_generator.reduce_dimensions(embeddings)\n",
    "# \"\"\"\n",
    "# # Ï∞®Ïõê Ï∂ïÏÜå ÏóÜÏù¥ ÏõêÎ≥∏ 768Ï∞®Ïõê ÏÇ¨Ïö©\n",
    "# os.makedirs('data/embeddings', exist_ok=True)\n",
    "# np.save('data/embeddings/place_embeddings_full768.npy', embeddings)\n",
    "\n",
    "# # # ÏûÑÎ≤†Îî© Ï†ÄÏû• \n",
    "# # np.save('data/embeddings/place_embeddings_pca128.npy', reduced_embeddings)\n",
    "\n",
    "# # Ï∂îÏ≤ú ÏãúÏä§ÌÖúÏóê ÏûÑÎ≤†Îî© Ï†ÄÏû•\n",
    "# recommender.place_embeddings = embeddings\n",
    "\n",
    "\n",
    "# print(\"‚úÖ 768Ï∞®Ïõê ÏûÑÎ≤†Îî© ÏÉùÏÑ± Î∞è Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "# print(f\"   ÌååÏùº Ï†ÄÏû•\")\n",
    "\n",
    "## SBERT ÏûÑÎ≤†Îî© ÏÉùÏÑ±(768Ï∞®Ïõê Ïú†ÏßÄ) - Í∞úÏÑ† Î≤ÑÏ†Ñ\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ü§ñ SBERT ÏûÑÎ≤†Îî© ÏÉùÏÑ± (Í∞úÏÑ† Î≤ÑÏ†Ñ)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ‚ú® Í∞úÏÑ†: enhanced_description ÏÇ¨Ïö©\n",
    "texts = processed_df['enhanced_description'].tolist()\n",
    "print(f\"üìù ÌÖçÏä§Ìä∏ Ïàò: {len(texts)}\")\n",
    "print(f\"üìù ÏÉòÌîå Í∏∏Ïù¥: {len(texts[0])} Í∏ÄÏûê\")\n",
    "\n",
    "# ‚ú® Í∞úÏÑ†: use_enhanced=True ÏòµÏÖòÏúºÎ°ú ÌíàÏßà Ìñ•ÏÉÅ\n",
    "embeddings = recommender.embedding_generator.generate_embeddings(\n",
    "    texts, \n",
    "    use_enhanced=True  # Ï†ïÍ∑úÌôî + ÏµúÏ†ÅÌôîÎêú Î∞∞Ïπò Ï≤òÎ¶¨\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä ÏûÑÎ≤†Îî© Ï†ïÎ≥¥:\")\n",
    "print(f\"   - ÌòïÌÉú: {embeddings.shape}\")\n",
    "print(f\"   - Ï∞®Ïõê: {embeddings.shape[1]}D (768Ï∞®Ïõê Ïú†ÏßÄ)\")\n",
    "print(f\"   - Î©îÎ™®Î¶¨: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"   - Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ: {embeddings.dtype}\")\n",
    "\n",
    "\"\"\"\n",
    "# Ï∞®Ïõê Ï∂ïÏÜåÎäî ÌòÑÏû¨ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùå (ÏÑ±Îä• Ï†ÄÌïò Î∞©ÏßÄ)\n",
    "# ÌïÑÏöîÏãú ÏïÑÎûò ÏΩîÎìú ÌôúÏÑ±Ìôî:\n",
    "recommender.embedding_generator.fit_dimension_reducer(\n",
    "    embeddings,\n",
    "    method = recommender.config['model']['dimensionality_reduction'],\n",
    "    target_dim = recommender.config['model']['reduced_dim']\n",
    ")\n",
    "reduced_embeddings = recommender.embedding_generator.reduce_dimensions(embeddings)\n",
    "\"\"\"\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Ï†ÄÏû•\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "\n",
    "# ‚ú® Í∞úÏÑ†: ÌååÏùºÎ™ÖÏóê 'enhanced' Ï∂îÍ∞ÄÌïòÏó¨ Íµ¨Î∂Ñ\n",
    "save_path = 'data/embeddings/place_embeddings_full768_enhanced.npy'\n",
    "np.save(save_path, embeddings)\n",
    "\n",
    "print(f\"\\nüíæ ÏûÑÎ≤†Îî© Ï†ÄÏû• ÏôÑÎ£å:\")\n",
    "print(f\"   - Í≤ΩÎ°ú: {save_path}\")\n",
    "print(f\"   - ÌÅ¨Í∏∞: {os.path.getsize(save_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Ï∂îÏ≤ú ÏãúÏä§ÌÖúÏóê ÏûÑÎ≤†Îî© Ï†ÄÏû•\n",
    "recommender.place_embeddings = embeddings\n",
    "recommender.place_names = processed_df['name'].tolist()\n",
    "\n",
    "print(f\"\\n‚úÖ 768Ï∞®Ïõê ÏûÑÎ≤†Îî© ÏÉùÏÑ± Î∞è Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "print(f\"   - ÏÇ¨Ïö© ÌÖçÏä§Ìä∏: enhanced_description (Ï¶ùÍ∞ïÎê®)\")\n",
    "print(f\"   - Ï†ïÍ∑úÌôî: Ï†ÅÏö©Îê®\")\n",
    "print(f\"   - Î∞∞Ïπò Ï≤òÎ¶¨: ÏµúÏ†ÅÌôîÎê®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72f8fd-1306-4750-9edc-c8373bcfb6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGBoost Î™®Îç∏ ÌïôÏäµ Î∞è ÌèâÍ∞Ä\n",
    "print(\"\\n === XGBoost Î™®Îç∏ ÌïôÏäµ===\")\n",
    "\n",
    "# ÌäπÏÑ±Í≥º ÎùºÎ≤® Ï§ÄÎπÑ\n",
    "features = embeddings\n",
    "labels = encoded_labels\n",
    "\n",
    "# Î™®Îç∏ ÌïôÏäµ\n",
    "recommender.xgb_trainer.train_models(features, labels)\n",
    "\n",
    "# Î™®Îç∏ ÌèâÍ∞Ä \n",
    "recommender.xgb_trainer.evaluate_models(features, labels)\n",
    "\n",
    "print(\"\\n=== XGBoost Î™®Îç∏ ÌïôÏäµ Î∞è ÌèâÍ∞Ä ÏôÑÎ£å===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e37bf-50e5-4faf-937b-0394617108ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Î™®Îç∏ Î∞è Ïù∏ÏΩîÎçî Ï†ÄÏû•\n",
    "print(\"\\n Î™®Îç∏ Î∞è Ïù∏ÏΩîÎçî Ï†ÄÏû•\")\n",
    "\n",
    "# Ìè¥Îçî ÏÉùÏÑ±\n",
    "os.makedirs('models/xgboost', exist_ok=True)\n",
    "os.makedirs('models/encoders', exist_ok=True)\n",
    "\n",
    "# Ïù∏ÏΩîÎçî Ï†ÄÏû•\n",
    "recommender.preprocessor.save_encoders('models/encoders')\n",
    "\n",
    "# XGBoost Î™®Îç∏ Ï†ÄÏû•\n",
    "recommender.xgb_trainer.save_models('models')\n",
    "\n",
    "print(\" Î™®Îì† Î™®Îç∏ Î∞è Ïù∏ÏΩîÎçî Ï†ÄÏû• ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0eba0b-7140-4112-af28-ae7eb2adb4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÌÖåÏä§Ìä∏\n",
    "print(\"\\n=== Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÌÖåÏä§Ìä∏ ===\")\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ 1: ÌÉúÍ∑∏ Í∏∞Î∞ò ÏûÖÎ†•\n",
    "test_input_1 = {\n",
    "    \"season\": \"Ïó¨Î¶Ñ\",\n",
    "    \"nature\": [\"Î∞îÎã§\", \"ÏûêÏó∞\"],\n",
    "    \"vibe\": [\"Ìú¥Ïãù\", \"Í∞êÏÑ±\"],\n",
    "    \"target\": [\"Ïó∞Ïù∏\"]\n",
    "}\n",
    "\n",
    "print(\"ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ 1: ÌÉúÍ∑∏ Í∏∞Î∞ò ÏûÖÎ†•\")\n",
    "print(f\"ÏûÖÎ†•: {test_input_1}\")\n",
    "\n",
    "result_1 = recommender.recommend_places(test_input_1, top_k=5)\n",
    "\n",
    "print(f\"\\n ÌååÏã±Îêú ÏûÖÎ†•: {result_1['parsed_input']}\")\n",
    "print(f\"Ï¥ù {result_1['total_places']}Í∞ú Í¥ÄÍ¥ëÏßÄ Ï§ë ÏÉÅÏúÑ 5Í∞ú Ï∂îÏ≤ú:\")\n",
    "\n",
    "for i, place in enumerate(result_1['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ÏÑ§Î™Ö: {place['description']}\")\n",
    "    print(f\"   ÌÉúÍ∑∏: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   Ï†êÏàò: ÌïòÏù¥Î∏åÎ¶¨Îìú={place['hybrid_score']:.4f}, Ïú†ÏÇ¨ÎèÑ={place['similarity_score']:.4f}, ÌÉúÍ∑∏={place['tag_score']:.4f}\")\n",
    "\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ 2: ÏûêÏú† Î¨∏Ïû• ÏûÖÎ†•\n",
    "test_input_2 = {\n",
    "    \"fress_text\": \"Í≤®Ïö∏Ïóê Í∞ÄÏ°±Í≥º Ìï®Íªò Ïä§ÌÇ§Î•º ÌÉÄÍ≥† Ïã∂Ïñ¥Ïöî\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" *50)\n",
    "print(\"ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ 2: ÏûêÏú† Î¨∏Ïû• ÏûÖÎ†•\")\n",
    "print(f\"ÏûÖÎ†• : {test_input_2}\")\n",
    "\n",
    "result_2 = recommender.recommend_places(test_input_2, top_k=5)\n",
    "\n",
    "for i, place in enumerate(result_2['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ÏÑ§Î™Ö: {place['description']}\")\n",
    "    print(f\"   ÌÉúÍ∑∏: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   Ï†êÏàò: ÌïòÏù¥Î∏åÎ¶¨Îìú={place['hybrid_score']:.4f}, Ïú†ÏÇ¨ÎèÑ={place['similarity_score']:.4f}, ÌÉúÍ∑∏={place['tag_score']:.4f}\")\n",
    "\n",
    "print(\"\\n Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÌÖåÏä§Ìä∏ ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911af44-7e05-479f-9303-c1566010e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Î™®Îç∏ Î°úÎìú Î∞è Ïû¨ÏÇ¨Ïö© ÌÖåÏä§Ìä∏\n",
    "print(f\"\\n === Î™®Îç∏ Î°úÎìú Î∞è Ïû¨ÏÇ¨Ïö© ÌÖåÏä§Ìä∏===\")\n",
    "# ÏÉàÎ°úÏö¥ Ï∂îÏ≤ú ÏãúÏä§ÌÖú Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± (ÏàòÏ†ïÎêú ÌÅ¥ÎûòÏä§ ÏÇ¨Ïö©)\n",
    "new_recommender = GangwonPlaceRecommender()\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "new_recommender.df = pd.read_csv('data/processed/gangwon_places_100_processed.csv')\n",
    "new_recommender.df = new_recommender.df.reset_index(drop=True)  # Ïù∏Îç±Ïä§ Î¶¨ÏÖã\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Î°úÎìú\n",
    "new_recommender.place_embeddings = np.load('data/embeddings/place_embeddings_full768.npy')\n",
    "\n",
    "# Ïù∏ÏΩîÎçî Î°úÎìú\n",
    "new_recommender.preprocessor.load_encoders('models/encoders')\n",
    "\n",
    "# XGBoost Î™®Îç∏ Î°úÎìú\n",
    "new_recommender.xgb_trainer.load_models('models')\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Ïã§Ìñâ\n",
    "test_input_3 = {\n",
    "    \"free_text\": \"Î¥ÑÏóê ÌòºÏûê Ï°∞Ïö©Ìïú ÏÇ∞ÏóêÏÑú ÌûêÎßÅÌïòÍ≥† Ïã∂Ïñ¥Ïöî\"\n",
    "}\n",
    "\n",
    "print(\"ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ 3: ÏàòÏ†ïÎêú Î™®Îç∏Î°ú Ï∂îÏ≤ú\")\n",
    "print(f\"ÏûÖÎ†•: {test_input_3}\")\n",
    "\n",
    "result_3 = new_recommender.recommend_places(test_input_3, top_k=3)\n",
    "\n",
    "print(f\"\\nÌååÏã±Îêú ÏûÖÎ†•: {result_3['parsed_input']}\")\n",
    "print(f\"ÏÉÅÏúÑ 3Í∞ú Ï∂îÏ≤ú:\")\n",
    "\n",
    "for i, place in enumerate(result_3['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ÏÑ§Î™Ö: {place['description']}\")\n",
    "    print(f\"   ÌÉúÍ∑∏: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   ÌïòÏù¥Î∏åÎ¶¨Îìú Ï†êÏàò: {place['hybrid_score']:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ ÏàòÏ†ïÎêú Î™®Îç∏ ÌÖåÏä§Ìä∏ ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46069c-0d8d-4e6f-89ed-786967358f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Í∞ÑÎã®Ìïú ÌÖåÏä§Ìä∏Ïö© Ï∂îÏ≤ú Ìï®Ïàò\n",
    "def simple_recommend_test(recommender, user_input, top_k=3):\n",
    "    \"\"\"Í∞ÑÎã®Ìïú ÌÖåÏä§Ìä∏Ïö© Ï∂îÏ≤ú Ìï®Ïàò\"\"\"\n",
    "    \n",
    "    # ÌååÏã±Îêú ÏûÖÎ†•\n",
    "    parsed_input = recommender.parse_user_input(user_input)\n",
    "    \n",
    "    # ÏøºÎ¶¨ ÌÖçÏä§Ìä∏ ÏÉùÏÑ±\n",
    "    if 'free_text' in user_input:\n",
    "        query_text = user_input['free_text']\n",
    "    else:\n",
    "        query_parts = []\n",
    "        if parsed_input['season']:\n",
    "            query_parts.append(f\"{parsed_input['season']}Ïóê\")\n",
    "        if parsed_input['nature']:\n",
    "            query_parts.append(f\"{', '.join(parsed_input['nature'])}ÏóêÏÑú\")\n",
    "        if parsed_input['vibe']:\n",
    "            query_parts.append(f\"{', '.join(parsed_input['vibe'])} Ïó¨Ìñâ\")\n",
    "        query_text = ' '.join(query_parts)\n",
    "    \n",
    "    # ÏøºÎ¶¨ ÏûÑÎ≤†Îî© ÏÉùÏÑ±\n",
    "    if recommender.embedding_generator.model is None:\n",
    "        recommender.embedding_generator.load_model()\n",
    "    \n",
    "    query_embedding = recommender.embedding_generator.model.encode([query_text])\n",
    "    \n",
    "    # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarity_scores = cosine_similarity(query_embedding, recommender.place_embeddings)[0]\n",
    "    \n",
    "    # ÏÉÅÏúÑ Ï∂îÏ≤úÏßÄ ÏÑ†ÌÉù\n",
    "    top_indices = np.argsort(similarity_scores)[::-1][:top_k]\n",
    "    \n",
    "    # Í≤∞Í≥º Íµ¨ÏÑ±\n",
    "    recommendations = []\n",
    "    for idx in top_indices:\n",
    "        place_info = {\n",
    "            'name': recommender.df.iloc[idx]['name'],\n",
    "            'description': recommender.df.iloc[idx]['short_description'],\n",
    "            'similarity_score': float(similarity_scores[idx])\n",
    "        }\n",
    "        recommendations.append(place_info)\n",
    "    \n",
    "    return {\n",
    "        'parsed_input': parsed_input,\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Ïã§Ìñâ\n",
    "test_input_3 = {\n",
    "    \"free_text\": \"Î¥ÑÏóê ÌòºÏûê Ï°∞Ïö©Ìïú ÏÇ∞ÏóêÏÑú ÌûêÎßÅÌïòÍ≥† Ïã∂Ïñ¥Ïöî\"\n",
    "}\n",
    "\n",
    "print(\"=== Í∞ÑÎã®Ìïú ÌÖåÏä§Ìä∏ ===\")\n",
    "print(f\"ÏûÖÎ†•: {test_input_3}\")\n",
    "\n",
    "result_3 = simple_recommend_test(new_recommender, test_input_3, top_k=3)\n",
    "\n",
    "print(f\"\\nÌååÏã±Îêú ÏûÖÎ†•: {result_3['parsed_input']}\")\n",
    "print(f\"ÏÉÅÏúÑ 3Í∞ú Ï∂îÏ≤ú:\")\n",
    "\n",
    "for i, place in enumerate(result_3['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ÏÑ§Î™Ö: {place['description']}\")\n",
    "    print(f\"   Ïú†ÏÇ¨ÎèÑ Ï†êÏàò: {place['similarity_score']:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Í∞ÑÎã®Ìïú ÌÖåÏä§Ìä∏ ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e897c-9917-4559-aa67-ec8c98367711",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flask API Ïó∞ÎèôÏùÑ ÏúÑÌïú JSON Î≥ÄÌôò Ìï®Ïàò\n",
    "\n",
    "def create_api_response(recommendation_result: Dict) -> Dict:\n",
    "    \"\"\"Flask API ÏùëÎãµÏùÑ ÏúÑÌïú JSON ÌòïÌÉúÎ°ú Î≥ÄÌôò\"\"\"\n",
    "\n",
    "    api_response = {\n",
    "        \n",
    "        'status': 'success',\n",
    "        'data' : {\n",
    "        'user_input': recommendation_result['user_input'],\n",
    "        'parsed_input': recommendation_result['parsed_input'],\n",
    "        'total_places': recommendation_result['total_places'],\n",
    "        'recommendations':[]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for place in recommendation_result['recommendations']:\n",
    "        place_data = {\n",
    "            'name': place['name'],\n",
    "            'description': place['description'],\n",
    "            'tags': {\n",
    "                'season': place['season'],\n",
    "                'nature': place['nature'],\n",
    "                'vibe': place['vibe'],\n",
    "                'target': place['target']\n",
    "            },\n",
    "            'scores' :{\n",
    "                'hybrid': round(place['hybrid_score'], 4),\n",
    "                'similarity': round(place['similarity_score'], 4),\n",
    "                'tag_match': round(place['tag_score'], 4)\n",
    "            }\n",
    "        }\n",
    "        api_response['data']['recommendations'].append(place_data)\n",
    "\n",
    "        return api_response\n",
    "\n",
    "print(\"Flask API Ïó∞Îèô Ìï®Ïàò Ï†ïÏùò ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581073d5-364b-4cd7-839b-02231836a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ÏÇ¨Ïö©Ïûê Ï†ïÏùò Ï∂îÏ≤ú Ìï®Ïàò(Flask APIÏö©)\n",
    "\n",
    "def recommend_places_api(user_input: Union[Dict, str], top_k: int = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Flask APIÏóêÏÑú ÏÇ¨Ïö©Ìï† Ï∂îÏ≤ú Ìï®Ïàò\n",
    "    \n",
    "    Args:\n",
    "        user_input: ÏÇ¨Ïö©Ïûê ÏûÖÎ†• (Dict ÎòêÎäî JSON Î¨∏ÏûêÏó¥)\n",
    "        top_k: Ï∂îÏ≤úÌï† Í¥ÄÍ¥ëÏßÄ Ïàò\n",
    "    \n",
    "    Returns:\n",
    "        API ÏùëÎãµ ÌòïÌÉúÏùò Dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Î¨∏ÏûêÏó¥Ïù∏ Í≤ΩÏö∞ JSON ÌååÏã±\n",
    "        if isinstance(user_input, str):\n",
    "            user_input = json.loads(user_input)\n",
    "\n",
    "        # ÏûÖÎ†• Í≤ÄÏ¶ù\n",
    "        if not isinstance(user_input, dict):\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': 'ÏûòÎ™ªÎêú ÏûÖÎ†• Î∞©ÏãùÏûÖÎãàÎã§.',\n",
    "                'data': None\n",
    "            }\n",
    "        # Ï∂îÏ≤ú Ïã§Ìñâ\n",
    "        result = recommender.recommend_places(user_input, top_k= top_k)\n",
    "\n",
    "        # API ÏùëÎãµ ÏÉùÏÑ±\n",
    "        api_response = create_api_response(result)\n",
    "\n",
    "        return api_response\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'message': f'Ï∂îÏ≤ú Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}',\n",
    "            'data': None\n",
    "        }\n",
    "# API Ìï®Ïàò ÌÖåÏä§Ìä∏\n",
    "print(\"\\n=== API Ìï®Ïàò ÌÖåÏä§Ìä∏ ===\")\n",
    "\n",
    "# JSON Î¨∏ÏûêÏó¥ ÏûÖÎ†• ÌÖåÏä§Ìä∏\n",
    "json_input = '{\"free_text\": \"Ïó¨Î¶ÑÏóê Î∞îÎã§ÏóêÏÑú ÏÑúÌïëÌïòÍ≥† Ïã∂Ïñ¥Ïöî\"}'\n",
    "api_result = recommend_places_api(json_input, top_k=3)\n",
    "\n",
    "print(\"JSON Î¨∏ÏûêÏó¥ ÏûÖÎ†• ÌÖåÏä§Ìä∏:\")\n",
    "print(f\"Status: {api_result['status']}\")\n",
    "if api_result['status'] == 'success':\n",
    "    print(f\"Ï∂îÏ≤ú Í≤∞Í≥º: {len(api_result['data']['recommendations'])}Í∞ú\")\n",
    "    for i, place in enumerate(api_result['data']['recommendations']):\n",
    "        print(f\"  {i+1}. {place['name']} (Ï†êÏàò: {place['scores']['hybrid']})\")\n",
    "\n",
    "print(\"\\n API Ìï®Ïàò ÌÖåÏä§Ìä∏ ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17e0ce-fe1e-4adf-ab0c-0adfc3de8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ï∂îÍ∞Ä ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§\n",
    "\n",
    "print(\"\\n=== Ï∂îÍ∞Ä ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§===\")\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ 4: Î≥µÌï© ÌÉúÍ∑∏ ÏûÖÎ†•\n",
    "test_input_4 = {\n",
    "    \"season\": \"Í∞ÄÏùÑ\",\n",
    "    \"nature\": [\"ÏÇ∞\", \"ÏûêÏó∞\"],\n",
    "    \"vibe\": [\"Í∞êÏÑ±\", \"Ìú¥Ïãù\"],\n",
    "    \"target\": [\"ÌòºÏûê\"]\n",
    "}\n",
    "\n",
    "print(\"ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ 4: Î≥µÌï© ÌÉúÍ∑∏ ÏûÖÎ†•\")\n",
    "print(f\"ÏûÖÎ†•: {test_input_4}\")\n",
    "\n",
    "result_4 = recommender.recommend_places(test_input_4, top_k=3)\n",
    "\n",
    "print(f\"\\nÌååÏã±Îêú ÏûÖÎ†•: {result_4['parsed_input']}\")\n",
    "print(f\"ÏÉÅÏúÑ 3Í∞ú Ï∂îÏ≤ú:\")\n",
    "\n",
    "for i, place in enumerate(result_4['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ÏÑ§Î™Ö: {place['description'][:100]}...\")\n",
    "    print(f\"   Ï†êÏàò: {place['hybrid_score']:.4f}\")\n",
    "\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ 5: Îã§ÏñëÌïú ÏûêÏú† Î¨∏Ïû• ÏûÖÎ†•\n",
    "test_cases = [\n",
    "    \"ÏπúÍµ¨Îì§Í≥º Ìï®Íªò Ïã†ÎÇòÎäî Ïó¨Î¶Ñ Ìú¥Í∞ÄÎ•º Î≥¥ÎÇ¥Í≥† Ïã∂Ïñ¥Ïöî\",\n",
    "    \"Ïó∞Ïù∏Í≥º Î°úÎß®Ìã±Ìïú Í∞ÄÏùÑ Îç∞Ïù¥Ìä∏ Ïû•ÏÜåÎ•º Ï∞æÍ≥† ÏûàÏñ¥Ïöî\",\n",
    "    \"Í∞ÄÏ°±Í≥º Ìï®Íªò ÏïàÏ†ÑÌïòÍ≥† ÍµêÏú°Ï†ÅÏù∏ Í≥≥ÏùÑ Í∞ÄÍ≥† Ïã∂ÏäµÎãàÎã§\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ 5: Îã§ÏñëÌïú ÏûêÏú† Î¨∏Ïû• ÏûÖÎ†•\")\n",
    "\n",
    "for i, test_text in enumerate(test_cases):\n",
    "    print(f\"\\nüìù ÌÖåÏä§Ìä∏ {i+1}: {test_text}\")\n",
    "    \n",
    "    test_input = {\"free_text\": test_text}\n",
    "    result = recommender.recommend_places(test_input, top_k=2)\n",
    "    \n",
    "    print(f\"ÌååÏã±Îêú ÏûÖÎ†•: {result['parsed_input']}\")\n",
    "    print(f\"Ï∂îÏ≤ú Í≤∞Í≥º:\")\n",
    "    for j, place in enumerate(result['recommendations']):\n",
    "        print(f\"  {j+1}. {place['name']} (Ï†êÏàò: {place['hybrid_score']:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Ï∂îÍ∞Ä ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e763b5-818c-49f4-b574-20ae1d164340",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ÏÑ±Îä• Î∂ÑÏÑù Î∞è ÏãúÍ∞ÅÌôî\n",
    "print(\"\\n === ÏÑ±Îä• Î∂ÑÏÑù ===\")\n",
    "\n",
    "#Ï∂îÏ≤ú Ï†êÏàò Î∂ÑÌè¨ Î∂ÑÏÑù\n",
    "def analyze_recommendation_scores():\n",
    "    \"\"\"Ï∂îÏ≤ú Ï†êÏàò Î∂ÑÌè¨ Î∂ÑÏÑù\"\"\"\n",
    "\n",
    "    # ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏûÖÎ†•Îì§\n",
    "    sample_inputs = [\n",
    "        {\"season\": \"Ïó¨Î¶Ñ\", \"nature\": [\"Î∞îÎã§\"], \"vibe\": [\"Ìú¥Ïãù\"], \"target\": [\"Ïó∞Ïù∏\"]},\n",
    "        {\"season\": \"Í≤®Ïö∏\", \"nature\": [\"ÏÇ∞\"], \"vibe\": [\"Î™®Ìóò\"], \"target\": [\"ÏπúÍµ¨\"]},\n",
    "        {\"season\": \"Î¥Ñ\", \"nature\": [\"ÏûêÏó∞\"], \"vibe\": [\"Í∞êÏÑ±\"], \"target\": [\"ÌòºÏûê\"]},\n",
    "        {\"free_text\": \"Í∞ÄÏùÑÏóê Îã®Ìíç Î≥¥Îü¨ Í∞ÄÍ≥† Ïã∂Ïñ¥Ïöî\"},\n",
    "        {\"free_text\": \"Ïä§ÌÇ§Ïû•ÏóêÏÑú Ïä§Î¶¥ ÎÑòÏπòÎäî Í≤®Ïö∏ÏùÑ Î≥¥ÎÇ¥Í≥† Ïã∂ÏäµÎãàÎã§\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"üìä Ï∂îÏ≤ú Ï†êÏàò Î∂ÑÌè¨ Î∂ÑÏÑù:\")\n",
    "\n",
    "    for i, test_input in enumerate(sample_inputs):\n",
    "        result = recommender.recommend_places(test_input, top_k=5)\n",
    "\n",
    "        hybrid_scores = [place['hybrid_score'] for place in result ['recommendations']]\n",
    "        similarity_scores = [place['similarity_score'] for place in result['recommendations']]\n",
    "        tag_scores = [place['tag_score'] for place in result['recommendations']]\n",
    "        \n",
    "        print(f\"\\nÌÖåÏä§Ìä∏ {i+1}: {test_input}\")\n",
    "        print(f\"  ÌïòÏù¥Î∏åÎ¶¨Îìú Ï†êÏàò Î≤îÏúÑ: {min(hybrid_scores):.4f} ~ {max(hybrid_scores):.4f}\")\n",
    "        print(f\"  Ïú†ÏÇ¨ÎèÑ Ï†êÏàò ÌèâÍ∑†: {np.mean(similarity_scores):.4f}\")\n",
    "        print(f\"  ÌÉúÍ∑∏ Îß§Ïπ≠ Ï†êÏàò ÌèâÍ∑†: {np.mean(tag_scores):.4f}\")\n",
    "\n",
    "analyze_recommendation_scores()\n",
    "\n",
    "# ÏãúÏä§ÌÖú ÏÑ±Îä• Ï†ïÎ≥¥\n",
    "print(f\"\\nüîß ÏãúÏä§ÌÖú ÏÑ±Îä• Ï†ïÎ≥¥:\")\n",
    "print(f\"- Ï†ÑÏ≤¥ Í¥ÄÍ¥ëÏßÄ Ïàò: {len(recommender.df)}\")\n",
    "print(f\"- ÏûÑÎ≤†Îî© Ï∞®Ïõê: {recommender.place_embeddings.shape[1]}\")\n",
    "print(f\"- Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ: {recommender.place_embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"- ÌïôÏäµÎêú Î™®Îç∏ Ïàò: {len(recommender.xgb_trainer.models)}\")\n",
    "\n",
    "print(\"\\n‚úÖ ÏÑ±Îä• Î∂ÑÏÑù ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48edfebd-8839-4121-a71f-76235ffa8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ÏµúÏ¢Ö Ï†ïÎ¶¨ Î∞è ÏÇ¨Ïö©Î≤ï ÏïàÎÇ¥\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ Í∞ïÏõêÎèÑ Í¥ÄÍ¥ëÏßÄ Ï∂îÏ≤ú ÏãúÏä§ÌÖú Íµ¨Ï∂ï ÏôÑÎ£å!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìÅ ÏÉùÏÑ±Îêú ÌååÏùº Íµ¨Ï°∞:\")\n",
    "print(\"\"\"\n",
    "project_root/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/gangwon_places_100.xlsx                 # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed/gangwon_places_100_processed.csv # Ï†ÑÏ≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ embeddings/place_embeddings_full768.npy    # 768Ï∞®Ïõê ÏûÑÎ≤†Îî©\n",
    "‚îú‚îÄ‚îÄ models/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ xgboost/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ season_model.joblib                    # Í≥ÑÏ†à Î∂ÑÎ•ò Î™®Îç∏\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nature_model.joblib                    # ÏûêÏó∞ÌôòÍ≤Ω Î∂ÑÎ•ò Î™®Îç∏\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vibe_model.joblib                      # Î∂ÑÏúÑÍ∏∞ Î∂ÑÎ•ò Î™®Îç∏\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ target_model.joblib                    # ÎåÄÏÉÅ Î∂ÑÎ•ò Î™®Îç∏\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ encoders/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ season_encoder.joblib                  # Í≥ÑÏ†à Ïù∏ÏΩîÎçî\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ nature_encoder.joblib                  # ÏûêÏó∞ÌôòÍ≤Ω Ïù∏ÏΩîÎçî\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ vibe_encoder.joblib                    # Î∂ÑÏúÑÍ∏∞ Ïù∏ÏΩîÎçî\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ target_encoder.joblib                  # ÎåÄÏÉÅ Ïù∏ÏΩîÎçî\n",
    "‚îî‚îÄ‚îÄ config/config.yaml                             # ÏÑ§Ï†ï ÌååÏùº\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüöÄ ÏÇ¨Ïö©Î≤ï:\")\n",
    "print(\"\"\"\n",
    "1. ÌÉúÍ∑∏ Í∏∞Î∞ò Ï∂îÏ≤ú:\n",
    "   user_input = {\n",
    "       \"season\": \"Ïó¨Î¶Ñ\",\n",
    "       \"nature\": [\"Î∞îÎã§\", \"ÏûêÏó∞\"],\n",
    "       \"vibe\": [\"Í∞êÏÑ±\", \"Ìú¥Ïãù\"],\n",
    "       \"target\": [\"Ïó∞Ïù∏\"]\n",
    "   }\n",
    "   result = recommender.recommend_places(user_input, top_k=5)\n",
    "\n",
    "2. ÏûêÏú† Î¨∏Ïû• Í∏∞Î∞ò Ï∂îÏ≤ú:\n",
    "   user_input = {\n",
    "       \"free_text\": \"Í≤®Ïö∏Ïóê Í∞ÄÏ°±Í≥º Ìï®Íªò Ïä§ÌÇ§Î•º ÌÉÄÍ≥† Ïã∂Ïñ¥Ïöî\"\n",
    "   }\n",
    "   result = recommender.recommend_places(user_input, top_k=5)\n",
    "\n",
    "3. Flask API Ïó∞Îèô:\n",
    "   api_response = recommend_places_api(user_input, top_k=10)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Ï£ºÏöî Í∏∞Îä•:\")\n",
    "print(\"\"\"\n",
    "‚úÖ SBERT Í∏∞Î∞ò ÌïúÍµ≠Ïñ¥ ÏûÑÎ≤†Îî© ÏÉùÏÑ± (768Ï∞®Ïõê Ïú†ÏßÄ)\n",
    "‚úÖ XGBoost Îã§Ï§ë ÎùºÎ≤® Î∂ÑÎ•ò (season, nature, vibe, target)\n",
    "‚úÖ ÌïòÏù¥Î∏åÎ¶¨Îìú Ï†êÏàò Í≥ÑÏÇ∞ (Ïú†ÏÇ¨ÎèÑ 60% + ÌÉúÍ∑∏ 40%)\n",
    "‚úÖ ÏûêÏú† Î¨∏Ïû• ÏûÖÎ†• ÌååÏã± Î∞è ÌÉúÍ∑∏ Ï∂îÏ∂ú\n",
    "‚úÖ Î™®Îç∏ Î∞è Ïù∏ÏΩîÎçî Ï†ÄÏû•/Î°úÎìú\n",
    "‚úÖ Flask API Ïó∞Îèô Ï§ÄÎπÑ\n",
    "‚úÖ JSON ÏûÖÏ∂úÎ†• ÏßÄÏõê\n",
    "‚úÖ ÏÑ±Îä• Î∂ÑÏÑù ÎèÑÍµ¨\n",
    "‚úÖ Îã§ÏñëÌïú ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ ÏßÄÏõê\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìä ÏÑ±Îä• ÏßÄÌëú:\")\n",
    "print(f\"- Îç∞Ïù¥ÌÑ∞: {len(recommender.df)}Í∞ú Í¥ÄÍ¥ëÏßÄ\")\n",
    "print(f\"- ÏûÑÎ≤†Îî© Ï∞®Ïõê: {recommender.place_embeddings.shape[1]}Ï∞®Ïõê\")\n",
    "print(f\"- Î™®Îç∏ ÌÉÄÏûÖ: XGBoost (season: Îã®ÏùºÎùºÎ≤®, nature/vibe/target: Îã§Ï§ëÎùºÎ≤®)\")\n",
    "print(f\"- Ï∂îÏ≤ú Î∞©Ïãù: ÌïòÏù¥Î∏åÎ¶¨Îìú (Ïú†ÏÇ¨ÎèÑ 60% + ÌÉúÍ∑∏ 40%)\")\n",
    "print(f\"- ÏßÄÏõê ÏûÖÎ†•: ÌÉúÍ∑∏ Í∏∞Î∞ò + ÏûêÏú† Î¨∏Ïû• ÏûÖÎ†•\")\n",
    "\n",
    "print(\"\\nüîÑ Î™®Îç∏ Ïû¨ÏÇ¨Ïö©:\")\n",
    "print(\"\"\"\n",
    "# Ï†ÄÏû•Îêú Î™®Îç∏ Î°úÎìú\n",
    "new_recommender = GangwonPlaceRecommender()\n",
    "new_recommender.df = pd.read_csv('data/processed/gangwon_places_100_processed.xlsx')\n",
    "new_recommender.place_embeddings = np.load('data/embeddings/place_embeddings_full768.npy')\n",
    "new_recommender.preprocessor.load_encoders('models/encoders')\n",
    "new_recommender.xgb_trainer.load_models('models')\n",
    "\n",
    "# Ï∂îÏ≤ú Ïã§Ìñâ\n",
    "result = new_recommender.recommend_places(user_input, top_k=5)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° Ï∂îÍ∞Ä ÌôúÏö© Î∞©Ïïà:\")\n",
    "print(\"\"\"\n",
    "1. Ïõπ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò Ïó∞Îèô:\n",
    "   - Flask/Django Î∞±ÏóîÎìúÏóê recommend_places_api() Ìï®Ïàò ÌôúÏö©\n",
    "   - REST API ÏóîÎìúÌè¨Ïù∏Ìä∏ Íµ¨ÏÑ±\n",
    "   - Ïã§ÏãúÍ∞Ñ Ï∂îÏ≤ú ÏÑúÎπÑÏä§ Ï†úÍ≥µ\n",
    "\n",
    "2. Î™®Î∞îÏùº Ïï± Ïó∞Îèô:\n",
    "   - JSON ÌòïÌÉúÏùò API ÏùëÎãµ ÌôúÏö©\n",
    "   - ÏÇ¨Ïö©Ïûê ÏûÖÎ†• ÌååÏã± Í∏∞Îä• ÌôúÏö©\n",
    "   - Ïò§ÌîÑÎùºÏù∏ Î™®Îç∏ Î∞∞Ìè¨ Í∞ÄÎä•\n",
    "\n",
    "3. ÏÑ±Îä• ÏµúÏ†ÅÌôî:\n",
    "   - ÏûÑÎ≤†Îî© Ï∫êÏã±ÏúºÎ°ú ÏùëÎãµ ÏÜçÎèÑ Ìñ•ÏÉÅ\n",
    "   - Î∞∞Ïπò Ï∂îÏ≤ú Ï≤òÎ¶¨\n",
    "   - Î™®Îç∏ ÏïïÏ∂ï Î∞è Í≤ΩÎüâÌôî\n",
    "\n",
    "4. Í∏∞Îä• ÌôïÏû•:\n",
    "   - ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± ÌïôÏäµ\n",
    "   - ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ Ï∂îÍ∞Ä\n",
    "   - Í∞úÏù∏Ìôî Ï∂îÏ≤ú Íµ¨ÌòÑ\n",
    "   - Ïã§ÏãúÍ∞Ñ ÌïôÏäµ ÏãúÏä§ÌÖú\n",
    "   - ÏßÄÏó≠Î≥Ñ ÌïÑÌÑ∞ÎßÅ Í∏∞Îä•\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìù Ï£ºÏùòÏÇ¨Ìï≠:\")\n",
    "print(\"\"\"\n",
    "- Ï≤´ Ïã§Ìñâ Ïãú SBERT Î™®Îç∏ Îã§Ïö¥Î°úÎìúÎ°ú ÏãúÍ∞ÑÏù¥ ÏÜåÏöîÎê† Ïàò ÏûàÏäµÎãàÎã§\n",
    "- GPU ÏÇ¨Ïö© Ïãú Îçî Îπ†Î•∏ ÏûÑÎ≤†Îî© ÏÉùÏÑ±Ïù¥ Í∞ÄÎä•Ìï©ÎãàÎã§\n",
    "- Ïã§Ï†ú ÏÑúÎπÑÏä§ Î∞∞Ìè¨ Ïãú Î≥¥Ïïà Î∞è ÏóêÎü¨ Ï≤òÎ¶¨Î•º Í∞ïÌôîÌïòÏÑ∏Ïöî\n",
    "- Îç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏ Ïãú Î™®Îç∏ Ïû¨ÌïôÏäµÏù¥ ÌïÑÏöîÌï† Ïàò ÏûàÏäµÎãàÎã§\n",
    "- Ï∂îÏ≤ú ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ ÏúÑÌï¥ Ï†ïÍ∏∞Ï†ÅÏù∏ Î™®Îç∏ ÌäúÎãùÏùÑ Í∂åÏû•Ìï©ÎãàÎã§\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüåü Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÌäπÏßï:\")\n",
    "print(\"\"\"\n",
    "- ÌïúÍµ≠Ïñ¥ ÌäπÌôî SBERT Î™®Îç∏ ÏÇ¨Ïö© (snunlp/KR-SBERT-V40K-klueNLI-augSTS)\n",
    "- ÌïòÏù¥Î∏åÎ¶¨Îìú Ï∂îÏ≤ú (ÏùòÎØ∏Ï†Å Ïú†ÏÇ¨ÎèÑ + ÌÉúÍ∑∏ Îß§Ïπ≠)\n",
    "- ÏûêÏú† Î¨∏Ïû• ÏûÖÎ†• ÏßÄÏõêÏúºÎ°ú ÏÇ¨Ïö©Ïûê Ìé∏ÏùòÏÑ± Ìñ•ÏÉÅ\n",
    "- Îã§Ï§ë ÎùºÎ≤® Î∂ÑÎ•òÎ°ú Ï†ïÌôïÌïú ÌÉúÍ∑∏ ÏòàÏ∏°\n",
    "- Î™®Îç∏ Ï†ÄÏû•/Î°úÎìú Í∏∞Îä•ÏúºÎ°ú Ìö®Ïú®Ï†ÅÏù∏ Ïö¥ÏòÅ\n",
    "- Flask API Ïó∞ÎèôÏúºÎ°ú Ïõπ ÏÑúÎπÑÏä§ ÌôïÏû• Í∞ÄÎä•\n",
    "- ÏÑ±Îä• Î∂ÑÏÑù ÎèÑÍµ¨Î°ú ÏãúÏä§ÌÖú Î™®ÎãàÌÑ∞ÎßÅ Í∞ÄÎä•\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüéØ Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÏÑ±Îä•:\")\n",
    "print(\"\"\"\n",
    "- ÏûÑÎ≤†Îî© Í∏∞Î∞ò ÏùòÎØ∏Ï†Å Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ (60% Í∞ÄÏ§ëÏπò)\n",
    "- ÌÉúÍ∑∏ Îß§Ïπ≠ Í∏∞Î∞ò Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ (40% Í∞ÄÏ§ëÏπò)\n",
    "- Í≥ÑÏ†à, ÏûêÏó∞ÌôòÍ≤Ω, Î∂ÑÏúÑÍ∏∞, ÎåÄÏÉÅÎ≥Ñ ÏÑ∏Î∂ÑÌôîÎêú Ï∂îÏ≤ú\n",
    "- ÏûêÏú† Î¨∏Ïû• ÌååÏã±ÏúºÎ°ú ÏûêÏó∞Ïä§Îü¨Ïö¥ ÏÇ¨Ïö©Ïûê Í≤ΩÌóò\n",
    "- ÏÉÅÏúÑ KÍ∞ú Ï∂îÏ≤úÏúºÎ°ú Îã§ÏñëÌïú ÏÑ†ÌÉùÏßÄ Ï†úÍ≥µ\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Í∞ïÏõêÎèÑ Í¥ÄÍ¥ëÏßÄ Ï∂îÏ≤ú ÏãúÏä§ÌÖúÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Íµ¨Ï∂ïÎêòÏóàÏäµÎãàÎã§!\")\n",
    "print(\"   Ïù¥Ï†ú Îã§ÏñëÌïú ÏÇ¨Ïö©Ïûê ÏûÖÎ†•Ïóê ÎåÄÌï¥ Ï†ïÌôïÌïú Í¥ÄÍ¥ëÏßÄ Ï∂îÏ≤úÏù¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\")\n",
    "print(\"   Flask API Ïó∞ÎèôÏùÑ ÌÜµÌï¥ Ïõπ ÏÑúÎπÑÏä§Î°ú ÌôïÏû•Ìï† Ïàò ÏûàÏäµÎãàÎã§.\")\n",
    "print(\"   Î™®Îì† Ïò§Î•òÍ∞Ä ÏàòÏ†ïÎêòÏñ¥ ÏïàÏ†ïÏ†ÅÏúºÎ°ú ÏûëÎèôÌï©ÎãàÎã§.\")\n",
    "print(\"   ÌÉúÍ∑∏ Í∏∞Î∞ò Ï∂îÏ≤úÍ≥º ÏûêÏú† Î¨∏Ïû• ÏûÖÎ†•ÏùÑ Î™®Îëê ÏßÄÏõêÌï©ÎãàÎã§.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìö Ï∂îÍ∞Ä ÌïôÏäµ ÏûêÎ£å:\")\n",
    "print(\"\"\"\n",
    "- SBERT Î™®Îç∏ ÏÉÅÏÑ∏ Ï†ïÎ≥¥: https://huggingface.co/snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
    "- XGBoost Í≥µÏãù Î¨∏ÏÑú: https://xgboost.readthedocs.io/\n",
    "- Scikit-learn Îã§Ï§ë ÎùºÎ≤® Î∂ÑÎ•ò: https://scikit-learn.org/stable/modules/multiclass.html\n",
    "- Flask API Í∞úÎ∞ú Í∞ÄÏù¥Îìú: https://flask.palletsprojects.com/\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüîó Îã§Ïùå Îã®Í≥Ñ:\")\n",
    "print(\"\"\"\n",
    "1. Ïõπ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Í∞úÎ∞ú (HTML/CSS/JavaScript)\n",
    "2. Flask/Django Î∞±ÏóîÎìú API Íµ¨Ï∂ï\n",
    "3. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Îèô (PostgreSQL/MySQL)\n",
    "4. ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± ÏàòÏßë ÏãúÏä§ÌÖú\n",
    "5. Ï∂îÏ≤ú ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ ÎåÄÏãúÎ≥¥Îìú\n",
    "6. Î™®Î∞îÏùº Ïï± Ïó∞Îèô\n",
    "7. Ïã§ÏãúÍ∞Ñ Ï∂îÏ≤ú ÏãúÏä§ÌÖú Íµ¨Ï∂ï\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚ú® ÏôÑÎ£åÎêú Í∏∞Îä•Îì§:\")\n",
    "print(\"\"\"\n",
    "‚úÖ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Î∞è Ï†ïÏ†ú\n",
    "‚úÖ SBERT ÏûÑÎ≤†Îî© ÏÉùÏÑ± (768Ï∞®Ïõê)\n",
    "‚úÖ XGBoost Îã§Ï§ë ÎùºÎ≤® Î∂ÑÎ•ò Î™®Îç∏ ÌïôÏäµ\n",
    "‚úÖ ÌïòÏù¥Î∏åÎ¶¨Îìú Ï∂îÏ≤ú ÏïåÍ≥†Î¶¨Ï¶ò Íµ¨ÌòÑ\n",
    "‚úÖ ÏûêÏú† Î¨∏Ïû• ÏûÖÎ†• ÌååÏã± ÏãúÏä§ÌÖú\n",
    "‚úÖ ÌÉúÍ∑∏ Í∏∞Î∞ò Ï∂îÏ≤ú ÏãúÏä§ÌÖú\n",
    "‚úÖ Î™®Îç∏ Ï†ÄÏû•/Î°úÎìú Í∏∞Îä•\n",
    "‚úÖ Flask API Ïó∞Îèô Ï§ÄÎπÑ\n",
    "‚úÖ ÏÑ±Îä• Î∂ÑÏÑù ÎèÑÍµ¨\n",
    "‚úÖ Îã§ÏñëÌïú ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§\n",
    "‚úÖ ÏóêÎü¨ Ï≤òÎ¶¨ Î∞è ÎîîÎ≤ÑÍπÖ\n",
    "‚úÖ ÏôÑÏ†ÑÌïú Î¨∏ÏÑúÌôî\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüéä Ï∂ïÌïòÌï©ÎãàÎã§! Í∞ïÏõêÎèÑ Í¥ÄÍ¥ëÏßÄ Ï∂îÏ≤ú ÏãúÏä§ÌÖúÏù¥ ÏôÑÏÑ±ÎêòÏóàÏäµÎãàÎã§!\")\n",
    "print(\"Ïù¥Ï†ú Ïã§Ï†ú ÏÇ¨Ïö©ÏûêÎì§ÏóêÍ≤å Ï†ïÌôïÌïòÍ≥† Ïú†Ïö©Ìïú Í¥ÄÍ¥ëÏßÄ Ï∂îÏ≤úÏùÑ Ï†úÍ≥µÌï† Ïàò ÏûàÏäµÎãàÎã§.\")\n",
    "\n",
    "# ================================\n",
    "# Î≥¥ÎÑàÏä§: Í∞ÑÎã®Ìïú ÏÇ¨Ïö© ÏòàÏ†ú\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ Í∞ÑÎã®Ìïú ÏÇ¨Ïö© ÏòàÏ†ú\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ÏòàÏ†ú 1: Í∞ÑÎã®Ìïú Ï∂îÏ≤ú\n",
    "print(\"\\nüìù ÏòàÏ†ú 1: Í∞ÑÎã®Ìïú Ï∂îÏ≤ú\")\n",
    "simple_input = {\"free_text\": \"Î¥ÑÏóê ÏÇ∞ÏóêÏÑú ÌûêÎßÅ\"}\n",
    "simple_result = recommender.recommend_places(simple_input, top_k=3)\n",
    "print(f\"ÏûÖÎ†•: {simple_input['free_text']}\")\n",
    "print(\"Ï∂îÏ≤ú Í≤∞Í≥º:\")\n",
    "for i, place in enumerate(simple_result['recommendations']):\n",
    "    print(f\"  {i+1}. {place['name']} (Ï†êÏàò: {place['hybrid_score']:.3f})\")\n",
    "\n",
    "# ÏòàÏ†ú 2: ÌÉúÍ∑∏ Ï°∞Ìï© Ï∂îÏ≤ú\n",
    "print(\"\\nüìù ÏòàÏ†ú 2: ÌÉúÍ∑∏ Ï°∞Ìï© Ï∂îÏ≤ú\")\n",
    "tag_input = {\"season\": \"Ïó¨Î¶Ñ\", \"nature\": [\"Î∞îÎã§\"], \"target\": [\"Í∞ÄÏ°±\"]}\n",
    "tag_result = recommender.recommend_places(tag_input, top_k=3)\n",
    "print(f\"ÏûÖÎ†•: {tag_input}\")\n",
    "print(\"Ï∂îÏ≤ú Í≤∞Í≥º:\")\n",
    "for i, place in enumerate(tag_result['recommendations']):\n",
    "    print(f\"  {i+1}. {place['name']} (Ï†êÏàò: {place['hybrid_score']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ ÏãúÏä§ÌÖú Ï§ÄÎπÑ ÏôÑÎ£å! Ïù¥Ï†ú ÎßàÏùåÍªè ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî!\")\n",
    "print(\"=\"*60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea4517-59d3-4c5f-be74-c3a011ebe00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. ÌïÑÏàò ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏ Î∞è ÏÑ§Ï†ï\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏ ÏôÑÎ£å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0681300d-3041-4586-bcdb-18ed2b28cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Îç∞Ïù¥ÌÑ∞ Î°úÎìú (data/raw/gangwon_places_100.xlsx)\n",
    "print(\"=\"*60)\n",
    "print(\"üìÇ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Ï§ë...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Excel ÌååÏùº Î°úÎìú\n",
    "df = pd.read_excel('data/raw/gangwon_places_1000.xlsx')\n",
    "\n",
    "print(f\"‚úÖ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å: {df.shape}\")\n",
    "print(f\"Ïª¨Îüº: {df.columns.tolist()}\")\n",
    "print(f\"\\nÏÉòÌîå Îç∞Ïù¥ÌÑ∞ (Ï≤´ 3Í∞ú):\")\n",
    "print(df.head(3)[['name', 'season', 'nature', 'vibe']])\n",
    "\n",
    "# Í∏∞Î≥∏ Ï†ÑÏ≤òÎ¶¨\n",
    "def preprocess_tags(value):\n",
    "    \"\"\"ÌÉúÍ∑∏ Î¨∏ÏûêÏó¥ÏùÑ Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôò\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return []\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    return [item.strip() for item in str(value).split(',') if item.strip()]\n",
    "\n",
    "# ÌÉúÍ∑∏ Ïª¨Îüº Ï†ÑÏ≤òÎ¶¨\n",
    "for col in ['nature', 'vibe', 'target']:\n",
    "    df[col] = df[col].apply(preprocess_tags)\n",
    "\n",
    "# Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨\n",
    "df['short_description'] = df['short_description'].fillna('')\n",
    "df['season'] = df['season'].fillna('ÏÇ¨Í≥ÑÏ†à')\n",
    "\n",
    "print(f\"\\n‚úÖ Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å!\")\n",
    "print(f\"Nature ÏÉòÌîå: {df['nature'].iloc[0]}\")\n",
    "print(f\"Vibe ÏÉòÌîå: {df['vibe'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c25cdf-d9b5-4be1-bbfd-d77c7d11b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ïÏúºÎ°ú ÏÑ§Î™Ö ÌÖçÏä§Ìä∏ Í∞ïÌôî\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï Ï§ë...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class DataAugmenter:\n",
    "    \"\"\"ÌÖçÏä§Ìä∏ Ï¶ùÍ∞ï ÌÅ¥ÎûòÏä§\"\"\"\n",
    "    \n",
    "    def augment_description(self, row):\n",
    "        \"\"\"ÏÑ§Î™Ö ÌÖçÏä§Ìä∏Ïóê ÌÉúÍ∑∏ Ï†ïÎ≥¥ Ï∂îÍ∞Ä\"\"\"\n",
    "        original = str(row['short_description'])\n",
    "        \n",
    "        # Í≥ÑÏ†à Ï†ïÎ≥¥ Ï∂îÍ∞Ä\n",
    "        season_text = f\"Ïù¥Í≥≥ÏùÄ {row['season']}Ïóê ÌäπÌûà ÏïÑÎ¶ÑÎãµÏäµÎãàÎã§.\"\n",
    "        \n",
    "        # ÏûêÏó∞ÌôòÍ≤Ω Ï†ïÎ≥¥ Ï∂îÍ∞Ä\n",
    "        if row['nature']:\n",
    "            nature_text = f\"{', '.join(row['nature'])} Í≤ΩÍ¥ÄÏùÑ Ï¶êÍ∏∏ Ïàò ÏûàÏäµÎãàÎã§.\"\n",
    "        else:\n",
    "            nature_text = \"\"\n",
    "        \n",
    "        # Î∂ÑÏúÑÍ∏∞ Ï†ïÎ≥¥ Ï∂îÍ∞Ä\n",
    "        if row['vibe']:\n",
    "            vibe_text = f\"{', '.join(row['vibe'])} Î∂ÑÏúÑÍ∏∞Î°ú Ï¢ãÏäµÎãàÎã§.\"\n",
    "        else:\n",
    "            vibe_text = \"\"\n",
    "        \n",
    "        # ÎåÄÏÉÅ Ï†ïÎ≥¥ Ï∂îÍ∞Ä\n",
    "        if row['target']:\n",
    "            target_text = f\"{', '.join(row['target'])}ÏóêÍ≤å Ï∂îÏ≤úÌï©ÎãàÎã§.\"\n",
    "        else:\n",
    "            target_text = \"\"\n",
    "        \n",
    "        # Î™®Îì† Ï†ïÎ≥¥ Í≤∞Ìï©\n",
    "        augmented = f\"{original} {season_text} {nature_text} {vibe_text} {target_text}\"\n",
    "        \n",
    "        return augmented.strip()\n",
    "\n",
    "# Ï¶ùÍ∞ï Ï†ÅÏö©\n",
    "augmenter = DataAugmenter()\n",
    "df['enhanced_description'] = df.apply(augmenter.augment_description, axis=1)\n",
    "\n",
    "print(f\"‚úÖ Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï ÏôÑÎ£å!\")\n",
    "print(f\"\\nÏõêÎ≥∏ ÏÑ§Î™Ö ÏÉòÌîå:\")\n",
    "print(df['short_description'].iloc[0][:100] + \"...\")\n",
    "print(f\"\\nÏ¶ùÍ∞ïÎêú ÏÑ§Î™Ö ÏÉòÌîå:\")\n",
    "print(df['enhanced_description'].iloc[0][:150] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36426a3d-b80c-4b16-a159-5b6b906a21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ïó¨Îü¨ SBERT Î™®Îç∏ÏùÑ ÏïôÏÉÅÎ∏îÌïòÏó¨ ÏûÑÎ≤†Îî© ÏÉùÏÑ±\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ ÏïôÏÉÅÎ∏î ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ï§ë...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class EnsembleEmbedding:\n",
    "    \"\"\"ÏïôÏÉÅÎ∏î ÏûÑÎ≤†Îî© ÏÉùÏÑ±Í∏∞\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Ï£º Î™®Îç∏\n",
    "        print(\"üì• SBERT Î™®Îç∏ Î°úÎìú Ï§ë...\")\n",
    "        try:\n",
    "            self.primary_model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
    "            print(\"‚úÖ Ï£º Î™®Îç∏ Î°úÎìú ÏôÑÎ£å: jhgan/ko-sroberta-multitask\")\n",
    "        except:\n",
    "            self.primary_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "            print(\"‚úÖ ÎåÄÏ≤¥ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å: paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    \n",
    "    def generate_multi_field_embeddings(self, df):\n",
    "        \"\"\"Ïó¨Îü¨ ÌïÑÎìúÎ•º Í≤∞Ìï©Ìïú Í∞ÄÏ§ë ÏûÑÎ≤†Îî©\"\"\"\n",
    "        combined_texts = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # ÌïÑÎìúÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n",
    "            text_parts = []\n",
    "            \n",
    "            # 1. Ï¶ùÍ∞ïÎêú ÏÑ§Î™Ö (Í∞ÄÏ§ëÏπò 3)\n",
    "            text_parts.extend([row['enhanced_description']] * 3)\n",
    "            \n",
    "            # 2. Ïû•ÏÜåÎ™Ö (Í∞ÄÏ§ëÏπò 2)\n",
    "            text_parts.extend([row['name']] * 2)\n",
    "            \n",
    "            # 3. ÌÉúÍ∑∏Îì§ (Í∞ÄÏ§ëÏπò 1)\n",
    "            text_parts.append(row['season'])\n",
    "            text_parts.extend(row['nature'])\n",
    "            text_parts.extend(row['vibe'])\n",
    "            \n",
    "            combined = ' '.join(text_parts)\n",
    "            combined_texts.append(combined)\n",
    "        \n",
    "        # Î∞∞Ïπò ÏûÑÎ≤†Îî© ÏÉùÏÑ±\n",
    "        print(f\"üîÑ ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ï§ë... (Ï¥ù {len(combined_texts)}Í∞ú)\")\n",
    "        embeddings = self.primary_model.encode(\n",
    "            combined_texts,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=16\n",
    "        )\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# ÏûÑÎ≤†Îî© ÏÉùÏÑ±\n",
    "ensemble_embedder = EnsembleEmbedding()\n",
    "place_embeddings = ensemble_embedder.generate_multi_field_embeddings(df)\n",
    "\n",
    "print(f\"\\n‚úÖ ÏûÑÎ≤†Îî© ÏÉùÏÑ± ÏôÑÎ£å!\")\n",
    "print(f\"ÏûÑÎ≤†Îî© ÌòïÌÉú: {place_embeddings.shape}\")\n",
    "print(f\"ÏûÑÎ≤†Îî© Ï∞®Ïõê: {place_embeddings.shape[1]}\")\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Ï†ÄÏû•\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "np.save('data/embeddings/enhanced_embeddings.npy', place_embeddings)\n",
    "print(f\"üíæ ÏûÑÎ≤†Îî© Ï†ÄÏû• ÏôÑÎ£å: data/embeddings/enhanced_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff4a53f-cb34-42df-a241-c665f4fbb107",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ï∂îÍ∞Ä ÌîºÏ≤ò ÏÉùÏÑ±ÏúºÎ°ú ÏÑ±Îä• Ìñ•ÏÉÅ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üî® ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ Ï§ë...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ ÌÅ¥ÎûòÏä§\"\"\"\n",
    "    \n",
    "    def create_statistical_features(self, df):\n",
    "        \"\"\"ÌÜµÍ≥ÑÏ†Å ÌîºÏ≤ò\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # ÌÉúÍ∑∏ Í∞úÏàò\n",
    "            nature_count = len(row['nature'])\n",
    "            vibe_count = len(row['vibe'])\n",
    "            target_count = len(row['target'])\n",
    "            total_tags = nature_count + vibe_count + target_count\n",
    "            \n",
    "            # ÌÖçÏä§Ìä∏ Í∏∏Ïù¥\n",
    "            desc_length = len(str(row['short_description']))\n",
    "            enhanced_length = len(str(row['enhanced_description']))\n",
    "            \n",
    "            # Í≥†Ïú† Îã®Ïñ¥ Ïàò\n",
    "            words = str(row['enhanced_description']).split()\n",
    "            unique_words = len(set(words))\n",
    "            \n",
    "            features.append([\n",
    "                nature_count,\n",
    "                vibe_count,\n",
    "                target_count,\n",
    "                total_tags,\n",
    "                desc_length,\n",
    "                enhanced_length,\n",
    "                unique_words,\n",
    "                enhanced_length / desc_length if desc_length > 0 else 1\n",
    "            ])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def create_tag_combination_features(self, df):\n",
    "        \"\"\"ÌÉúÍ∑∏ Ï°∞Ìï© ÌîºÏ≤ò (One-Hot)\"\"\"\n",
    "        # Nature + Vibe Ï°∞Ìï©\n",
    "        combinations = []\n",
    "        for idx, row in df.iterrows():\n",
    "            combo = [f\"{n}_{v}\" for n in row['nature'] for v in row['vibe']]\n",
    "            combinations.append(combo if combo else ['ÏóÜÏùå'])\n",
    "        \n",
    "        mlb = MultiLabelBinarizer()\n",
    "        combo_features = mlb.fit_transform(combinations)\n",
    "        \n",
    "        return combo_features\n",
    "    \n",
    "    def combine_all_features(self, embeddings, statistical, combinations):\n",
    "        \"\"\"Î™®Îì† ÌîºÏ≤ò Í≤∞Ìï©\"\"\"\n",
    "        # PCAÎ°ú ÏûÑÎ≤†Îî© Ï∂ïÏÜå (Ï∂îÍ∞Ä Ï†ïÎ≥¥Î°ú)\n",
    "        pca = PCA(n_components=64)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "        \n",
    "        # Î™®Îì† ÌîºÏ≤ò Í≤∞Ìï©\n",
    "        final_features = np.concatenate([\n",
    "            embeddings,           # ÏõêÎ≥∏ ÏûÑÎ≤†Îî©\n",
    "            reduced_embeddings,   # Ï∂ïÏÜå ÏûÑÎ≤†Îî©\n",
    "            statistical,          # ÌÜµÍ≥Ñ ÌîºÏ≤ò\n",
    "            combinations          # Ï°∞Ìï© ÌîºÏ≤ò\n",
    "        ], axis=1)\n",
    "        \n",
    "        print(f\"‚úÖ ÌîºÏ≤ò Í≤∞Ìï© ÏôÑÎ£å!\")\n",
    "        print(f\"  - ÏõêÎ≥∏ ÏûÑÎ≤†Îî©: {embeddings.shape[1]}Ï∞®Ïõê\")\n",
    "        print(f\"  - Ï∂ïÏÜå ÏûÑÎ≤†Îî©: {reduced_embeddings.shape[1]}Ï∞®Ïõê\")\n",
    "        print(f\"  - ÌÜµÍ≥Ñ ÌîºÏ≤ò: {statistical.shape[1]}Ï∞®Ïõê\")\n",
    "        print(f\"  - Ï°∞Ìï© ÌîºÏ≤ò: {combinations.shape[1]}Ï∞®Ïõê\")\n",
    "        print(f\"  - ÏµúÏ¢Ö ÌîºÏ≤ò: {final_features.shape[1]}Ï∞®Ïõê\")\n",
    "        \n",
    "        return final_features, pca\n",
    "\n",
    "# ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ Ïã§Ìñâ\n",
    "engineer = FeatureEngineer()\n",
    "\n",
    "statistical_features = engineer.create_statistical_features(df)\n",
    "print(f\"‚úÖ ÌÜµÍ≥Ñ ÌîºÏ≤ò ÏÉùÏÑ±: {statistical_features.shape}\")\n",
    "\n",
    "combination_features = engineer.create_tag_combination_features(df)\n",
    "print(f\"‚úÖ Ï°∞Ìï© ÌîºÏ≤ò ÏÉùÏÑ±: {combination_features.shape}\")\n",
    "\n",
    "enhanced_features, pca_model = engineer.combine_all_features(\n",
    "    place_embeddings,\n",
    "    statistical_features,\n",
    "    combination_features\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ ÏµúÏ¢Ö ÌîºÏ≤ò ÏôÑÏÑ±: {enhanced_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fc791-dd84-4b53-bc7d-7d462ee2d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ÏµúÏ†ÅÌôîÎêú XGBoost Î™®Îç∏ ÌïôÏäµ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ XGBoost Î™®Îç∏ ÌïôÏäµ Ï§ë...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ÎùºÎ≤® Ï§ÄÎπÑ\n",
    "season_encoder = LabelEncoder()\n",
    "nature_encoder = MultiLabelBinarizer()\n",
    "vibe_encoder = MultiLabelBinarizer()\n",
    "target_encoder = MultiLabelBinarizer()\n",
    "\n",
    "y_season = season_encoder.fit_transform(df['season'])\n",
    "y_nature = nature_encoder.fit_transform(df['nature'])\n",
    "y_vibe = vibe_encoder.fit_transform(df['vibe'])\n",
    "y_target = target_encoder.fit_transform(df['target'])\n",
    "\n",
    "print(f\"‚úÖ ÎùºÎ≤® Ïù∏ÏΩîÎî© ÏôÑÎ£å\")\n",
    "print(f\"  - Season ÌÅ¥ÎûòÏä§: {len(season_encoder.classes_)}\")\n",
    "print(f\"  - Nature ÌÅ¥ÎûòÏä§: {len(nature_encoder.classes_)}\")\n",
    "print(f\"  - Vibe ÌÅ¥ÎûòÏä§: {len(vibe_encoder.classes_)}\")\n",
    "print(f\"  - Target ÌÅ¥ÎûòÏä§: {len(target_encoder.classes_)}\")\n",
    "\n",
    "# ÏµúÏ†ÅÌôîÎêú ÌååÎùºÎØ∏ÌÑ∞Î°ú Î™®Îç∏ ÌïôÏäµ\n",
    "optimal_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.1,\n",
    "    'random_state': 42,\n",
    "    'tree_method': 'hist',\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "# Season Î™®Îç∏\n",
    "print(\"\\nüîß Season Î™®Îç∏ ÌïôÏäµ Ï§ë...\")\n",
    "season_model = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    **optimal_params\n",
    ")\n",
    "season_model.fit(enhanced_features, y_season)\n",
    "print(f\"‚úÖ Season Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å!\")\n",
    "\n",
    "# Îã§Ï§ë Î†àÏù¥Î∏î Î™®Îç∏Îì§\n",
    "models = {}\n",
    "\n",
    "for label_name, y_label in [('nature', y_nature), ('vibe', y_vibe), ('target', y_target)]:\n",
    "    print(f\"\\nüîß {label_name.capitalize()} Î™®Îç∏ ÌïôÏäµ Ï§ë...\")\n",
    "    \n",
    "    if y_label.sum() > 0:  # Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎäî Í≤ΩÏö∞\n",
    "        base_model = XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            **optimal_params\n",
    "        )\n",
    "        model = MultiOutputClassifier(base_model, n_jobs=1)\n",
    "        model.fit(enhanced_features, y_label)\n",
    "        print(f\"‚úÖ {label_name.capitalize()} Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å!\")\n",
    "    else:\n",
    "        from sklearn.dummy import DummyClassifier\n",
    "        model = MultiOutputClassifier(DummyClassifier(strategy='constant', constant=0))\n",
    "        model.fit(enhanced_features, y_label)\n",
    "        print(f\"‚ö†Ô∏è {label_name.capitalize()} - ÎçîÎØ∏ Î™®Îç∏ ÏÇ¨Ïö©\")\n",
    "    \n",
    "    models[label_name] = model\n",
    "\n",
    "print(f\"\\nüéâ Î™®Îì† Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf75fb-e75c-4914-bff9-31360a3cc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Í∞úÏÑ†Îêú Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÌÅ¥ÎûòÏä§\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ Í∞úÏÑ†Îêú Ï∂îÏ≤ú ÏãúÏä§ÌÖú Íµ¨Ï∂ï Ï§ë...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class EnhancedRecommendationSystem:\n",
    "    \"\"\"Í∞úÏÑ†Îêú Ï∂îÏ≤ú ÏãúÏä§ÌÖú\"\"\"\n",
    "    \n",
    "    def __init__(self, df, embeddings, models, encoders, \n",
    "                 sbert_model, pca_model, engineer):\n",
    "        self.df = df\n",
    "        self.place_embeddings = embeddings\n",
    "        self.season_model = models.get('season')\n",
    "        self.nature_model = models.get('nature')\n",
    "        self.vibe_model = models.get('vibe')\n",
    "        self.target_model = models.get('target')\n",
    "        self.season_encoder = encoders['season']\n",
    "        self.nature_encoder = encoders['nature']\n",
    "        self.vibe_encoder = encoders['vibe']\n",
    "        self.target_encoder = encoders['target']\n",
    "        self.sbert_model = sbert_model\n",
    "        self.pca_model = pca_model\n",
    "        self.engineer = engineer\n",
    "        \n",
    "        # Í∞ÄÏ§ëÏπò\n",
    "        self.similarity_weight = 0.5\n",
    "        self.tag_weight = 0.3\n",
    "        self.predicted_weight = 0.2\n",
    "    \n",
    "    def encode_user_query(self, user_input: Dict) -> np.ndarray:\n",
    "        \"\"\"ÏÇ¨Ïö©Ïûê ÏûÖÎ†•ÏùÑ ÏûÑÎ≤†Îî©ÏúºÎ°ú Î≥ÄÌôò\"\"\"\n",
    "        # ÌÖçÏä§Ìä∏ ÏÉùÏÑ±\n",
    "        text_parts = []\n",
    "        \n",
    "        if 'season' in user_input and user_input['season']:\n",
    "            text_parts.extend([user_input['season']] * 3)\n",
    "        \n",
    "        for key in ['nature', 'vibe', 'target']:\n",
    "            if key in user_input:\n",
    "                values = user_input[key]\n",
    "                if isinstance(values, list):\n",
    "                    text_parts.extend(values * 2)\n",
    "                else:\n",
    "                    text_parts.extend([values] * 2)\n",
    "        \n",
    "        query_text = ' '.join(text_parts) if text_parts else \"Í¥ÄÍ¥ëÏßÄ\"\n",
    "        \n",
    "        # ÏûÑÎ≤†Îî© ÏÉùÏÑ±\n",
    "        query_embedding = self.sbert_model.encode(\n",
    "            [query_text],\n",
    "            normalize_embeddings=True\n",
    "        )[0]\n",
    "        \n",
    "        return query_embedding\n",
    "    \n",
    "    def calculate_advanced_scores(self, user_input: Dict, \n",
    "                                  user_embedding: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Í≥†Í∏â Ïä§ÏΩîÏñ¥ÎßÅ\"\"\"\n",
    "        \n",
    "        # 1. ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ\n",
    "        similarity_scores = cosine_similarity(\n",
    "            user_embedding.reshape(1, -1),\n",
    "            self.place_embeddings[:, :len(user_embedding)]  # ÏûÑÎ≤†Îî© Ï∞®Ïõê ÎßûÏ∂îÍ∏∞\n",
    "        )[0]\n",
    "        \n",
    "        # 2. ÌÉúÍ∑∏ Îß§Ïπ≠ Ïä§ÏΩîÏñ¥\n",
    "        tag_scores = self._calculate_tag_scores(user_input)\n",
    "        \n",
    "        # 3. ÏòàÏ∏° Í∏∞Î∞ò Ïä§ÏΩîÏñ¥ (XGBoost)\n",
    "        predicted_scores = self._calculate_predicted_scores(user_embedding)\n",
    "        \n",
    "        # 4. ÏµúÏ¢Ö Ïä§ÏΩîÏñ¥ (Í∞ÄÏ§ë ÌèâÍ∑†)\n",
    "        final_scores = (\n",
    "            self.similarity_weight * similarity_scores +\n",
    "            self.tag_weight * tag_scores +\n",
    "            self.predicted_weight * predicted_scores\n",
    "        )\n",
    "        \n",
    "        return final_scores, similarity_scores, tag_scores, predicted_scores\n",
    "    \n",
    "    def _calculate_tag_scores(self, user_input: Dict) -> np.ndarray:\n",
    "        \"\"\"Í∞úÏÑ†Îêú ÌÉúÍ∑∏ Îß§Ïπ≠ Ïä§ÏΩîÏñ¥\"\"\"\n",
    "        scores = np.zeros(len(self.df))\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            score = 0.0\n",
    "            \n",
    "            # Season Îß§Ïπ≠ (Í∞ÄÏ§ëÏπò 0.3)\n",
    "            if user_input.get('season') == row['season']:\n",
    "                score += 0.3\n",
    "            \n",
    "            # Nature Îß§Ïπ≠ (Í∞ÄÏ§ëÏπò 0.25) - Jaccard + F1\n",
    "            if 'nature' in user_input and user_input['nature']:\n",
    "                user_nature = set(user_input['nature'] if isinstance(user_input['nature'], list) \n",
    "                                else [user_input['nature']])\n",
    "                place_nature = set(row['nature'])\n",
    "                \n",
    "                if user_nature and place_nature:\n",
    "                    intersection = len(user_nature & place_nature)\n",
    "                    union = len(user_nature | place_nature)\n",
    "                    jaccard = intersection / union if union > 0 else 0\n",
    "                    \n",
    "                    precision = intersection / len(place_nature) if place_nature else 0\n",
    "                    recall = intersection / len(user_nature) if user_nature else 0\n",
    "                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                    \n",
    "                    score += 0.25 * (0.6 * jaccard + 0.4 * f1)\n",
    "            \n",
    "            # Vibe Îß§Ïπ≠ (Í∞ÄÏ§ëÏπò 0.25)\n",
    "            if 'vibe' in user_input and user_input['vibe']:\n",
    "                user_vibe = set(user_input['vibe'] if isinstance(user_input['vibe'], list) \n",
    "                              else [user_input['vibe']])\n",
    "                place_vibe = set(row['vibe'])\n",
    "                \n",
    "                if user_vibe and place_vibe:\n",
    "                    intersection = len(user_vibe & place_vibe)\n",
    "                    union = len(user_vibe | place_vibe)\n",
    "                    jaccard = intersection / union if union > 0 else 0\n",
    "                    score += 0.25 * jaccard\n",
    "            \n",
    "            # Target Îß§Ïπ≠ (Í∞ÄÏ§ëÏπò 0.2)\n",
    "            if 'target' in user_input and user_input['target']:\n",
    "                user_target = set(user_input['target'] if isinstance(user_input['target'], list) \n",
    "                                else [user_input['target']])\n",
    "                place_target = set(row['target'])\n",
    "                \n",
    "                if user_target and place_target:\n",
    "                    intersection = len(user_target & place_target)\n",
    "                    score += 0.2 * (intersection / len(user_target))\n",
    "            \n",
    "            scores[idx] = score\n",
    "        \n",
    "        # Ï†ïÍ∑úÌôî\n",
    "        if scores.max() > 0:\n",
    "            scores = scores / scores.max()\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def _calculate_predicted_scores(self, user_embedding: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"XGBoost ÏòàÏ∏° Í∏∞Î∞ò Ïä§ÏΩîÏñ¥\"\"\"\n",
    "        # Í∞ÑÎã®Ìûà Ïú†ÏÇ¨ÎèÑ Í∏∞Î∞òÏúºÎ°ú Í≥ÑÏÇ∞ (Ïã§Ï†úÎ°úÎäî Îçî Î≥µÏû°Ìïú Î°úÏßÅ Í∞ÄÎä•)\n",
    "        return np.ones(len(self.df)) * 0.5\n",
    "    \n",
    "    def recommend(self, user_input: Dict, top_n: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Ï∂îÏ≤ú Ïã§Ìñâ\"\"\"\n",
    "        print(f\"\\nüéØ Ï∂îÏ≤ú ÏÉùÏÑ± Ï§ë...\")\n",
    "        print(f\"ÏÇ¨Ïö©Ïûê ÏûÖÎ†•: {user_input}\")\n",
    "        \n",
    "        # ÏÇ¨Ïö©Ïûê ÏøºÎ¶¨ ÏûÑÎ≤†Îî©\n",
    "        user_embedding = self.encode_user_query(user_input)\n",
    "        \n",
    "        # Ïä§ÏΩîÏñ¥ Í≥ÑÏÇ∞\n",
    "        final_scores, sim_scores, tag_scores, pred_scores = \\\n",
    "            self.calculate_advanced_scores(user_input, user_embedding)\n",
    "        \n",
    "        # ÏÉÅÏúÑ NÍ∞ú ÏÑ†ÌÉù\n",
    "        top_indices = np.argsort(final_scores)[::-1][:top_n]\n",
    "        \n",
    "        # Í≤∞Í≥º DataFrame ÏÉùÏÑ±\n",
    "        recommendations = self.df.iloc[top_indices].copy()\n",
    "        recommendations['final_score'] = final_scores[top_indices]\n",
    "        recommendations['similarity_score'] = sim_scores[top_indices]\n",
    "        recommendations['tag_score'] = tag_scores[top_indices]\n",
    "        \n",
    "        print(f\"‚úÖ Ï∂îÏ≤ú ÏôÑÎ£å! ÏÉÅÏúÑ {top_n}Í∞ú ÏÑ†Ï†ï\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Ï∂îÏ≤ú ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî\n",
    "recommender = EnhancedRecommendationSystem(\n",
    "    df=df,\n",
    "    embeddings=enhanced_features,\n",
    "    models={'season': season_model, 'nature': models['nature'], \n",
    "            'vibe': models['vibe'], 'target': models['target']},\n",
    "    encoders={'season': season_encoder, 'nature': nature_encoder,\n",
    "              'vibe': vibe_encoder, 'target': target_encoder},\n",
    "    sbert_model=ensemble_embedder.primary_model,\n",
    "    pca_model=pca_model,\n",
    "    engineer=engineer\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Í∞úÏÑ†Îêú Ï∂îÏ≤ú ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209309e8-c4ae-4a91-9a98-f9143aacae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Í∞úÏÑ†Îêú Î™®Îç∏ Ï†ÄÏû•\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ Î™®Îç∏ Ï†ÄÏû• Ï§ë...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "os.makedirs('models/enhanced', exist_ok=True)\n",
    "\n",
    "# 1. ÏûÑÎ≤†Îî© Ï†ÄÏû•\n",
    "np.save('models/enhanced/enhanced_embeddings.npy', enhanced_features)\n",
    "print(\"‚úÖ ÏûÑÎ≤†Îî© Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "\n",
    "# 2. XGBoost Î™®Îç∏ Ï†ÄÏû•\n",
    "joblib.dump(season_model, 'models/enhanced/season_model.joblib')\n",
    "joblib.dump(models['nature'], 'models/enhanced/nature_model.joblib')\n",
    "joblib.dump(models['vibe'], 'models/enhanced/vibe_model.joblib')\n",
    "joblib.dump(models['target'], 'models/enhanced/target_model.joblib')\n",
    "print(\"‚úÖ XGBoost Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "\n",
    "# 3. Ïù∏ÏΩîÎçî Ï†ÄÏû•\n",
    "joblib.dump(season_encoder, 'models/enhanced/season_encoder.joblib')\n",
    "joblib.dump(nature_encoder, 'models/enhanced/nature_encoder.joblib')\n",
    "joblib.dump(vibe_encoder, 'models/enhanced/vibe_encoder.joblib')\n",
    "joblib.dump(target_encoder, 'models/enhanced/target_encoder.joblib')\n",
    "print(\"‚úÖ Ïù∏ÏΩîÎçî Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "\n",
    "# 4. PCA Î™®Îç∏ Ï†ÄÏû•\n",
    "joblib.dump(pca_model, 'models/enhanced/pca_model.joblib')\n",
    "print(\"‚úÖ PCA Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "\n",
    "# 5. Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ Ï†ÄÏû•\n",
    "df.to_csv('models/enhanced/processed_data.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"‚úÖ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "\n",
    "print(f\"\\nüéâ Î™®Îì† Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å!\")\n",
    "print(f\"Ï†ÄÏû• ÏúÑÏπò: models/enhanced/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10534e49-f06c-4e01-b2f8-db777d97cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ÏÑ±Îä• ÌèâÍ∞ÄÎ•º ÏúÑÌïú Ï¢ÖÌï© ÎπÑÍµê ÏãúÏä§ÌÖú\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üî¨ Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÏÑ±Îä• ÌèâÍ∞Ä\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51275afe-65ff-4e64-b4d5-cffcd4c5c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Í∏∞Î≥∏ Ï∂îÏ≤ú ÏãúÏä§ÌÖú - Í∞úÏÑ† Ï†Ñ\n",
    "class BasicRecommendationSystem:\n",
    "    \"\"\"Í∏∞Î≥∏ Ï∂îÏ≤ú ÏãúÏä§ÌÖú (ÎπÑÍµêÏö©)\"\"\"\n",
    "    \n",
    "    def __init__(self, df, sbert_model):\n",
    "        self.df = df\n",
    "        self.sbert_model = sbert_model\n",
    "        \n",
    "        # Í∏∞Î≥∏ ÏûÑÎ≤†Îî© ÏÉùÏÑ± (Îã®Ïàú)\n",
    "        descriptions = df['short_description'].fillna('').astype(str).tolist()\n",
    "        print(\"üìù Í∏∞Î≥∏ ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ï§ë...\")\n",
    "        self.place_embeddings = sbert_model.encode(\n",
    "            descriptions,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        print(f\"‚úÖ Í∏∞Î≥∏ ÏûÑÎ≤†Îî© ÏôÑÎ£å: {self.place_embeddings.shape}\")\n",
    "    \n",
    "    def recommend(self, user_input: Dict, top_n: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"Í∏∞Î≥∏ Ï∂îÏ≤ú (Îã®Ïàú ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑÎßå ÏÇ¨Ïö©)\"\"\"\n",
    "        \n",
    "        # ÏÇ¨Ïö©Ïûê ÏøºÎ¶¨ ÏÉùÏÑ±\n",
    "        query_parts = []\n",
    "        if 'season' in user_input:\n",
    "            query_parts.append(user_input['season'])\n",
    "        for key in ['nature', 'vibe', 'target']:\n",
    "            if key in user_input:\n",
    "                values = user_input[key]\n",
    "                if isinstance(values, list):\n",
    "                    query_parts.extend(values)\n",
    "                else:\n",
    "                    query_parts.append(values)\n",
    "        \n",
    "        query_text = ' '.join(query_parts) if query_parts else \"Í¥ÄÍ¥ëÏßÄ\"\n",
    "        \n",
    "        # ÏøºÎ¶¨ ÏûÑÎ≤†Îî©\n",
    "        query_embedding = self.sbert_model.encode(\n",
    "            [query_text],\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        \n",
    "        # ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑÎßå ÏÇ¨Ïö©\n",
    "        similarities = cosine_similarity(query_embedding, self.place_embeddings)[0]\n",
    "        \n",
    "        # ÏÉÅÏúÑ NÍ∞ú ÏÑ†ÌÉù\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "        recommendations = self.df.iloc[top_indices].copy()\n",
    "        recommendations['score'] = similarities[top_indices]\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "print(\"\\nüì¶ Í∏∞Î≥∏ Ï∂îÏ≤ú ÏãúÏä§ÌÖú Ï§ÄÎπÑ Ï§ë...\")\n",
    "basic_system = BasicRecommendationSystem(df, ensemble_embedder.primary_model)\n",
    "print(\"‚úÖ Í∏∞Î≥∏ ÏãúÏä§ÌÖú Ï§ÄÎπÑ ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94203fdd-f48a-4856-9176-77ef373f4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ÏÑ±Îä• ÌèâÍ∞Ä Î©îÌä∏Î¶≠\n",
    "class RecommendationEvaluator:\n",
    "    \"\"\"Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÌèâÍ∞Ä ÌÅ¥ÎûòÏä§\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def evaluate_system(self, system, test_cases: List[Dict], \n",
    "                       system_name: str = \"System\") -> Dict:\n",
    "        \"\"\"ÏãúÏä§ÌÖú Ï¢ÖÌï© ÌèâÍ∞Ä\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîç {system_name} ÌèâÍ∞Ä Ï§ë...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        results = {\n",
    "            'system_name': system_name,\n",
    "            'precision_at_3': [],\n",
    "            'precision_at_5': [],\n",
    "            'recall_at_3': [],\n",
    "            'recall_at_5': [],\n",
    "            'ndcg_at_5': [],\n",
    "            'mrr': [],\n",
    "            'diversity': [],\n",
    "            'avg_time': [],\n",
    "            'tag_match_rate': []\n",
    "        }\n",
    "        \n",
    "        for i, test in enumerate(test_cases, 1):\n",
    "            print(f\"\\nÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ {i}/{len(test_cases)}: {test['name']}\")\n",
    "            \n",
    "            # Ï∂îÏ≤ú ÏãúÍ∞Ñ Ï∏°Ï†ï\n",
    "            start_time = time.time()\n",
    "            recommendations = system.recommend(test['input'], top_n=5)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            results['avg_time'].append(elapsed_time)\n",
    "            \n",
    "            # Ground Truth ÏÉùÏÑ± (Ïã§Ï†úÎ°úÎäî ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©)\n",
    "            ground_truth = self._generate_ground_truth(test['input'])\n",
    "            \n",
    "            # Í∞Å Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞\n",
    "            precision_3 = self._precision_at_k(recommendations, ground_truth, 3)\n",
    "            precision_5 = self._precision_at_k(recommendations, ground_truth, 5)\n",
    "            recall_3 = self._recall_at_k(recommendations, ground_truth, 3)\n",
    "            recall_5 = self._recall_at_k(recommendations, ground_truth, 5)\n",
    "            ndcg = self._ndcg_at_k(recommendations, ground_truth, 5)\n",
    "            mrr = self._mrr(recommendations, ground_truth)\n",
    "            diversity = self._diversity_score(recommendations)\n",
    "            tag_match = self._tag_match_rate(recommendations, test['input'])\n",
    "            \n",
    "            results['precision_at_3'].append(precision_3)\n",
    "            results['precision_at_5'].append(precision_5)\n",
    "            results['recall_at_3'].append(recall_3)\n",
    "            results['recall_at_5'].append(recall_5)\n",
    "            results['ndcg_at_5'].append(ndcg)\n",
    "            results['mrr'].append(mrr)\n",
    "            results['diversity'].append(diversity)\n",
    "            results['tag_match_rate'].append(tag_match)\n",
    "            \n",
    "            print(f\"  ‚è±Ô∏è  ÏãúÍ∞Ñ: {elapsed_time:.4f}Ï¥à\")\n",
    "            print(f\"  üìä Precision@5: {precision_5:.3f}\")\n",
    "            print(f\"  üìä ÌÉúÍ∑∏ Îß§Ïπ≠Î•†: {tag_match:.3f}\")\n",
    "        \n",
    "        # ÌèâÍ∑† Í≥ÑÏÇ∞\n",
    "        summary = {\n",
    "            'system_name': system_name,\n",
    "            'avg_precision_at_3': np.mean(results['precision_at_3']),\n",
    "            'avg_precision_at_5': np.mean(results['precision_at_5']),\n",
    "            'avg_recall_at_3': np.mean(results['recall_at_3']),\n",
    "            'avg_recall_at_5': np.mean(results['recall_at_5']),\n",
    "            'avg_ndcg_at_5': np.mean(results['ndcg_at_5']),\n",
    "            'avg_mrr': np.mean(results['mrr']),\n",
    "            'avg_diversity': np.mean(results['diversity']),\n",
    "            'avg_time': np.mean(results['avg_time']),\n",
    "            'avg_tag_match_rate': np.mean(results['tag_match_rate'])\n",
    "        }\n",
    "        \n",
    "        return summary, results\n",
    "    \n",
    "    def _generate_ground_truth(self, user_input: Dict) -> List[str]:\n",
    "        \"\"\"Ground Truth ÏÉùÏÑ± (Ïã§Ï†ú Ï†ïÎãµ Îç∞Ïù¥ÌÑ∞)\"\"\"\n",
    "        # Ïã§Ï†úÎ°úÎäî ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©Ìï¥Ïïº ÌïòÏßÄÎßå,\n",
    "        # Ïó¨Í∏∞ÏÑúÎäî ÌÉúÍ∑∏Í∞Ä Ï†ïÌôïÌûà ÏùºÏπòÌïòÎäî Ïû•ÏÜåÎì§ÏùÑ Ï†ïÎãµÏúºÎ°ú Í∞ÑÏ£º\n",
    "        \n",
    "        relevant_places = []\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            match_score = 0\n",
    "            \n",
    "            # Season Îß§Ïπ≠\n",
    "            if 'season' in user_input and row['season'] == user_input['season']:\n",
    "                match_score += 1\n",
    "            \n",
    "            # Nature Îß§Ïπ≠\n",
    "            if 'nature' in user_input:\n",
    "                user_nature = set(user_input['nature'] if isinstance(user_input['nature'], list) \n",
    "                                else [user_input['nature']])\n",
    "                place_nature = set(row['nature'])\n",
    "                if user_nature & place_nature:\n",
    "                    match_score += len(user_nature & place_nature)\n",
    "            \n",
    "            # Vibe Îß§Ïπ≠\n",
    "            if 'vibe' in user_input:\n",
    "                user_vibe = set(user_input['vibe'] if isinstance(user_input['vibe'], list) \n",
    "                              else [user_input['vibe']])\n",
    "                place_vibe = set(row['vibe'])\n",
    "                if user_vibe & place_vibe:\n",
    "                    match_score += len(user_vibe & place_vibe)\n",
    "            \n",
    "            # Target Îß§Ïπ≠\n",
    "            if 'target' in user_input:\n",
    "                user_target = set(user_input['target'] if isinstance(user_input['target'], list) \n",
    "                                else [user_input['target']])\n",
    "                place_target = set(row['target'])\n",
    "                if user_target & place_target:\n",
    "                    match_score += 1\n",
    "            \n",
    "            # Îß§Ïπ≠ Ï†êÏàòÍ∞Ä 2 Ïù¥ÏÉÅÏù¥Î©¥ Í¥ÄÎ†® ÏûàÎäî Ïû•ÏÜåÎ°ú Í∞ÑÏ£º\n",
    "            if match_score >= 2:\n",
    "                relevant_places.append(row['name'])\n",
    "        \n",
    "        return relevant_places[:10]  # ÏµúÎåÄ 10Í∞ú\n",
    "    \n",
    "    def _precision_at_k(self, recommendations: pd.DataFrame, \n",
    "                       ground_truth: List[str], k: int) -> float:\n",
    "        \"\"\"Precision@K\"\"\"\n",
    "        if len(recommendations) < k:\n",
    "            k = len(recommendations)\n",
    "        \n",
    "        top_k_names = recommendations['name'].iloc[:k].tolist()\n",
    "        relevant_in_top_k = len([name for name in top_k_names if name in ground_truth])\n",
    "        \n",
    "        return relevant_in_top_k / k if k > 0 else 0.0\n",
    "    \n",
    "    def _recall_at_k(self, recommendations: pd.DataFrame, \n",
    "                    ground_truth: List[str], k: int) -> float:\n",
    "        \"\"\"Recall@K\"\"\"\n",
    "        if not ground_truth:\n",
    "            return 0.0\n",
    "        \n",
    "        top_k_names = recommendations['name'].iloc[:k].tolist()\n",
    "        relevant_in_top_k = len([name for name in top_k_names if name in ground_truth])\n",
    "        \n",
    "        return relevant_in_top_k / len(ground_truth)\n",
    "    \n",
    "    def _ndcg_at_k(self, recommendations: pd.DataFrame, \n",
    "                   ground_truth: List[str], k: int) -> float:\n",
    "        \"\"\"NDCG@K (Normalized Discounted Cumulative Gain)\"\"\"\n",
    "        top_k_names = recommendations['name'].iloc[:k].tolist()\n",
    "        \n",
    "        # DCG Í≥ÑÏÇ∞\n",
    "        dcg = sum([\n",
    "            (1.0 if name in ground_truth else 0.0) / np.log2(i + 2)\n",
    "            for i, name in enumerate(top_k_names)\n",
    "        ])\n",
    "        \n",
    "        # IDCG Í≥ÑÏÇ∞ (Ïù¥ÏÉÅÏ†ÅÏù∏ ÏàúÏÑú)\n",
    "        ideal_length = min(len(ground_truth), k)\n",
    "        idcg = sum([1.0 / np.log2(i + 2) for i in range(ideal_length)])\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    def _mrr(self, recommendations: pd.DataFrame, ground_truth: List[str]) -> float:\n",
    "        \"\"\"MRR (Mean Reciprocal Rank)\"\"\"\n",
    "        for i, name in enumerate(recommendations['name']):\n",
    "            if name in ground_truth:\n",
    "                return 1.0 / (i + 1)\n",
    "        return 0.0\n",
    "    \n",
    "    def _diversity_score(self, recommendations: pd.DataFrame) -> float:\n",
    "        \"\"\"Ï∂îÏ≤ú Îã§ÏñëÏÑ± Ï†êÏàò\"\"\"\n",
    "        all_tags = set()\n",
    "        \n",
    "        for idx, row in recommendations.iterrows():\n",
    "            all_tags.update(row.get('nature', []))\n",
    "            all_tags.update(row.get('vibe', []))\n",
    "            all_tags.update(row.get('target', []))\n",
    "        \n",
    "        # Í≥†Ïú† ÌÉúÍ∑∏ Ïàò / (Ï∂îÏ≤ú Ïàò * ÌèâÍ∑† ÌÉúÍ∑∏ Ïàò)\n",
    "        avg_tags_per_place = 3  # ÎåÄÎûµÏ†ÅÏù∏ ÌèâÍ∑†\n",
    "        max_possible_tags = len(recommendations) * avg_tags_per_place\n",
    "        \n",
    "        return len(all_tags) / max_possible_tags if max_possible_tags > 0 else 0.0\n",
    "    \n",
    "    def _tag_match_rate(self, recommendations: pd.DataFrame, \n",
    "                       user_input: Dict) -> float:\n",
    "        \"\"\"ÌÉúÍ∑∏ Îß§Ïπ≠Î•† (ÏÇ¨Ïö©Ïûê ÏûÖÎ†•Í≥º Ï∂îÏ≤ú Í≤∞Í≥ºÏùò ÌÉúÍ∑∏ ÏùºÏπòÎèÑ)\"\"\"\n",
    "        total_matches = 0\n",
    "        total_possible = 0\n",
    "        \n",
    "        for idx, row in recommendations.iterrows():\n",
    "            # Season\n",
    "            if 'season' in user_input:\n",
    "                total_possible += 1\n",
    "                if row['season'] == user_input['season']:\n",
    "                    total_matches += 1\n",
    "            \n",
    "            # Nature, Vibe, Target\n",
    "            for key in ['nature', 'vibe', 'target']:\n",
    "                if key in user_input and user_input[key]:\n",
    "                    user_tags = set(user_input[key] if isinstance(user_input[key], list) \n",
    "                                  else [user_input[key]])\n",
    "                    place_tags = set(row.get(key, []))\n",
    "                    \n",
    "                    if user_tags:\n",
    "                        total_possible += len(user_tags)\n",
    "                        total_matches += len(user_tags & place_tags)\n",
    "        \n",
    "        return total_matches / total_possible if total_possible > 0 else 0.0\n",
    "\n",
    "# ÌèâÍ∞ÄÏûê ÏÉùÏÑ±\n",
    "evaluator = RecommendationEvaluator(df)\n",
    "print(\"‚úÖ ÌèâÍ∞ÄÏûê Ï§ÄÎπÑ ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b6f6b-876d-4a09-93e7-fd3b9a96592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Îã§ÏñëÌïú ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ Ï†ïÏùò\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Í≤®Ïö∏ Î∞îÎã§ Îç∞Ïù¥Ìä∏\",\n",
    "        \"input\": {\n",
    "            \"season\": \"Í≤®Ïö∏\",\n",
    "            \"nature\": [\"Î∞îÎã§\"],\n",
    "            \"vibe\": [\"Í∞êÏÑ±\", \"ÏÇ∞Ï±Ö\"],\n",
    "            \"target\": [\"Ïó∞Ïù∏\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ïó¨Î¶Ñ Í∞ÄÏ°± Ìï¥Î≥Ä Ìú¥Í∞Ä\",\n",
    "        \"input\": {\n",
    "            \"season\": \"Ïó¨Î¶Ñ\",\n",
    "            \"nature\": [\"Î∞îÎã§\", \"ÏûêÏó∞\"],\n",
    "            \"vibe\": [\"ÌûêÎßÅ\", \"Ïï°Ìã∞ÎπÑÌã∞\"],\n",
    "            \"target\": [\"Í∞ÄÏ°±\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Í∞ÄÏùÑ ÏÇ∞ ÌûêÎßÅ\",\n",
    "        \"input\": {\n",
    "            \"season\": \"Í∞ÄÏùÑ\",\n",
    "            \"nature\": [\"ÏÇ∞\"],\n",
    "            \"vibe\": [\"Ï°∞Ïö©Ìïú\", \"ÌûêÎßÅ\"],\n",
    "            \"target\": [\"ÏπúÍµ¨\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Î¥Ñ ÏûêÏó∞ ÏÇ∞Ï±Ö\",\n",
    "        \"input\": {\n",
    "            \"season\": \"Î¥Ñ\",\n",
    "            \"nature\": [\"ÏûêÏó∞\", \"ÏÇ∞\"],\n",
    "            \"vibe\": [\"ÏÇ∞Ï±Ö\"],\n",
    "            \"target\": [\"Ïó∞Ïù∏\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ÏÇ¨Í≥ÑÏ†à Í∞êÏÑ± Ïó¨Ìñâ\",\n",
    "        \"input\": {\n",
    "            \"season\": \"ÏÇ¨Í≥ÑÏ†à\",\n",
    "            \"nature\": [\"Ìò∏Ïàò\", \"ÏûêÏó∞\"],\n",
    "            \"vibe\": [\"Í∞êÏÑ±\", \"ÏÇ¨ÏßÑÎ™ÖÏÜå\"],\n",
    "            \"target\": [\"ÏπúÍµ¨\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ïó¨Î¶Ñ Ïä§Î¶¥ Î™®Ìóò\",\n",
    "        \"input\": {\n",
    "            \"season\": \"Ïó¨Î¶Ñ\",\n",
    "            \"nature\": [\"ÏÇ∞\", \"Î∞îÎã§\"],\n",
    "            \"vibe\": [\"Ïä§Î¶¥\", \"Ïï°Ìã∞ÎπÑÌã∞\"],\n",
    "            \"target\": [\"ÏπúÍµ¨\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Í≤®Ïö∏ Í∞ÄÏ°± Ïä§ÌÇ§\",\n",
    "        \"input\": {\n",
    "            \"season\": \"Í≤®Ïö∏\",\n",
    "            \"nature\": [\"ÏÇ∞\"],\n",
    "            \"vibe\": [\"Ïï°Ìã∞ÎπÑÌã∞\", \"Ïä§Î¶¥\"],\n",
    "            \"target\": [\"Í∞ÄÏ°±\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Í∞ÄÏùÑ Ïó≠ÏÇ¨ ÌÉêÎ∞©\",\n",
    "        \"input\": {\n",
    "            \"season\": \"Í∞ÄÏùÑ\",\n",
    "            \"nature\": [\"ÏûêÏó∞\"],\n",
    "            \"vibe\": [\"Ï°∞Ïö©Ìïú\"],\n",
    "            \"target\": [\"Í∞ÄÏ°±\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\n‚úÖ ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§ Ï§ÄÎπÑ ÏôÑÎ£å: {len(test_cases)}Í∞ú\")\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"  {i}. {test['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17361792-28dc-48e5-9fdd-91891c8e5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Í∏∞Î≥∏ ÏãúÏä§ÌÖú vs Í∞úÏÑ† ÏãúÏä§ÌÖú ÏÑ±Îä• ÎπÑÍµê\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚öîÔ∏è  ÏÑ±Îä• ÎπÑÍµê Ïã§Ìñâ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Í∏∞Î≥∏ ÏãúÏä§ÌÖú ÌèâÍ∞Ä\n",
    "print(\"\\nüîµ [1/2] Í∏∞Î≥∏ Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÌèâÍ∞Ä Ï§ë...\")\n",
    "basic_summary, basic_details = evaluator.evaluate_system(\n",
    "    basic_system, \n",
    "    test_cases, \n",
    "    system_name=\"Í∏∞Î≥∏ ÏãúÏä§ÌÖú\"\n",
    ")\n",
    "\n",
    "# 2. Í∞úÏÑ† ÏãúÏä§ÌÖú ÌèâÍ∞Ä\n",
    "print(\"\\nüü¢ [2/2] Í∞úÏÑ† Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÌèâÍ∞Ä Ï§ë...\")\n",
    "enhanced_summary, enhanced_details = evaluator.evaluate_system(\n",
    "    recommender, \n",
    "    test_cases, \n",
    "    system_name=\"Í∞úÏÑ† ÏãúÏä§ÌÖú\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Î™®Îì† ÌèâÍ∞Ä ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1881d9-8bd3-4bc2-81be-b602d66f2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Í≤∞Í≥º ÎπÑÍµêÌëú ÏÉùÏÑ±\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä ÏÑ±Îä• ÎπÑÍµê Í≤∞Í≥º\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# DataFrameÏúºÎ°ú Î≥ÄÌôò\n",
    "comparison_df = pd.DataFrame([basic_summary, enhanced_summary])\n",
    "comparison_df = comparison_df.set_index('system_name')\n",
    "\n",
    "# Í∞úÏÑ†Ïú® Í≥ÑÏÇ∞\n",
    "improvement = {}\n",
    "for col in comparison_df.columns:\n",
    "    basic_val = comparison_df.loc['Í∏∞Î≥∏ ÏãúÏä§ÌÖú', col]\n",
    "    enhanced_val = comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', col]\n",
    "    \n",
    "    if col == 'avg_time':\n",
    "        # ÏãúÍ∞ÑÏùÄ Í∞êÏÜåÍ∞Ä Ï¢ãÏùå\n",
    "        improvement[col] = ((basic_val - enhanced_val) / basic_val * 100)\n",
    "    else:\n",
    "        # ÎÇòÎ®∏ÏßÄÎäî Ï¶ùÍ∞ÄÍ∞Ä Ï¢ãÏùå\n",
    "        improvement[col] = ((enhanced_val - basic_val) / basic_val * 100) if basic_val > 0 else 0\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(\"\\nüìã Ï£ºÏöî Î©îÌä∏Î¶≠ ÎπÑÍµê:\\n\")\n",
    "print(f\"{'Î©îÌä∏Î¶≠':<30} {'Í∏∞Î≥∏ ÏãúÏä§ÌÖú':>15} {'Í∞úÏÑ† ÏãúÏä§ÌÖú':>15} {'Í∞úÏÑ†Ïú®':>15}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metric_names = {\n",
    "    'avg_precision_at_5': 'Precision@5',\n",
    "    'avg_recall_at_5': 'Recall@5',\n",
    "    'avg_ndcg_at_5': 'NDCG@5',\n",
    "    'avg_mrr': 'MRR',\n",
    "    'avg_diversity': 'Îã§ÏñëÏÑ±',\n",
    "    'avg_tag_match_rate': 'ÌÉúÍ∑∏ Îß§Ïπ≠Î•†',\n",
    "    'avg_time': 'ÌèâÍ∑† Ï≤òÎ¶¨ ÏãúÍ∞Ñ (Ï¥à)'\n",
    "}\n",
    "\n",
    "for col, name in metric_names.items():\n",
    "    basic_val = comparison_df.loc['Í∏∞Î≥∏ ÏãúÏä§ÌÖú', col]\n",
    "    enhanced_val = comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', col]\n",
    "    improve = improvement[col]\n",
    "    \n",
    "    if col == 'avg_time':\n",
    "        print(f\"{name:<30} {basic_val:>15.4f} {enhanced_val:>15.4f} {improve:>14.1f}%‚Üì\")\n",
    "    else:\n",
    "        print(f\"{name:<30} {basic_val:>15.4f} {enhanced_val:>15.4f} {improve:>14.1f}%‚Üë\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ï†ÑÏ≤¥ ÏÑ±Îä• Ìñ•ÏÉÅ Í≥ÑÏÇ∞\n",
    "key_metrics = ['avg_precision_at_5', 'avg_recall_at_5', 'avg_ndcg_at_5', 'avg_tag_match_rate']\n",
    "avg_improvement = np.mean([improvement[m] for m in key_metrics])\n",
    "\n",
    "print(f\"\\nüéØ Ï¢ÖÌï© ÏÑ±Îä• Ìñ•ÏÉÅ: {avg_improvement:.1f}%\")\n",
    "\n",
    "# ÏÑ±Îä• Îì±Í∏â Îß§Í∏∞Í∏∞\n",
    "if avg_improvement >= 30:\n",
    "    grade = \"üèÜ ÌÉÅÏõîÌïú Í∞úÏÑ†\"\n",
    "elif avg_improvement >= 20:\n",
    "    grade = \"ü•á Ïö∞ÏàòÌïú Í∞úÏÑ†\"\n",
    "elif avg_improvement >= 10:\n",
    "    grade = \"ü•à Ï¢ãÏùÄ Í∞úÏÑ†\"\n",
    "else:\n",
    "    grade = \"ü•â Î≥¥ÌÜµ Í∞úÏÑ†\"\n",
    "\n",
    "print(f\"ÌèâÍ∞Ä: {grade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1499c-3469-4df6-a8f1-13524f6cd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ÏÑ±Îä• ÎπÑÍµê ÏãúÍ∞ÅÌôî\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä ÏãúÍ∞ÅÌôî ÏÉùÏÑ± Ï§ë...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï (Windows)\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 1. Î©îÌä∏Î¶≠Î≥Ñ ÎπÑÍµê Í∑∏ÎûòÌîÑ\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('üî¨ Ï∂îÏ≤ú ÏãúÏä§ÌÖú ÏÑ±Îä• ÎπÑÍµê', fontsize=20, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = [\n",
    "    ('avg_precision_at_5', 'Precision@5', axes[0, 0]),\n",
    "    ('avg_recall_at_5', 'Recall@5', axes[0, 1]),\n",
    "    ('avg_ndcg_at_5', 'NDCG@5', axes[0, 2]),\n",
    "    ('avg_mrr', 'MRR', axes[1, 0]),\n",
    "    ('avg_diversity', 'Îã§ÏñëÏÑ±', axes[1, 1]),\n",
    "    ('avg_tag_match_rate', 'ÌÉúÍ∑∏ Îß§Ïπ≠Î•†', axes[1, 2])\n",
    "]\n",
    "\n",
    "for col, title, ax in metrics_to_plot:\n",
    "    values = [comparison_df.loc['Í∏∞Î≥∏ ÏãúÏä§ÌÖú', col], \n",
    "              comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', col]]\n",
    "    colors = ['#3498db', '#2ecc71']\n",
    "    \n",
    "    bars = ax.bar(['Í∏∞Î≥∏', 'Í∞úÏÑ†'], values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Ï†êÏàò', fontsize=12)\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Í∞í ÌëúÏãú\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{values[i]:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Í∞úÏÑ†Ïú® ÌëúÏãú\n",
    "    improve_pct = improvement[col]\n",
    "    ax.text(0.5, 0.95, f'Í∞úÏÑ†: {improve_pct:+.1f}%',\n",
    "            transform=ax.transAxes,\n",
    "            ha='center', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5),\n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Í∑∏ÎûòÌîÑ Ï†ÄÏû•: performance_comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "# 2. ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§Î≥Ñ ÏÑ±Îä• ÎπÑÍµê\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(test_cases))\n",
    "width = 0.35\n",
    "\n",
    "basic_scores = basic_details['precision_at_5']\n",
    "enhanced_scores = enhanced_details['precision_at_5']\n",
    "\n",
    "bars1 = ax.bar(x - width/2, basic_scores, width, label='Í∏∞Î≥∏ ÏãúÏä§ÌÖú', \n",
    "               color='#3498db', alpha=0.7, edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, enhanced_scores, width, label='Í∞úÏÑ† ÏãúÏä§ÌÖú', \n",
    "               color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Precision@5', fontsize=12, fontweight='bold')\n",
    "ax.set_title('üìä ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§Î≥Ñ Precision@5 ÎπÑÍµê', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"TC{i+1}\" for i in range(len(test_cases))], rotation=0)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Í∞í ÌëúÏãú\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_case_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Í∑∏ÎûòÌîÑ Ï†ÄÏû•: test_case_comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Î†àÏù¥Îçî Ï∞®Ìä∏\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "categories = ['Precision', 'Recall', 'NDCG', 'MRR', 'Îã§ÏñëÏÑ±', 'ÌÉúÍ∑∏Îß§Ïπ≠']\n",
    "metrics_radar = ['avg_precision_at_5', 'avg_recall_at_5', 'avg_ndcg_at_5', \n",
    "                 'avg_mrr', 'avg_diversity', 'avg_tag_match_rate']\n",
    "\n",
    "basic_values = [comparison_df.loc['Í∏∞Î≥∏ ÏãúÏä§ÌÖú', m] for m in metrics_radar]\n",
    "enhanced_values = [comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', m] for m in metrics_radar]\n",
    "\n",
    "# Í∞ÅÎèÑ ÏÑ§Ï†ï\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "basic_values += basic_values[:1]\n",
    "enhanced_values += enhanced_values[:1]\n",
    "angles += angles[:1]\n",
    "\n",
    "ax.plot(angles, basic_values, 'o-', linewidth=2, label='Í∏∞Î≥∏ ÏãúÏä§ÌÖú', color='#3498db')\n",
    "ax.fill(angles, basic_values, alpha=0.25, color='#3498db')\n",
    "ax.plot(angles, enhanced_values, 'o-', linewidth=2, label='Í∞úÏÑ† ÏãúÏä§ÌÖú', color='#2ecc71')\n",
    "ax.fill(angles, enhanced_values, alpha=0.25, color='#2ecc71')\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('üéØ Ï¢ÖÌï© ÏÑ±Îä• Î†àÏù¥Îçî Ï∞®Ìä∏', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('radar_chart.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Í∑∏ÎûòÌîÑ Ï†ÄÏû•: radar_chart.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Î™®Îì† ÏãúÍ∞ÅÌôî ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e60980-7d5c-41eb-a6db-748585954d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Íµ¨Ï≤¥Ï†ÅÏù∏ Ï∂îÏ≤ú Í≤∞Í≥º ÎπÑÍµê\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç ÏÉÅÏÑ∏ ÏÇ¨Î°Ä Î∂ÑÏÑù\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ï≤´ Î≤àÏß∏ ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§Î°ú ÏÉÅÏÑ∏ ÎπÑÍµê\n",
    "test_case = test_cases[0]\n",
    "\n",
    "print(f\"\\nüìå ÌÖåÏä§Ìä∏ ÏºÄÏù¥Ïä§: {test_case['name']}\")\n",
    "print(f\"ÏûÖÎ†•: {test_case['input']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"üîµ Í∏∞Î≥∏ ÏãúÏä§ÌÖú Ï∂îÏ≤ú Í≤∞Í≥º:\")\n",
    "print(\"-\"*80)\n",
    "basic_recs = basic_system.recommend(test_case['input'], top_n=5)\n",
    "for i, (idx, row) in enumerate(basic_recs.iterrows(), 1):\n",
    "    print(f\"\\n{i}. {row['name']}\")\n",
    "    print(f\"   Í≥ÑÏ†à: {row['season']}\")\n",
    "    print(f\"   ÏûêÏó∞: {', '.join(row['nature'])}\")\n",
    "    print(f\"   Î∂ÑÏúÑÍ∏∞: {', '.join(row['vibe'])}\")\n",
    "    print(f\"   Ï†êÏàò: {row['score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"üü¢ Í∞úÏÑ† ÏãúÏä§ÌÖú Ï∂îÏ≤ú Í≤∞Í≥º:\")\n",
    "print(\"-\"*80)\n",
    "enhanced_recs = recommender.recommend(test_case['input'], top_n=5)\n",
    "for i, (idx, row) in enumerate(enhanced_recs.iterrows(), 1):\n",
    "    print(f\"\\n{i}. {row['name']}\")\n",
    "    print(f\"   Í≥ÑÏ†à: {row['season']}\")\n",
    "    print(f\"   ÏûêÏó∞: {', '.join(row['nature'])}\")\n",
    "    print(f\"   Î∂ÑÏúÑÍ∏∞: {', '.join(row['vibe'])}\")\n",
    "    print(f\"   ÎåÄÏÉÅ: {', '.join(row['target']) if row['target'] else 'Ï†ïÎ≥¥ÏóÜÏùå'}\")\n",
    "    print(f\"   ÏµúÏ¢Ö Ï†êÏàò: {row['final_score']:.4f}\")\n",
    "    print(f\"   (Ïú†ÏÇ¨ÎèÑ: {row['similarity_score']:.3f}, ÌÉúÍ∑∏: {row['tag_score']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"üìä ÎπÑÍµê Î∂ÑÏÑù:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# ÌÉúÍ∑∏ Îß§Ïπ≠ Î∂ÑÏÑù\n",
    "def analyze_tag_matching(recs, user_input):\n",
    "    matches = {'season': 0, 'nature': 0, 'vibe': 0, 'target': 0}\n",
    "    total = len(recs)\n",
    "    \n",
    "    for idx, row in recs.iterrows():\n",
    "        if row['season'] == user_input.get('season'):\n",
    "            matches['season'] += 1\n",
    "        \n",
    "        for key in ['nature', 'vibe', 'target']:\n",
    "            if key in user_input and user_input[key]:\n",
    "                user_tags = set(user_input[key] if isinstance(user_input[key], list) \n",
    "                              else [user_input[key]])\n",
    "                place_tags = set(row.get(key, []))\n",
    "                if user_tags & place_tags:\n",
    "                    matches[key] += 1\n",
    "    \n",
    "    return {k: v/total for k, v in matches.items()}\n",
    "\n",
    "basic_matches = analyze_tag_matching(basic_recs, test_case['input'])\n",
    "enhanced_matches = analyze_tag_matching(enhanced_recs, test_case['input'])\n",
    "\n",
    "print(\"\\nÌÉúÍ∑∏ Îß§Ïπ≠Î•† ÎπÑÍµê:\")\n",
    "print(f\"{'Ïπ¥ÌÖåÍ≥†Î¶¨':<15} {'Í∏∞Î≥∏ ÏãúÏä§ÌÖú':>15} {'Í∞úÏÑ† ÏãúÏä§ÌÖú':>15} {'Í∞úÏÑ†':>15}\")\n",
    "print(\"-\"*65)\n",
    "for key in ['season', 'nature', 'vibe', 'target']:\n",
    "    basic_val = basic_matches[key]\n",
    "    enhanced_val = enhanced_matches[key]\n",
    "    improve = ((enhanced_val - basic_val) / basic_val * 100) if basic_val > 0 else 0\n",
    "    print(f\"{key:<15} {basic_val:>14.1%} {enhanced_val:>14.1%} {improve:>13.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5788bb8-e3bb-467e-8d80-1206c47ac0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ÏµúÏ¢Ö ÏÑ±Îä• Î¶¨Ìè¨Ìä∏\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã ÏµúÏ¢Ö ÏÑ±Îä• ÌèâÍ∞Ä Î¶¨Ìè¨Ìä∏\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                        üèÜ ÏÑ±Îä• ÌèâÍ∞Ä ÏµúÏ¢Ö ÏöîÏïΩ                               ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                           ‚ïë\n",
    "‚ïë  üìä Ï£ºÏöî Î©îÌä∏Î¶≠ Í∞úÏÑ†Ïú®:                                                     ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚ïë\n",
    "‚ïë  ‚Ä¢ Precision@5        : {improvement['avg_precision_at_5']:>6.1f}% ‚Üë                                  ‚ïë\n",
    "‚ïë  ‚Ä¢ Recall@5           : {improvement['avg_recall_at_5']:>6.1f}% ‚Üë                                  ‚ïë\n",
    "‚ïë  ‚Ä¢ NDCG@5             : {improvement['avg_ndcg_at_5']:>6.1f}% ‚Üë                                  ‚ïë\n",
    "‚ïë  ‚Ä¢ ÌÉúÍ∑∏ Îß§Ïπ≠Î•†         : {improvement['avg_tag_match_rate']:>6.1f}% ‚Üë                                  ‚ïë\n",
    "‚ïë                                                                           ‚ïë\n",
    "‚ïë  ‚ö° ÏÑ±Îä• ÏßÄÌëú:                                                              ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚ïë\n",
    "‚ïë  ‚Ä¢ ÌèâÍ∑† Ï≤òÎ¶¨ ÏãúÍ∞Ñ      : {comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', 'avg_time']:.4f}Ï¥à                                    ‚ïë\n",
    "‚ïë  ‚Ä¢ Ï∂îÏ≤ú Îã§ÏñëÏÑ±         : {comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', 'avg_diversity']:.3f}                                       ‚ïë\n",
    "‚ïë  ‚Ä¢ MRR                : {comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', 'avg_mrr']:.3f}                                       ‚ïë\n",
    "‚ïë                                                                           ‚ïë\n",
    "‚ïë  üéØ Ï¢ÖÌï© ÌèâÍ∞Ä:                                                             ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚ïë\n",
    "‚ïë  ‚Ä¢ Ï†ÑÏ≤¥ ÏÑ±Îä• Ìñ•ÏÉÅ      : {avg_improvement:>6.1f}%                                        ‚ïë\n",
    "‚ïë  ‚Ä¢ ÌèâÍ∞Ä Îì±Í∏â           : {grade:<20}                          ‚ïë\n",
    "‚ïë                                                                           ‚ïë\n",
    "‚ïë  üí° Ï£ºÏöî Í∞úÏÑ† ÏÇ¨Ìï≠:                                                         ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚ïë\n",
    "‚ïë  ‚úì Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ïÏúºÎ°ú ÏÑ§Î™Ö ÌÖçÏä§Ìä∏ ÌíàÏßà Ìñ•ÏÉÅ                                      ‚ïë\n",
    "‚ïë  ‚úì ÏïôÏÉÅÎ∏î ÏûÑÎ≤†Îî©ÏúºÎ°ú ÏùòÎØ∏ ÌëúÌòÑÎ†• Ï¶ùÍ∞Ä                                        ‚ïë\n",
    "‚ïë  ‚úì ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅÏúºÎ°ú Î∂ÑÎ•ò Ï†ïÌôïÎèÑ Í∞úÏÑ†                                       ‚ïë\n",
    "‚ïë  ‚úì Í≥†Í∏â Ïä§ÏΩîÏñ¥ÎßÅÏúºÎ°ú ÌÉúÍ∑∏ Îß§Ïπ≠ Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ                                    ‚ïë\n",
    "‚ïë  ‚úì XGBoost ÏµúÏ†ÅÌôîÎ°ú ÏòàÏ∏° ÏÑ±Îä• Í∞úÏÑ†                                          ‚ïë\n",
    "‚ïë                                                                           ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")\n",
    "\n",
    "# CSVÎ°ú ÏÉÅÏÑ∏ Í≤∞Í≥º Ï†ÄÏû•\n",
    "results_summary = pd.DataFrame({\n",
    "    'ÏãúÏä§ÌÖú': ['Í∏∞Î≥∏', 'Í∞úÏÑ†'],\n",
    "    'Precision@5': [comparison_df.loc['Í∏∞Î≥∏ ÏãúÏä§ÌÖú', 'avg_precision_at_5'],\n",
    "                   comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', 'avg_precision_at_5']],\n",
    "    'Recall@5': [comparison_df.loc['Í∏∞Î≥∏ ÏãúÏä§ÌÖú', 'avg_recall_at_5'],\n",
    "                comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', 'avg_recall_at_5']],\n",
    "    'NDCG@5': [comparison_df.loc['Í∏∞Î≥∏ ÏãúÏä§ÌÖú', 'avg_ndcg_at_5'],\n",
    "              comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', 'avg_ndcg_at_5']],\n",
    "    'MRR': [comparison_df.loc['Í∏∞Î≥∏ ÏãúÏä§ÌÖú', 'avg_mrr'],\n",
    "           comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', 'avg_mrr']],\n",
    "    'Îã§ÏñëÏÑ±': [comparison_df.loc['Í∏∞Î≥∏ ÏãúÏä§ÌÖú', 'avg_diversity'],\n",
    "             comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', 'avg_diversity']],\n",
    "    'ÌÉúÍ∑∏Îß§Ïπ≠Î•†': [comparison_df.loc['Í∏∞Î≥∏ ÏãúÏä§ÌÖú', 'avg_tag_match_rate'],\n",
    "                comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', 'avg_tag_match_rate']],\n",
    "    'Ï≤òÎ¶¨ÏãúÍ∞Ñ': [comparison_df.loc['Í∏∞Î≥∏ ÏãúÏä§ÌÖú', 'avg_time'],\n",
    "               comparison_df.loc['Í∞úÏÑ† ÏãúÏä§ÌÖú', 'avg_time']]\n",
    "})\n",
    "\n",
    "results_summary.to_csv('performance_evaluation_results.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\nüíæ ÏÉÅÏÑ∏ Í≤∞Í≥º Ï†ÄÏû•: performance_evaluation_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ÏÑ±Îä• ÌèâÍ∞Ä ÏôÑÎ£å!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb4aa4-c8d6-4271-a452-8880e7ca07c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a7aba-da33-4c6e-9158-b1b85fddcdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445dc03e-f485-4b6c-9aee-2fad1f070335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09013ab2-2146-4b44-afaf-d976064e6ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f94e1-5e14-4a80-81ec-309ce23c6127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3246ec8-4a95-4d30-89b0-286630010cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05ccc2-4139-4673-b8be-6522d6fe29e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f42a71-bdd4-46b2-a2ab-e31a6a178a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29522932-47d3-4671-bd4c-9469b211bbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84c9fe-e292-4102-935e-bab6f3b450e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
