{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import joblib\n",
        "import os\n",
        "import warnings\n",
        "from typing import Dict, List, Optional, Union, Tuple\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Core ML libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.multioutput import MultiOutputClassifier\n"
      ],
      "metadata": {
        "id": "0g6MfbtblNB7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 경고 메시지 숨기기\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TourismRecommendationSystem:\n",
        "    \"\"\"\n",
        "    SBERT + XGBoost 기반 강원도 관광지 추천 시스템\n",
        "\n",
        "    TF-IDF + LogisticRegression → SBERT + XGBoost 업그레이드\n",
        "    기존 recommend_places() 함수 시그니처 완전 호환\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, model_name: str = 'jhgan/ko-sroberta-multitask'):\n",
        "        \"\"\"\n",
        "        추천 시스템 초기화\n",
        "\n",
        "        Args:\n",
        "            data_path: CSV 파일 경로 (gangwon_places_100_enriched.csv)\n",
        "            model_name: SBERT 모델명 (한국어 KoBERT)\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # 데이터 및 모델 초기화\n",
        "        self.df = None\n",
        "        self.sbert_model = None\n",
        "        self.place_embeddings = None\n",
        "\n",
        "        # XGBoost 모델들\n",
        "        self.season_model = None\n",
        "        self.nature_model = None\n",
        "        self.vibe_model = None\n",
        "        self.target_model = None\n",
        "\n",
        "        # 인코더들\n",
        "        self.season_encoder = LabelEncoder()\n",
        "        self.nature_encoder = MultiLabelBinarizer()\n",
        "        self.vibe_encoder = MultiLabelBinarizer()\n",
        "        self.target_encoder = MultiLabelBinarizer()\n",
        "\n",
        "        # 가중치 설정 (코사인 유사도 0.6 + 태그 매칭 0.4)\n",
        "        self.similarity_weight = 0.6\n",
        "        self.tag_weight = 0.4\n",
        "\n",
        "        # 시스템 초기화\n",
        "        self._initialize_system()\n",
        "\n",
        "    def _initialize_system(self):\n",
        "        \"\"\"시스템 전체 초기화\"\"\"\n",
        "        logger.info(\"🚀 관광지 추천 시스템 초기화 시작...\")\n",
        "\n",
        "        # 1. 데이터 로드 및 전처리\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "        # 2. SBERT 모델 로드\n",
        "        self._load_sbert_model()\n",
        "\n",
        "        # 3. 관광지 임베딩 생성\n",
        "        self._generate_place_embeddings()\n",
        "\n",
        "        # 4. XGBoost 모델 훈련\n",
        "        self._train_xgboost_models()\n",
        "\n",
        "        logger.info(\"✅ 시스템 초기화 완료!\")\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        \"\"\"CSV 데이터 로드 및 전처리\"\"\"\n",
        "        logger.info(\"📁 데이터 로드 중...\")\n",
        "\n",
        "        try:\n",
        "            self.df = pd.read_csv(self.data_path)\n",
        "            logger.info(f\"데이터 로드 완료: {len(self.df)}개 관광지\")\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"데이터 파일을 찾을 수 없습니다: {self.data_path}\")\n",
        "\n",
        "        # 데이터 전처리\n",
        "        self._preprocess_data()\n",
        "\n",
        "    def _preprocess_data(self):\n",
        "        \"\"\"데이터 전처리: 문자열 → 리스트 변환\"\"\"\n",
        "        logger.info(\"🔧 데이터 전처리 중...\")\n",
        "\n",
        "        # 필수 컬럼 확인\n",
        "        required_cols = ['name', 'season', 'nature', 'vibe', 'target', 'short_description']\n",
        "        missing_cols = [col for col in required_cols if col not in self.df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"필수 컬럼이 누락되었습니다: {missing_cols}\")\n",
        "\n",
        "        # nature, vibe, target을 쉼표 구분 문자열에서 리스트로 변환\n",
        "        for col in ['nature', 'vibe', 'target']:\n",
        "            self.df[col] = self.df[col].apply(self._string_to_list)\n",
        "\n",
        "        # 결측치 처리\n",
        "        self.df['short_description'] = self.df['short_description'].fillna('')\n",
        "        self.df['season'] = self.df['season'].fillna('사계절')\n",
        "\n",
        "        logger.info(\"✅ 데이터 전처리 완료\")\n",
        "\n",
        "    def _string_to_list(self, value) -> List[str]:\n",
        "        \"\"\"문자열을 리스트로 변환 (쉼표 구분)\"\"\"\n",
        "        if pd.isna(value) or value == '':\n",
        "            return []\n",
        "        if isinstance(value, list):\n",
        "            return value\n",
        "        return [item.strip() for item in str(value).split(',') if item.strip()]\n",
        "\n",
        "    def _load_sbert_model(self):\n",
        "        \"\"\"SBERT 모델 로드 (안전한 폴백 처리)\"\"\"\n",
        "        logger.info(f\"🤖 SBERT 모델 로드 중: {self.model_name}\")\n",
        "\n",
        "        # 안정적인 한국어 모델 우선순위\n",
        "        model_candidates = [\n",
        "            self.model_name,\n",
        "            'jhgan/ko-sroberta-multitask',\n",
        "            'jhgan/ko-sbert-multitask',\n",
        "            'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "        ]\n",
        "\n",
        "        for model_name in model_candidates:\n",
        "            try:\n",
        "                logger.info(f\"모델 시도 중: {model_name}\")\n",
        "                self.sbert_model = SentenceTransformer(model_name)\n",
        "                self.model_name = model_name  # 성공한 모델명으로 업데이트\n",
        "                logger.info(f\"✅ SBERT 모델 로드 완료: {model_name}\")\n",
        "                return\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"모델 {model_name} 로드 실패: {e}\")\n",
        "                continue\n",
        "\n",
        "        raise RuntimeError(\"모든 SBERT 모델 로드에 실패했습니다.\")\n",
        "\n",
        "    def _generate_place_embeddings(self):\n",
        "        \"\"\"관광지 설명에 대한 SBERT 임베딩 생성 (안전한 처리)\"\"\"\n",
        "        logger.info(\"🔮 관광지 임베딩 생성 중...\")\n",
        "\n",
        "        descriptions = self.df['short_description'].fillna('').astype(str).tolist()\n",
        "\n",
        "        # 빈 설명 처리\n",
        "        descriptions = [desc if desc.strip() else \"관광지\" for desc in descriptions]\n",
        "\n",
        "        try:\n",
        "            # 배치 처리로 임베딩 생성\n",
        "            batch_size = 16  # 작은 배치 크기로 안정성 향상\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(descriptions), batch_size):\n",
        "                batch = descriptions[i:i + batch_size]\n",
        "                logger.info(f\"임베딩 배치 처리 중: {i+1}-{min(i+batch_size, len(descriptions))}/{len(descriptions)}\")\n",
        "\n",
        "                # 각 텍스트의 길이 제한 (토크나이저 오류 방지)\n",
        "                batch = [text[:500] if len(text) > 500 else text for text in batch]\n",
        "\n",
        "                batch_embeddings = self.sbert_model.encode(\n",
        "                    batch,\n",
        "                    normalize_embeddings=True,\n",
        "                    show_progress_bar=False,\n",
        "                    convert_to_tensor=False,  # NumPy 배열로 반환\n",
        "                    device='cpu'  # CPU 사용으로 안정성 향상\n",
        "                )\n",
        "                embeddings.extend(batch_embeddings)\n",
        "\n",
        "            self.place_embeddings = np.array(embeddings)\n",
        "            logger.info(f\"✅ 임베딩 생성 완료: {self.place_embeddings.shape}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"임베딩 생성 실패: {e}\")\n",
        "            # 폴백: 랜덤 임베딩 생성\n",
        "            logger.warning(\"랜덤 임베딩으로 대체합니다.\")\n",
        "            embedding_dim = 384  # 기본 차원\n",
        "            self.place_embeddings = np.random.rand(len(descriptions), embedding_dim)\n",
        "            logger.info(f\"랜덤 임베딩 생성: {self.place_embeddings.shape}\")\n",
        "\n",
        "    def _prepare_training_data(self) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
        "        \"\"\"XGBoost 훈련용 데이터 준비\"\"\"\n",
        "        logger.info(\"📊 훈련 데이터 준비 중...\")\n",
        "\n",
        "        # 특성: SBERT 임베딩\n",
        "        X = self.place_embeddings\n",
        "\n",
        "        # 라벨 준비\n",
        "        labels = {}\n",
        "\n",
        "        # Season (단일 라벨)\n",
        "        labels['season'] = self.season_encoder.fit_transform(self.df['season'])\n",
        "\n",
        "        # Nature, Vibe, Target (다중 라벨)\n",
        "        labels['nature'] = self.nature_encoder.fit_transform(self.df['nature'])\n",
        "        labels['vibe'] = self.vibe_encoder.fit_transform(self.df['vibe'])\n",
        "        labels['target'] = self.target_encoder.fit_transform(self.df['target'])\n",
        "\n",
        "        logger.info(\"✅ 훈련 데이터 준비 완료\")\n",
        "        return X, labels\n",
        "\n",
        "    def _train_xgboost_models(self):\n",
        "        \"\"\"XGBoost 모델들 훈련 (안전한 처리)\"\"\"\n",
        "        logger.info(\"🏋️ XGBoost 모델 훈련 시작...\")\n",
        "\n",
        "        try:\n",
        "            X, y = self._prepare_training_data()\n",
        "\n",
        "            # 데이터 검증\n",
        "            if X.shape[0] < 2:\n",
        "                logger.warning(\"데이터가 부족합니다. 최소 2개 이상의 샘플이 필요합니다.\")\n",
        "                return\n",
        "\n",
        "            # Season 모델 (multi:softmax)\n",
        "            logger.info(\"계절 분류기 훈련 중...\")\n",
        "            n_classes = len(np.unique(y['season']))\n",
        "\n",
        "            if n_classes > 1:\n",
        "                self.season_model = XGBClassifier(\n",
        "                    objective='multi:softprob' if n_classes > 2 else 'binary:logistic',\n",
        "                    n_estimators=min(150, len(X) * 2),  # 데이터 크기에 맞게 조정\n",
        "                    max_depth=min(5, max(2, len(X) // 10)),\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=42,\n",
        "                    tree_method='hist',\n",
        "                    verbosity=0  # 로그 출력 최소화\n",
        "                )\n",
        "                self.season_model.fit(X, y['season'])\n",
        "            else:\n",
        "                logger.warning(\"계절 데이터에 클래스가 1개뿐입니다. 더미 모델을 사용합니다.\")\n",
        "                from sklearn.dummy import DummyClassifier\n",
        "                self.season_model = DummyClassifier(strategy='most_frequent')\n",
        "                self.season_model.fit(X, y['season'])\n",
        "\n",
        "            # 다중 라벨 모델들 (binary:logistic)\n",
        "            for label_name in ['nature', 'vibe', 'target']:\n",
        "                logger.info(f\"{label_name} 분류기 훈련 중...\")\n",
        "\n",
        "                # 라벨이 모두 0인지 확인\n",
        "                if y[label_name].sum() == 0:\n",
        "                    logger.warning(f\"{label_name} 라벨이 모두 비어있습니다. 더미 모델을 사용합니다.\")\n",
        "                    from sklearn.dummy import DummyClassifier\n",
        "                    dummy_model = DummyClassifier(strategy='constant', constant=0)\n",
        "                    # MultiOutputClassifier 형태로 래핑\n",
        "                    model = MultiOutputClassifier(dummy_model)\n",
        "                    model.fit(X, y[label_name])\n",
        "                else:\n",
        "                    base_model = XGBClassifier(\n",
        "                        objective='binary:logistic',\n",
        "                        n_estimators=min(150, len(X) * 2),\n",
        "                        max_depth=min(5, max(2, len(X) // 10)),\n",
        "                        learning_rate=0.1,\n",
        "                        random_state=42,\n",
        "                        tree_method='hist',\n",
        "                        verbosity=0\n",
        "                    )\n",
        "\n",
        "                    # MultiOutputClassifier로 다중 라벨 처리\n",
        "                    model = MultiOutputClassifier(base_model, n_jobs=1)  # n_jobs=1로 안정성 향상\n",
        "                    model.fit(X, y[label_name])\n",
        "\n",
        "                setattr(self, f\"{label_name}_model\", model)\n",
        "\n",
        "            logger.info(\"✅ 모든 XGBoost 모델 훈련 완료!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"XGBoost 모델 훈련 실패: {e}\")\n",
        "            # 폴백: 더미 모델들 생성\n",
        "            logger.warning(\"더미 모델들로 대체합니다.\")\n",
        "            self._create_dummy_models()\n",
        "\n",
        "    def _create_dummy_models(self):\n",
        "        \"\"\"더미 모델들 생성 (폴백용)\"\"\"\n",
        "        from sklearn.dummy import DummyClassifier\n",
        "\n",
        "        X, y = self._prepare_training_data()\n",
        "\n",
        "        # Season 더미 모델\n",
        "        self.season_model = DummyClassifier(strategy='most_frequent')\n",
        "        self.season_model.fit(X, y['season'])\n",
        "\n",
        "        # 다중 라벨 더미 모델들\n",
        "        for label_name in ['nature', 'vibe', 'target']:\n",
        "            dummy_model = DummyClassifier(strategy='constant', constant=0)\n",
        "            model = MultiOutputClassifier(dummy_model)\n",
        "            model.fit(X, y[label_name])\n",
        "            setattr(self, f\"{label_name}_model\", model)\n",
        "\n",
        "    def _encode_user_input(self, user_input: Dict[str, Union[str, List[str]]]) -> np.ndarray:\n",
        "        \"\"\"사용자 입력을 SBERT 임베딩으로 변환 (안전한 처리)\"\"\"\n",
        "        try:\n",
        "            # 사용자 입력을 하나의 텍스트로 결합\n",
        "            combined_text = f\"{user_input.get('season', '')} \"\n",
        "\n",
        "            for key in ['nature', 'vibe', 'target']:\n",
        "                if key in user_input:\n",
        "                    values = user_input[key]\n",
        "                    if isinstance(values, list):\n",
        "                        combined_text += ' '.join(str(v) for v in values) + ' '\n",
        "                    else:\n",
        "                        combined_text += str(values) + ' '\n",
        "\n",
        "            # 텍스트 정리\n",
        "            combined_text = combined_text.strip()\n",
        "            if not combined_text:\n",
        "                combined_text = \"관광지\"\n",
        "\n",
        "            # 길이 제한\n",
        "            if len(combined_text) > 500:\n",
        "                combined_text = combined_text[:500]\n",
        "\n",
        "            # SBERT 임베딩 생성\n",
        "            user_embedding = self.sbert_model.encode(\n",
        "                [combined_text],\n",
        "                normalize_embeddings=True,\n",
        "                convert_to_tensor=False,\n",
        "                device='cpu'\n",
        "            )\n",
        "            return user_embedding[0]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"사용자 입력 인코딩 실패: {e}\")\n",
        "            # 폴백: 랜덤 임베딩\n",
        "            embedding_dim = self.place_embeddings.shape[1] if self.place_embeddings is not None else 384\n",
        "            return np.random.rand(embedding_dim)\n",
        "\n",
        "    def _predict_categories(self, user_embedding: np.ndarray) -> Dict[str, Union[str, List[str]]]:\n",
        "        \"\"\"사용자 임베딩으로부터 카테고리 예측 (안전한 처리)\"\"\"\n",
        "        user_embedding_2d = user_embedding.reshape(1, -1)\n",
        "        predictions = {}\n",
        "\n",
        "        try:\n",
        "            # Season 예측\n",
        "            if self.season_model is not None:\n",
        "                season_pred = self.season_model.predict(user_embedding_2d)[0]\n",
        "                predictions['season'] = self.season_encoder.inverse_transform([season_pred])[0]\n",
        "            else:\n",
        "                predictions['season'] = '사계절'\n",
        "\n",
        "            # 다중 라벨 예측\n",
        "            for label_name in ['nature', 'vibe', 'target']:\n",
        "                model = getattr(self, f\"{label_name}_model\", None)\n",
        "                encoder = getattr(self, f\"{label_name}_encoder\", None)\n",
        "\n",
        "                if model is not None and encoder is not None:\n",
        "                    try:\n",
        "                        pred = model.predict(user_embedding_2d)\n",
        "                        predicted_labels = encoder.inverse_transform(pred)[0]\n",
        "                        predictions[label_name] = list(predicted_labels)\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"{label_name} 예측 실패: {e}\")\n",
        "                        predictions[label_name] = []\n",
        "                else:\n",
        "                    predictions[label_name] = []\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"카테고리 예측 실패: {e}\")\n",
        "            # 기본값 반환\n",
        "            predictions = {\n",
        "                'season': '사계절',\n",
        "                'nature': [],\n",
        "                'vibe': [],\n",
        "                'target': []\n",
        "            }\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def _calculate_similarity_scores(self, user_embedding: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"사용자 임베딩과 모든 관광지 간 코사인 유사도 계산 (안전한 처리)\"\"\"\n",
        "        try:\n",
        "            user_embedding_2d = user_embedding.reshape(1, -1)\n",
        "\n",
        "            # 임베딩 차원 확인 및 조정\n",
        "            if user_embedding_2d.shape[1] != self.place_embeddings.shape[1]:\n",
        "                logger.warning(f\"임베딩 차원 불일치: {user_embedding_2d.shape[1]} vs {self.place_embeddings.shape[1]}\")\n",
        "                # 차원 맞추기\n",
        "                min_dim = min(user_embedding_2d.shape[1], self.place_embeddings.shape[1])\n",
        "                user_embedding_2d = user_embedding_2d[:, :min_dim]\n",
        "                place_embeddings_adjusted = self.place_embeddings[:, :min_dim]\n",
        "            else:\n",
        "                place_embeddings_adjusted = self.place_embeddings\n",
        "\n",
        "            similarities = cosine_similarity(user_embedding_2d, place_embeddings_adjusted)[0]\n",
        "\n",
        "            # NaN 값 처리\n",
        "            similarities = np.nan_to_num(similarities, nan=0.0)\n",
        "\n",
        "            return similarities\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"유사도 계산 실패: {e}\")\n",
        "            # 폴백: 랜덤 유사도\n",
        "            return np.random.rand(len(self.df)) * 0.1  # 낮은 점수\n",
        "\n",
        "    def _calculate_tag_matching_scores(self, user_input: Dict, predicted_categories: Dict) -> np.ndarray:\n",
        "        \"\"\"태그 매칭 점수 계산 (안전한 처리)\"\"\"\n",
        "        scores = np.zeros(len(self.df))\n",
        "\n",
        "        try:\n",
        "            for idx, row in self.df.iterrows():\n",
        "                score = 0\n",
        "\n",
        "                # Season 매칭\n",
        "                user_season = user_input.get('season', '')\n",
        "                place_season = row.get('season', '')\n",
        "                if user_season and place_season and user_season == place_season:\n",
        "                    score += 1\n",
        "\n",
        "                # 다중 라벨 매칭 (Jaccard 유사도)\n",
        "                for category in ['nature', 'vibe', 'target']:\n",
        "                    if category in user_input:\n",
        "                        user_tags = user_input[category]\n",
        "                        if isinstance(user_tags, str):\n",
        "                            user_tags = [user_tags]\n",
        "                        elif not isinstance(user_tags, list):\n",
        "                            user_tags = []\n",
        "\n",
        "                        user_tags = set(str(tag).strip() for tag in user_tags if tag)\n",
        "\n",
        "                        place_tags = row.get(category, [])\n",
        "                        if isinstance(place_tags, str):\n",
        "                            place_tags = [place_tags]\n",
        "                        elif not isinstance(place_tags, list):\n",
        "                            place_tags = []\n",
        "\n",
        "                        place_tags = set(str(tag).strip() for tag in place_tags if tag)\n",
        "\n",
        "                        if user_tags and place_tags:\n",
        "                            intersection = len(user_tags.intersection(place_tags))\n",
        "                            union = len(user_tags.union(place_tags))\n",
        "                            jaccard = intersection / union if union > 0 else 0\n",
        "                            score += jaccard\n",
        "\n",
        "                scores[idx] = score\n",
        "\n",
        "            # 정규화 (0-1 범위)\n",
        "            if scores.max() > 0:\n",
        "                scores = scores / scores.max()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"태그 매칭 점수 계산 실패: {e}\")\n",
        "            # 폴백: 균등한 점수\n",
        "            scores = np.ones(len(self.df)) * 0.5\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def recommend_places(self, user_input: Dict[str, Union[str, List[str]]],\n",
        "                        tfidf_vectorizer=None, season_model=None, nature_model=None,\n",
        "                        vibe_model=None, target_model=None, df=None, top_n: int = 3) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        기존 시그니처와 완전 호환되는 추천 함수\n",
        "\n",
        "        Args:\n",
        "            user_input: 사용자 선호도\n",
        "                - season: str (예: \"여름\")\n",
        "                - nature: List[str] (예: [\"바다\", \"자연\"])\n",
        "                - vibe: List[str] (예: [\"감성\", \"산책\"])\n",
        "                - target: List[str] (예: [\"연인\"])\n",
        "            tfidf_vectorizer: 호환성용 (사용하지 않음)\n",
        "            season_model: 호환성용 (사용하지 않음)\n",
        "            nature_model: 호환성용 (사용하지 않음)\n",
        "            vibe_model: 호환성용 (사용하지 않음)\n",
        "            target_model: 호환성용 (사용하지 않음)\n",
        "            df: 호환성용 (사용하지 않음)\n",
        "            top_n: 추천할 관광지 수\n",
        "\n",
        "        Returns:\n",
        "            추천 관광지 DataFrame (name, city, description 컬럼 포함)\n",
        "        \"\"\"\n",
        "        logger.info(f\"🎯 {top_n}개 관광지 추천 생성 중...\")\n",
        "\n",
        "        try:\n",
        "            # 1. 사용자 입력을 SBERT 임베딩으로 변환\n",
        "            user_embedding = self._encode_user_input(user_input)\n",
        "\n",
        "            # 2. 카테고리 예측 (참고용)\n",
        "            predicted_categories = self._predict_categories(user_embedding)\n",
        "            logger.info(f\"예측된 카테고리: {predicted_categories}\")\n",
        "\n",
        "            # 3. 코사인 유사도 계산\n",
        "            similarity_scores = self._calculate_similarity_scores(user_embedding)\n",
        "\n",
        "            # 4. 태그 매칭 점수 계산\n",
        "            tag_scores = self._calculate_tag_matching_scores(user_input, predicted_categories)\n",
        "\n",
        "            # 5. 최종 점수 계산 (가중 평균)\n",
        "            final_scores = (\n",
        "                self.similarity_weight * similarity_scores +\n",
        "                self.tag_weight * tag_scores\n",
        "            )\n",
        "\n",
        "            # 6. 상위 N개 추천\n",
        "            top_indices = np.argsort(final_scores)[::-1][:top_n]\n",
        "\n",
        "            # 7. 결과 DataFrame 생성 (기존 포맷 호환)\n",
        "            recommendations = self.df.iloc[top_indices].copy()\n",
        "\n",
        "            # 추천 점수 추가\n",
        "            recommendations['similarity_score'] = similarity_scores[top_indices]\n",
        "            recommendations['tag_score'] = tag_scores[top_indices]\n",
        "            recommendations['final_score'] = final_scores[top_indices]\n",
        "\n",
        "            # 기존 호환 형식으로 반환 (name, city, description)\n",
        "            # city 컬럼이 없으면 빈 값으로 추가\n",
        "            if 'city' not in recommendations.columns:\n",
        "                recommendations['city'] = ''\n",
        "\n",
        "            # description이 없으면 short_description 사용\n",
        "            if 'description' not in recommendations.columns:\n",
        "                recommendations['description'] = recommendations['short_description']\n",
        "\n",
        "            result = recommendations[['name', 'city', 'description']].reset_index(drop=True)\n",
        "\n",
        "            logger.info(\"✅ 추천 생성 완료!\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"추천 생성 실패: {e}\")\n",
        "            # 폴백: 첫 번째 N개 관광지 반환\n",
        "            fallback_df = self.df.head(top_n).copy()\n",
        "            if 'city' not in fallback_df.columns:\n",
        "                fallback_df['city'] = ''\n",
        "            if 'description' not in fallback_df.columns:\n",
        "                fallback_df['description'] = fallback_df['short_description']\n",
        "\n",
        "            return fallback_df[['name', 'city', 'description']].reset_index(drop=True)\n",
        "\n",
        "    def get_detailed_recommendations(self, user_input: Dict[str, Union[str, List[str]]],\n",
        "                                   top_n: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"상세 정보가 포함된 추천 결과 반환\"\"\"\n",
        "        logger.info(f\"🔍 상세 추천 정보 생성 중...\")\n",
        "\n",
        "        user_embedding = self._encode_user_input(user_input)\n",
        "        predicted_categories = self._predict_categories(user_embedding)\n",
        "        similarity_scores = self._calculate_similarity_scores(user_embedding)\n",
        "        tag_scores = self._calculate_tag_matching_scores(user_input, predicted_categories)\n",
        "        final_scores = (\n",
        "            self.similarity_weight * similarity_scores +\n",
        "            self.tag_weight * tag_scores\n",
        "        )\n",
        "\n",
        "        top_indices = np.argsort(final_scores)[::-1][:top_n]\n",
        "        recommendations = self.df.iloc[top_indices].copy()\n",
        "\n",
        "        # 점수 정보 추가\n",
        "        recommendations['similarity_score'] = similarity_scores[top_indices]\n",
        "        recommendations['tag_score'] = tag_scores[top_indices]\n",
        "        recommendations['final_score'] = final_scores[top_indices]\n",
        "        recommendations['predicted_season'] = predicted_categories['season']\n",
        "        recommendations['predicted_nature'] = str(predicted_categories['nature'])\n",
        "        recommendations['predicted_vibe'] = str(predicted_categories['vibe'])\n",
        "        recommendations['predicted_target'] = str(predicted_categories['target'])\n",
        "\n",
        "        return recommendations.reset_index(drop=True)\n",
        "\n",
        "    def save_models(self, model_dir: str):\n",
        "        \"\"\"모델들을 지정된 디렉토리에 저장\"\"\"\n",
        "        logger.info(f\"💾 모델 저장 중: {model_dir}\")\n",
        "\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "        # SBERT 임베딩 저장\n",
        "        np.save(os.path.join(model_dir, 'place_embeddings.npy'), self.place_embeddings)\n",
        "\n",
        "        # XGBoost 모델들 저장\n",
        "        models_to_save = {\n",
        "            'season_model': self.season_model,\n",
        "            'nature_model': self.nature_model,\n",
        "            'vibe_model': self.vibe_model,\n",
        "            'target_model': self.target_model\n",
        "        }\n",
        "\n",
        "        for name, model in models_to_save.items():\n",
        "            if model is not None:\n",
        "                joblib.dump(model, os.path.join(model_dir, f'{name}.joblib'))\n",
        "\n",
        "        # 인코더들 저장\n",
        "        encoders_to_save = {\n",
        "            'season_encoder': self.season_encoder,\n",
        "            'nature_encoder': self.nature_encoder,\n",
        "            'vibe_encoder': self.vibe_encoder,\n",
        "            'target_encoder': self.target_encoder\n",
        "        }\n",
        "\n",
        "        for name, encoder in encoders_to_save.items():\n",
        "            joblib.dump(encoder, os.path.join(model_dir, f'{name}.joblib'))\n",
        "\n",
        "        # 데이터프레임과 설정 저장\n",
        "        self.df.to_csv(os.path.join(model_dir, 'processed_data.csv'), index=False)\n",
        "\n",
        "        config = {\n",
        "            'model_name': self.model_name,\n",
        "            'similarity_weight': self.similarity_weight,\n",
        "            'tag_weight': self.tag_weight,\n",
        "            'data_path': self.data_path\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(model_dir, 'config.pickle'), 'wb') as f:\n",
        "            pickle.dump(config, f)\n",
        "\n",
        "        logger.info(\"✅ 모델 저장 완료!\")\n",
        "\n",
        "    def load_models(self, model_dir: str):\n",
        "        \"\"\"저장된 모델들을 로드\"\"\"\n",
        "        logger.info(f\"📂 모델 로드 중: {model_dir}\")\n",
        "\n",
        "        # 설정 로드\n",
        "        with open(os.path.join(model_dir, 'config.pickle'), 'rb') as f:\n",
        "            config = pickle.load(f)\n",
        "\n",
        "        self.model_name = config['model_name']\n",
        "        self.similarity_weight = config['similarity_weight']\n",
        "        self.tag_weight = config['tag_weight']\n",
        "\n",
        "        # 데이터 로드\n",
        "        self.df = pd.read_csv(os.path.join(model_dir, 'processed_data.csv'))\n",
        "\n",
        "        # nature, vibe, target 컬럼을 다시 리스트로 변환\n",
        "        for col in ['nature', 'vibe', 'target']:\n",
        "            self.df[col] = self.df[col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "        # SBERT 모델 로드\n",
        "        self.sbert_model = SentenceTransformer(self.model_name)\n",
        "\n",
        "        # 임베딩 로드\n",
        "        self.place_embeddings = np.load(os.path.join(model_dir, 'place_embeddings.npy'))\n",
        "\n",
        "        # XGBoost 모델들 로드\n",
        "        model_names = ['season_model', 'nature_model', 'vibe_model', 'target_model']\n",
        "        for name in model_names:\n",
        "            model_path = os.path.join(model_dir, f'{name}.joblib')\n",
        "            if os.path.exists(model_path):\n",
        "                setattr(self, name, joblib.load(model_path))\n",
        "\n",
        "        # 인코더들 로드\n",
        "        encoder_names = ['season_encoder', 'nature_encoder', 'vibe_encoder', 'target_encoder']\n",
        "        for name in encoder_names:\n",
        "            encoder_path = os.path.join(model_dir, f'{name}.joblib')\n",
        "            if os.path.exists(encoder_path):\n",
        "                setattr(self, name, joblib.load(encoder_path))\n",
        "\n",
        "        logger.info(\"✅ 모델 로드 완료!\")\n",
        "\n",
        "    def evaluate_models(self) -> Dict[str, float]:\n",
        "        \"\"\"모델 성능 평가\"\"\"\n",
        "        logger.info(\"📊 모델 성능 평가 중...\")\n",
        "\n",
        "        X = self.place_embeddings\n",
        "        results = {}\n",
        "\n",
        "        # Season 모델 평가\n",
        "        y_season = self.season_encoder.transform(self.df['season'])\n",
        "        season_pred = self.season_model.predict(X)\n",
        "        results['season_accuracy'] = (y_season == season_pred).mean()\n",
        "\n",
        "        # 다중 라벨 모델들 평가\n",
        "        for label_name in ['nature', 'vibe', 'target']:\n",
        "            encoder = getattr(self, f\"{label_name}_encoder\")\n",
        "            model = getattr(self, f\"{label_name}_model\")\n",
        "\n",
        "            y_true = encoder.transform(self.df[label_name])\n",
        "            y_pred = model.predict(X)\n",
        "\n",
        "            f1 = f1_score(y_true, y_pred, average='micro')\n",
        "            results[f'{label_name}_f1_score'] = f1\n",
        "\n",
        "        logger.info(\"평가 결과:\")\n",
        "        for metric, score in results.items():\n",
        "            logger.info(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "nge5ZfUNojdk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 함수 시그니처와 완전 호환되는 래퍼 함수\n",
        "def recommend_places(user_input: Dict[str, Union[str, List[str]]],\n",
        "                    tfidf_vectorizer=None, season_model=None, nature_model=None,\n",
        "                    vibe_model=None, target_model=None, df=None, top_n: int = 3,\n",
        "                    system: TourismRecommendationSystem = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    기존 코드와 완전 호환되는 추천 함수\n",
        "\n",
        "    새로운 시스템을 사용하려면 system 파라미터에 TourismRecommendationSystem 인스턴스를 전달\n",
        "    \"\"\"\n",
        "    if system is None:\n",
        "        raise ValueError(\"TourismRecommendationSystem 인스턴스를 system 파라미터로 전달해주세요.\")\n",
        "\n",
        "    return system.recommend_places(\n",
        "        user_input=user_input,\n",
        "        tfidf_vectorizer=tfidf_vectorizer,\n",
        "        season_model=season_model,\n",
        "        nature_model=nature_model,\n",
        "        vibe_model=vibe_model,\n",
        "        target_model=target_model,\n",
        "        df=df,\n",
        "        top_n=top_n\n",
        "    )\n",
        "\n",
        "\n",
        "# 사용 예시 및 테스트 코드\n",
        "def demo_usage():\n",
        "    \"\"\"사용 예시 데모 (안전한 처리)\"\"\"\n",
        "    print(\"🌟 강원도 관광지 추천 시스템 - SBERT + XGBoost 버전\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Google Drive 마운트 시도\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"✅ Google Drive 마운트 완료\")\n",
        "\n",
        "        # 사용자 지정 CSV 파일 경로\n",
        "        data_path = \"/content/drive/My Drive/졸업논문/gangwon_places_100.csv\"\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"ℹ️ Google Colab 환경이 아닙니다.\")\n",
        "        data_path = \"/content/drive/My Drive/졸업논문/gangwon_places_100.csv\"\n",
        "\n",
        "    # 샘플 데이터 생성\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"❌ 파일을 찾을 수 없습니다: {data_path}\")\n",
        "        print(\"💡 올바른 파일 경로를 확인해주세요.\")\n",
        "        return\n",
        "    else:\n",
        "        print(f\"✅ 실제 데이터 파일 발견: 100개 관광지\")\n",
        "\n",
        "    try:\n",
        "        # 1. 시스템 초기화 (실제 100개 관광지 데이터 사용)\n",
        "        print(f\"\\n🔧 시스템 초기화 중... (데이터: {data_path})\")\n",
        "        system = TourismRecommendationSystem(data_path)\n",
        "\n",
        "        # 2. 사용자 입력 예시\n",
        "        user_input = {\n",
        "            \"season\": \"겨울\",\n",
        "            \"nature\": [\"바다\", \"자연\"],\n",
        "            \"vibe\": [\"감성\", \"산책\"],\n",
        "            \"target\": [\"연인\"]\n",
        "        }\n",
        "\n",
        "        print(f\"\\n🔍 사용자 선호도: {user_input}\")\n",
        "\n",
        "        # 3. 기존 함수 시그니처로 추천 실행\n",
        "        print(\"\\n📋 기존 시그니처 호환 테스트:\")\n",
        "        recommendations = system.recommend_places(\n",
        "            user_input=user_input,\n",
        "            tfidf_vectorizer=None,  # 더 이상 사용하지 않음\n",
        "            season_model=None,      # 더 이상 사용하지 않음\n",
        "            nature_model=None,      # 더 이상 사용하지 않음\n",
        "            vibe_model=None,        # 더 이상 사용하지 않음\n",
        "            target_model=None,      # 더 이상 사용하지 않음\n",
        "            df=None,               # 더 이상 사용하지 않음\n",
        "            top_n=5  # 🎯 추천 개수: 5개로 변경\n",
        "        )\n",
        "\n",
        "        print(\"🏆 추천 결과:\")\n",
        "        print(recommendations.to_string(index=False))\n",
        "\n",
        "        # 4. 상세 추천 결과\n",
        "        print(\"\\n📊 상세 추천 결과:\")\n",
        "        detailed = system.get_detailed_recommendations(user_input, top_n=5)  # 🎯 상세 추천도 5개로 변경\n",
        "        print(detailed[['name', 'season', 'final_score', 'similarity_score', 'tag_score']].to_string(index=False))\n",
        "\n",
        "        # 5. 모델 성능 평가\n",
        "        print(\"\\n📈 모델 성능:\")\n",
        "        performance = system.evaluate_models()\n",
        "\n",
        "        # 6. 모델 저장 테스트\n",
        "        print(\"\\n💾 모델 저장 테스트:\")\n",
        "        try:\n",
        "            system.save_models(\"saved_models\")\n",
        "            print(\"✅ 모델 저장 완료!\")\n",
        "\n",
        "            # 새 인스턴스로 로드 테스트\n",
        "            print(\"🔄 모델 로드 테스트...\")\n",
        "            new_system = TourismRecommendationSystem.__new__(TourismRecommendationSystem)\n",
        "            new_system.load_models(\"saved_models\")\n",
        "            print(\"✅ 모델 로드 완료!\")\n",
        "\n",
        "            # 로드된 시스템으로 추천 테스트\n",
        "            test_rec = new_system.recommend_places(user_input=user_input, top_n=5)  # 🎯 로드 테스트도 5개로 변경\n",
        "            print(\"✅ 로드된 모델 추천 테스트 완료!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ 모델 저장/로드 경고: {e}\")\n",
        "\n",
        "        print(\"\\n🎉 모든 테스트 완료!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 시스템 초기화 실패: {e}\")\n",
        "        print(\"상세 오류:\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_usage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktW3vTSCpdya",
        "outputId": "30cb6435-6608-4742-ac37-64a8a96a8f39"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌟 강원도 관광지 추천 시스템 - SBERT + XGBoost 버전\n",
            "============================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive 마운트 완료\n",
            "✅ 실제 데이터 파일 발견: 100개 관광지\n",
            "\n",
            "🔧 시스템 초기화 중... (데이터: /content/drive/My Drive/졸업논문/gangwon_places_100.csv)\n",
            "\n",
            "🔍 사용자 선호도: {'season': '겨울', 'nature': ['바다', '자연'], 'vibe': ['감성', '산책'], 'target': ['연인']}\n",
            "\n",
            "📋 기존 시그니처 호환 테스트:\n",
            "🏆 추천 결과:\n",
            "        name city                                                    description\n",
            "        설악산책               설악산책은(는) 겨울에 특히 아름다워 산 경관이 뛰어나며 (감성) 분위기로 연인에게 추천됩니다.\n",
            "구봉산 전망대 카페거리       구봉산 전망대 카페거리은(는) 겨울에 특히 아름다워 산 경관이 뛰어나며 (감성) 분위기로 연인에게 추천됩니다.\n",
            "   이승만별장(고성)         이승만별장(고성)은(는) 겨울에 특히 아름다워 호수 경관이 뛰어나며 (감성) 분위기로 연인에게 추천됩니다.\n",
            "사근진 해중공원 전망대      사근진 해중공원 전망대은(는) 여름에 특히 아름다워 바다 경관이 뛰어나며 (감성) 분위기로 연인에게 추천됩니다.\n",
            "        순담계곡               순담계곡은(는) 여름에 특히 아름다워 산 경관이 뛰어나며 (감성) 분위기로 연인에게 추천됩니다.\n",
            "\n",
            "📊 상세 추천 결과:\n",
            "        name season  final_score  similarity_score  tag_score\n",
            "        설악산책     겨울     0.853182          0.755304   1.000000\n",
            "구봉산 전망대 카페거리     겨울     0.832358          0.739115   0.972222\n",
            "   이승만별장(고성)     겨울     0.790047          0.691745   0.937500\n",
            "사근진 해중공원 전망대     여름     0.717062          0.639547   0.833333\n",
            "        순담계곡     여름     0.710684          0.652066   0.798611\n",
            "\n",
            "📈 모델 성능:\n",
            "\n",
            "💾 모델 저장 테스트:\n",
            "✅ 모델 저장 완료!\n",
            "🔄 모델 로드 테스트...\n",
            "✅ 모델 로드 완료!\n",
            "✅ 로드된 모델 추천 테스트 완료!\n",
            "\n",
            "🎉 모든 테스트 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cpk66qZXpfcg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}