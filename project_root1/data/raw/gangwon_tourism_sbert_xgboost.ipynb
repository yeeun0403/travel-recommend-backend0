{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import joblib\n",
        "import os\n",
        "import warnings\n",
        "from typing import Dict, List, Optional, Union, Tuple\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Core ML libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.multioutput import MultiOutputClassifier\n"
      ],
      "metadata": {
        "id": "0g6MfbtblNB7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ë¡œê¹… ì„¤ì •\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class TourismRecommendationSystem:\n",
        "    \"\"\"\n",
        "    SBERT + XGBoost ê¸°ë°˜ ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œ\n",
        "\n",
        "    TF-IDF + LogisticRegression â†’ SBERT + XGBoost ì—…ê·¸ë ˆì´ë“œ\n",
        "    ê¸°ì¡´ recommend_places() í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ì™„ì „ í˜¸í™˜\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, model_name: str = 'jhgan/ko-sroberta-multitask'):\n",
        "        \"\"\"\n",
        "        ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
        "\n",
        "        Args:\n",
        "            data_path: CSV íŒŒì¼ ê²½ë¡œ (gangwon_places_100_enriched.csv)\n",
        "            model_name: SBERT ëª¨ë¸ëª… (í•œêµ­ì–´ KoBERT)\n",
        "        \"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # ë°ì´í„° ë° ëª¨ë¸ ì´ˆê¸°í™”\n",
        "        self.df = None\n",
        "        self.sbert_model = None\n",
        "        self.place_embeddings = None\n",
        "\n",
        "        # XGBoost ëª¨ë¸ë“¤\n",
        "        self.season_model = None\n",
        "        self.nature_model = None\n",
        "        self.vibe_model = None\n",
        "        self.target_model = None\n",
        "\n",
        "        # ì¸ì½”ë”ë“¤\n",
        "        self.season_encoder = LabelEncoder()\n",
        "        self.nature_encoder = MultiLabelBinarizer()\n",
        "        self.vibe_encoder = MultiLabelBinarizer()\n",
        "        self.target_encoder = MultiLabelBinarizer()\n",
        "\n",
        "        # ê°€ì¤‘ì¹˜ ì„¤ì • (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ 0.6 + íƒœê·¸ ë§¤ì¹­ 0.4)\n",
        "        self.similarity_weight = 0.6\n",
        "        self.tag_weight = 0.4\n",
        "\n",
        "        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
        "        self._initialize_system()\n",
        "\n",
        "    def _initialize_system(self):\n",
        "        \"\"\"ì‹œìŠ¤í…œ ì „ì²´ ì´ˆê¸°í™”\"\"\"\n",
        "        logger.info(\"ğŸš€ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...\")\n",
        "\n",
        "        # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "        self._load_and_preprocess_data()\n",
        "\n",
        "        # 2. SBERT ëª¨ë¸ ë¡œë“œ\n",
        "        self._load_sbert_model()\n",
        "\n",
        "        # 3. ê´€ê´‘ì§€ ì„ë² ë”© ìƒì„±\n",
        "        self._generate_place_embeddings()\n",
        "\n",
        "        # 4. XGBoost ëª¨ë¸ í›ˆë ¨\n",
        "        self._train_xgboost_models()\n",
        "\n",
        "        logger.info(\"âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
        "\n",
        "    def _load_and_preprocess_data(self):\n",
        "        \"\"\"CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\"\"\"\n",
        "        logger.info(\"ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
        "\n",
        "        try:\n",
        "            self.df = pd.read_csv(self.data_path)\n",
        "            logger.info(f\"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ ê´€ê´‘ì§€\")\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_path}\")\n",
        "\n",
        "        # ë°ì´í„° ì „ì²˜ë¦¬\n",
        "        self._preprocess_data()\n",
        "\n",
        "    def _preprocess_data(self):\n",
        "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬: ë¬¸ìì—´ â†’ ë¦¬ìŠ¤íŠ¸ ë³€í™˜\"\"\"\n",
        "        logger.info(\"ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
        "\n",
        "        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n",
        "        required_cols = ['name', 'season', 'nature', 'vibe', 'target', 'short_description']\n",
        "        missing_cols = [col for col in required_cols if col not in self.df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_cols}\")\n",
        "\n",
        "        # nature, vibe, targetì„ ì‰¼í‘œ êµ¬ë¶„ ë¬¸ìì—´ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "        for col in ['nature', 'vibe', 'target']:\n",
        "            self.df[col] = self.df[col].apply(self._string_to_list)\n",
        "\n",
        "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
        "        self.df['short_description'] = self.df['short_description'].fillna('')\n",
        "        self.df['season'] = self.df['season'].fillna('ì‚¬ê³„ì ˆ')\n",
        "\n",
        "        logger.info(\"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
        "\n",
        "    def _string_to_list(self, value) -> List[str]:\n",
        "        \"\"\"ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì‰¼í‘œ êµ¬ë¶„)\"\"\"\n",
        "        if pd.isna(value) or value == '':\n",
        "            return []\n",
        "        if isinstance(value, list):\n",
        "            return value\n",
        "        return [item.strip() for item in str(value).split(',') if item.strip()]\n",
        "\n",
        "    def _load_sbert_model(self):\n",
        "        \"\"\"SBERT ëª¨ë¸ ë¡œë“œ (ì•ˆì „í•œ í´ë°± ì²˜ë¦¬)\"\"\"\n",
        "        logger.info(f\"ğŸ¤– SBERT ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}\")\n",
        "\n",
        "        # ì•ˆì •ì ì¸ í•œêµ­ì–´ ëª¨ë¸ ìš°ì„ ìˆœìœ„\n",
        "        model_candidates = [\n",
        "            self.model_name,\n",
        "            'jhgan/ko-sroberta-multitask',\n",
        "            'jhgan/ko-sbert-multitask',\n",
        "            'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "        ]\n",
        "\n",
        "        for model_name in model_candidates:\n",
        "            try:\n",
        "                logger.info(f\"ëª¨ë¸ ì‹œë„ ì¤‘: {model_name}\")\n",
        "                self.sbert_model = SentenceTransformer(model_name)\n",
        "                self.model_name = model_name  # ì„±ê³µí•œ ëª¨ë¸ëª…ìœ¼ë¡œ ì—…ë°ì´íŠ¸\n",
        "                logger.info(f\"âœ… SBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}\")\n",
        "                return\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"ëª¨ë¸ {model_name} ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "                continue\n",
        "\n",
        "        raise RuntimeError(\"ëª¨ë“  SBERT ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    def _generate_place_embeddings(self):\n",
        "        \"\"\"ê´€ê´‘ì§€ ì„¤ëª…ì— ëŒ€í•œ SBERT ì„ë² ë”© ìƒì„± (ì•ˆì „í•œ ì²˜ë¦¬)\"\"\"\n",
        "        logger.info(\"ğŸ”® ê´€ê´‘ì§€ ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
        "\n",
        "        descriptions = self.df['short_description'].fillna('').astype(str).tolist()\n",
        "\n",
        "        # ë¹ˆ ì„¤ëª… ì²˜ë¦¬\n",
        "        descriptions = [desc if desc.strip() else \"ê´€ê´‘ì§€\" for desc in descriptions]\n",
        "\n",
        "        try:\n",
        "            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±\n",
        "            batch_size = 16  # ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì•ˆì •ì„± í–¥ìƒ\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(descriptions), batch_size):\n",
        "                batch = descriptions[i:i + batch_size]\n",
        "                logger.info(f\"ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1}-{min(i+batch_size, len(descriptions))}/{len(descriptions)}\")\n",
        "\n",
        "                # ê° í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ ì œí•œ (í† í¬ë‚˜ì´ì € ì˜¤ë¥˜ ë°©ì§€)\n",
        "                batch = [text[:500] if len(text) > 500 else text for text in batch]\n",
        "\n",
        "                batch_embeddings = self.sbert_model.encode(\n",
        "                    batch,\n",
        "                    normalize_embeddings=True,\n",
        "                    show_progress_bar=False,\n",
        "                    convert_to_tensor=False,  # NumPy ë°°ì—´ë¡œ ë°˜í™˜\n",
        "                    device='cpu'  # CPU ì‚¬ìš©ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ\n",
        "                )\n",
        "                embeddings.extend(batch_embeddings)\n",
        "\n",
        "            self.place_embeddings = np.array(embeddings)\n",
        "            logger.info(f\"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {self.place_embeddings.shape}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}\")\n",
        "            # í´ë°±: ëœë¤ ì„ë² ë”© ìƒì„±\n",
        "            logger.warning(\"ëœë¤ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
        "            embedding_dim = 384  # ê¸°ë³¸ ì°¨ì›\n",
        "            self.place_embeddings = np.random.rand(len(descriptions), embedding_dim)\n",
        "            logger.info(f\"ëœë¤ ì„ë² ë”© ìƒì„±: {self.place_embeddings.shape}\")\n",
        "\n",
        "    def _prepare_training_data(self) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
        "        \"\"\"XGBoost í›ˆë ¨ìš© ë°ì´í„° ì¤€ë¹„\"\"\"\n",
        "        logger.info(\"ğŸ“Š í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì¤‘...\")\n",
        "\n",
        "        # íŠ¹ì„±: SBERT ì„ë² ë”©\n",
        "        X = self.place_embeddings\n",
        "\n",
        "        # ë¼ë²¨ ì¤€ë¹„\n",
        "        labels = {}\n",
        "\n",
        "        # Season (ë‹¨ì¼ ë¼ë²¨)\n",
        "        labels['season'] = self.season_encoder.fit_transform(self.df['season'])\n",
        "\n",
        "        # Nature, Vibe, Target (ë‹¤ì¤‘ ë¼ë²¨)\n",
        "        labels['nature'] = self.nature_encoder.fit_transform(self.df['nature'])\n",
        "        labels['vibe'] = self.vibe_encoder.fit_transform(self.df['vibe'])\n",
        "        labels['target'] = self.target_encoder.fit_transform(self.df['target'])\n",
        "\n",
        "        logger.info(\"âœ… í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\")\n",
        "        return X, labels\n",
        "\n",
        "    def _train_xgboost_models(self):\n",
        "        \"\"\"XGBoost ëª¨ë¸ë“¤ í›ˆë ¨ (ì•ˆì „í•œ ì²˜ë¦¬)\"\"\"\n",
        "        logger.info(\"ğŸ‹ï¸ XGBoost ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "\n",
        "        try:\n",
        "            X, y = self._prepare_training_data()\n",
        "\n",
        "            # ë°ì´í„° ê²€ì¦\n",
        "            if X.shape[0] < 2:\n",
        "                logger.warning(\"ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ 2ê°œ ì´ìƒì˜ ìƒ˜í”Œì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "                return\n",
        "\n",
        "            # Season ëª¨ë¸ (multi:softmax)\n",
        "            logger.info(\"ê³„ì ˆ ë¶„ë¥˜ê¸° í›ˆë ¨ ì¤‘...\")\n",
        "            n_classes = len(np.unique(y['season']))\n",
        "\n",
        "            if n_classes > 1:\n",
        "                self.season_model = XGBClassifier(\n",
        "                    objective='multi:softprob' if n_classes > 2 else 'binary:logistic',\n",
        "                    n_estimators=min(150, len(X) * 2),  # ë°ì´í„° í¬ê¸°ì— ë§ê²Œ ì¡°ì •\n",
        "                    max_depth=min(5, max(2, len(X) // 10)),\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=42,\n",
        "                    tree_method='hist',\n",
        "                    verbosity=0  # ë¡œê·¸ ì¶œë ¥ ìµœì†Œí™”\n",
        "                )\n",
        "                self.season_model.fit(X, y['season'])\n",
        "            else:\n",
        "                logger.warning(\"ê³„ì ˆ ë°ì´í„°ì— í´ë˜ìŠ¤ê°€ 1ê°œë¿ì…ë‹ˆë‹¤. ë”ë¯¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
        "                from sklearn.dummy import DummyClassifier\n",
        "                self.season_model = DummyClassifier(strategy='most_frequent')\n",
        "                self.season_model.fit(X, y['season'])\n",
        "\n",
        "            # ë‹¤ì¤‘ ë¼ë²¨ ëª¨ë¸ë“¤ (binary:logistic)\n",
        "            for label_name in ['nature', 'vibe', 'target']:\n",
        "                logger.info(f\"{label_name} ë¶„ë¥˜ê¸° í›ˆë ¨ ì¤‘...\")\n",
        "\n",
        "                # ë¼ë²¨ì´ ëª¨ë‘ 0ì¸ì§€ í™•ì¸\n",
        "                if y[label_name].sum() == 0:\n",
        "                    logger.warning(f\"{label_name} ë¼ë²¨ì´ ëª¨ë‘ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë”ë¯¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
        "                    from sklearn.dummy import DummyClassifier\n",
        "                    dummy_model = DummyClassifier(strategy='constant', constant=0)\n",
        "                    # MultiOutputClassifier í˜•íƒœë¡œ ë˜í•‘\n",
        "                    model = MultiOutputClassifier(dummy_model)\n",
        "                    model.fit(X, y[label_name])\n",
        "                else:\n",
        "                    base_model = XGBClassifier(\n",
        "                        objective='binary:logistic',\n",
        "                        n_estimators=min(150, len(X) * 2),\n",
        "                        max_depth=min(5, max(2, len(X) // 10)),\n",
        "                        learning_rate=0.1,\n",
        "                        random_state=42,\n",
        "                        tree_method='hist',\n",
        "                        verbosity=0\n",
        "                    )\n",
        "\n",
        "                    # MultiOutputClassifierë¡œ ë‹¤ì¤‘ ë¼ë²¨ ì²˜ë¦¬\n",
        "                    model = MultiOutputClassifier(base_model, n_jobs=1)  # n_jobs=1ë¡œ ì•ˆì •ì„± í–¥ìƒ\n",
        "                    model.fit(X, y[label_name])\n",
        "\n",
        "                setattr(self, f\"{label_name}_model\", model)\n",
        "\n",
        "            logger.info(\"âœ… ëª¨ë“  XGBoost ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"XGBoost ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}\")\n",
        "            # í´ë°±: ë”ë¯¸ ëª¨ë¸ë“¤ ìƒì„±\n",
        "            logger.warning(\"ë”ë¯¸ ëª¨ë¸ë“¤ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
        "            self._create_dummy_models()\n",
        "\n",
        "    def _create_dummy_models(self):\n",
        "        \"\"\"ë”ë¯¸ ëª¨ë¸ë“¤ ìƒì„± (í´ë°±ìš©)\"\"\"\n",
        "        from sklearn.dummy import DummyClassifier\n",
        "\n",
        "        X, y = self._prepare_training_data()\n",
        "\n",
        "        # Season ë”ë¯¸ ëª¨ë¸\n",
        "        self.season_model = DummyClassifier(strategy='most_frequent')\n",
        "        self.season_model.fit(X, y['season'])\n",
        "\n",
        "        # ë‹¤ì¤‘ ë¼ë²¨ ë”ë¯¸ ëª¨ë¸ë“¤\n",
        "        for label_name in ['nature', 'vibe', 'target']:\n",
        "            dummy_model = DummyClassifier(strategy='constant', constant=0)\n",
        "            model = MultiOutputClassifier(dummy_model)\n",
        "            model.fit(X, y[label_name])\n",
        "            setattr(self, f\"{label_name}_model\", model)\n",
        "\n",
        "    def _encode_user_input(self, user_input: Dict[str, Union[str, List[str]]]) -> np.ndarray:\n",
        "        \"\"\"ì‚¬ìš©ì ì…ë ¥ì„ SBERT ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ì•ˆì „í•œ ì²˜ë¦¬)\"\"\"\n",
        "        try:\n",
        "            # ì‚¬ìš©ì ì…ë ¥ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©\n",
        "            combined_text = f\"{user_input.get('season', '')} \"\n",
        "\n",
        "            for key in ['nature', 'vibe', 'target']:\n",
        "                if key in user_input:\n",
        "                    values = user_input[key]\n",
        "                    if isinstance(values, list):\n",
        "                        combined_text += ' '.join(str(v) for v in values) + ' '\n",
        "                    else:\n",
        "                        combined_text += str(values) + ' '\n",
        "\n",
        "            # í…ìŠ¤íŠ¸ ì •ë¦¬\n",
        "            combined_text = combined_text.strip()\n",
        "            if not combined_text:\n",
        "                combined_text = \"ê´€ê´‘ì§€\"\n",
        "\n",
        "            # ê¸¸ì´ ì œí•œ\n",
        "            if len(combined_text) > 500:\n",
        "                combined_text = combined_text[:500]\n",
        "\n",
        "            # SBERT ì„ë² ë”© ìƒì„±\n",
        "            user_embedding = self.sbert_model.encode(\n",
        "                [combined_text],\n",
        "                normalize_embeddings=True,\n",
        "                convert_to_tensor=False,\n",
        "                device='cpu'\n",
        "            )\n",
        "            return user_embedding[0]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ì‚¬ìš©ì ì…ë ¥ ì¸ì½”ë”© ì‹¤íŒ¨: {e}\")\n",
        "            # í´ë°±: ëœë¤ ì„ë² ë”©\n",
        "            embedding_dim = self.place_embeddings.shape[1] if self.place_embeddings is not None else 384\n",
        "            return np.random.rand(embedding_dim)\n",
        "\n",
        "    def _predict_categories(self, user_embedding: np.ndarray) -> Dict[str, Union[str, List[str]]]:\n",
        "        \"\"\"ì‚¬ìš©ì ì„ë² ë”©ìœ¼ë¡œë¶€í„° ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡ (ì•ˆì „í•œ ì²˜ë¦¬)\"\"\"\n",
        "        user_embedding_2d = user_embedding.reshape(1, -1)\n",
        "        predictions = {}\n",
        "\n",
        "        try:\n",
        "            # Season ì˜ˆì¸¡\n",
        "            if self.season_model is not None:\n",
        "                season_pred = self.season_model.predict(user_embedding_2d)[0]\n",
        "                predictions['season'] = self.season_encoder.inverse_transform([season_pred])[0]\n",
        "            else:\n",
        "                predictions['season'] = 'ì‚¬ê³„ì ˆ'\n",
        "\n",
        "            # ë‹¤ì¤‘ ë¼ë²¨ ì˜ˆì¸¡\n",
        "            for label_name in ['nature', 'vibe', 'target']:\n",
        "                model = getattr(self, f\"{label_name}_model\", None)\n",
        "                encoder = getattr(self, f\"{label_name}_encoder\", None)\n",
        "\n",
        "                if model is not None and encoder is not None:\n",
        "                    try:\n",
        "                        pred = model.predict(user_embedding_2d)\n",
        "                        predicted_labels = encoder.inverse_transform(pred)[0]\n",
        "                        predictions[label_name] = list(predicted_labels)\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"{label_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}\")\n",
        "                        predictions[label_name] = []\n",
        "                else:\n",
        "                    predictions[label_name] = []\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}\")\n",
        "            # ê¸°ë³¸ê°’ ë°˜í™˜\n",
        "            predictions = {\n",
        "                'season': 'ì‚¬ê³„ì ˆ',\n",
        "                'nature': [],\n",
        "                'vibe': [],\n",
        "                'target': []\n",
        "            }\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def _calculate_similarity_scores(self, user_embedding: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"ì‚¬ìš©ì ì„ë² ë”©ê³¼ ëª¨ë“  ê´€ê´‘ì§€ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì•ˆì „í•œ ì²˜ë¦¬)\"\"\"\n",
        "        try:\n",
        "            user_embedding_2d = user_embedding.reshape(1, -1)\n",
        "\n",
        "            # ì„ë² ë”© ì°¨ì› í™•ì¸ ë° ì¡°ì •\n",
        "            if user_embedding_2d.shape[1] != self.place_embeddings.shape[1]:\n",
        "                logger.warning(f\"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: {user_embedding_2d.shape[1]} vs {self.place_embeddings.shape[1]}\")\n",
        "                # ì°¨ì› ë§ì¶”ê¸°\n",
        "                min_dim = min(user_embedding_2d.shape[1], self.place_embeddings.shape[1])\n",
        "                user_embedding_2d = user_embedding_2d[:, :min_dim]\n",
        "                place_embeddings_adjusted = self.place_embeddings[:, :min_dim]\n",
        "            else:\n",
        "                place_embeddings_adjusted = self.place_embeddings\n",
        "\n",
        "            similarities = cosine_similarity(user_embedding_2d, place_embeddings_adjusted)[0]\n",
        "\n",
        "            # NaN ê°’ ì²˜ë¦¬\n",
        "            similarities = np.nan_to_num(similarities, nan=0.0)\n",
        "\n",
        "            return similarities\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
        "            # í´ë°±: ëœë¤ ìœ ì‚¬ë„\n",
        "            return np.random.rand(len(self.df)) * 0.1  # ë‚®ì€ ì ìˆ˜\n",
        "\n",
        "    def _calculate_tag_matching_scores(self, user_input: Dict, predicted_categories: Dict) -> np.ndarray:\n",
        "        \"\"\"íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (ì•ˆì „í•œ ì²˜ë¦¬)\"\"\"\n",
        "        scores = np.zeros(len(self.df))\n",
        "\n",
        "        try:\n",
        "            for idx, row in self.df.iterrows():\n",
        "                score = 0\n",
        "\n",
        "                # Season ë§¤ì¹­\n",
        "                user_season = user_input.get('season', '')\n",
        "                place_season = row.get('season', '')\n",
        "                if user_season and place_season and user_season == place_season:\n",
        "                    score += 1\n",
        "\n",
        "                # ë‹¤ì¤‘ ë¼ë²¨ ë§¤ì¹­ (Jaccard ìœ ì‚¬ë„)\n",
        "                for category in ['nature', 'vibe', 'target']:\n",
        "                    if category in user_input:\n",
        "                        user_tags = user_input[category]\n",
        "                        if isinstance(user_tags, str):\n",
        "                            user_tags = [user_tags]\n",
        "                        elif not isinstance(user_tags, list):\n",
        "                            user_tags = []\n",
        "\n",
        "                        user_tags = set(str(tag).strip() for tag in user_tags if tag)\n",
        "\n",
        "                        place_tags = row.get(category, [])\n",
        "                        if isinstance(place_tags, str):\n",
        "                            place_tags = [place_tags]\n",
        "                        elif not isinstance(place_tags, list):\n",
        "                            place_tags = []\n",
        "\n",
        "                        place_tags = set(str(tag).strip() for tag in place_tags if tag)\n",
        "\n",
        "                        if user_tags and place_tags:\n",
        "                            intersection = len(user_tags.intersection(place_tags))\n",
        "                            union = len(user_tags.union(place_tags))\n",
        "                            jaccard = intersection / union if union > 0 else 0\n",
        "                            score += jaccard\n",
        "\n",
        "                scores[idx] = score\n",
        "\n",
        "            # ì •ê·œí™” (0-1 ë²”ìœ„)\n",
        "            if scores.max() > 0:\n",
        "                scores = scores / scores.max()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
        "            # í´ë°±: ê· ë“±í•œ ì ìˆ˜\n",
        "            scores = np.ones(len(self.df)) * 0.5\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def recommend_places(self, user_input: Dict[str, Union[str, List[str]]],\n",
        "                        tfidf_vectorizer=None, season_model=None, nature_model=None,\n",
        "                        vibe_model=None, target_model=None, df=None, top_n: int = 3) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        ê¸°ì¡´ ì‹œê·¸ë‹ˆì²˜ì™€ ì™„ì „ í˜¸í™˜ë˜ëŠ” ì¶”ì²œ í•¨ìˆ˜\n",
        "\n",
        "        Args:\n",
        "            user_input: ì‚¬ìš©ì ì„ í˜¸ë„\n",
        "                - season: str (ì˜ˆ: \"ì—¬ë¦„\")\n",
        "                - nature: List[str] (ì˜ˆ: [\"ë°”ë‹¤\", \"ìì—°\"])\n",
        "                - vibe: List[str] (ì˜ˆ: [\"ê°ì„±\", \"ì‚°ì±…\"])\n",
        "                - target: List[str] (ì˜ˆ: [\"ì—°ì¸\"])\n",
        "            tfidf_vectorizer: í˜¸í™˜ì„±ìš© (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)\n",
        "            season_model: í˜¸í™˜ì„±ìš© (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)\n",
        "            nature_model: í˜¸í™˜ì„±ìš© (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)\n",
        "            vibe_model: í˜¸í™˜ì„±ìš© (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)\n",
        "            target_model: í˜¸í™˜ì„±ìš© (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)\n",
        "            df: í˜¸í™˜ì„±ìš© (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)\n",
        "            top_n: ì¶”ì²œí•  ê´€ê´‘ì§€ ìˆ˜\n",
        "\n",
        "        Returns:\n",
        "            ì¶”ì²œ ê´€ê´‘ì§€ DataFrame (name, city, description ì»¬ëŸ¼ í¬í•¨)\n",
        "        \"\"\"\n",
        "        logger.info(f\"ğŸ¯ {top_n}ê°œ ê´€ê´‘ì§€ ì¶”ì²œ ìƒì„± ì¤‘...\")\n",
        "\n",
        "        try:\n",
        "            # 1. ì‚¬ìš©ì ì…ë ¥ì„ SBERT ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜\n",
        "            user_embedding = self._encode_user_input(user_input)\n",
        "\n",
        "            # 2. ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡ (ì°¸ê³ ìš©)\n",
        "            predicted_categories = self._predict_categories(user_embedding)\n",
        "            logger.info(f\"ì˜ˆì¸¡ëœ ì¹´í…Œê³ ë¦¬: {predicted_categories}\")\n",
        "\n",
        "            # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "            similarity_scores = self._calculate_similarity_scores(user_embedding)\n",
        "\n",
        "            # 4. íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°\n",
        "            tag_scores = self._calculate_tag_matching_scores(user_input, predicted_categories)\n",
        "\n",
        "            # 5. ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )\n",
        "            final_scores = (\n",
        "                self.similarity_weight * similarity_scores +\n",
        "                self.tag_weight * tag_scores\n",
        "            )\n",
        "\n",
        "            # 6. ìƒìœ„ Nê°œ ì¶”ì²œ\n",
        "            top_indices = np.argsort(final_scores)[::-1][:top_n]\n",
        "\n",
        "            # 7. ê²°ê³¼ DataFrame ìƒì„± (ê¸°ì¡´ í¬ë§· í˜¸í™˜)\n",
        "            recommendations = self.df.iloc[top_indices].copy()\n",
        "\n",
        "            # ì¶”ì²œ ì ìˆ˜ ì¶”ê°€\n",
        "            recommendations['similarity_score'] = similarity_scores[top_indices]\n",
        "            recommendations['tag_score'] = tag_scores[top_indices]\n",
        "            recommendations['final_score'] = final_scores[top_indices]\n",
        "\n",
        "            # ê¸°ì¡´ í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (name, city, description)\n",
        "            # city ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ê°’ìœ¼ë¡œ ì¶”ê°€\n",
        "            if 'city' not in recommendations.columns:\n",
        "                recommendations['city'] = ''\n",
        "\n",
        "            # descriptionì´ ì—†ìœ¼ë©´ short_description ì‚¬ìš©\n",
        "            if 'description' not in recommendations.columns:\n",
        "                recommendations['description'] = recommendations['short_description']\n",
        "\n",
        "            result = recommendations[['name', 'city', 'description']].reset_index(drop=True)\n",
        "\n",
        "            logger.info(\"âœ… ì¶”ì²œ ìƒì„± ì™„ë£Œ!\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
        "            # í´ë°±: ì²« ë²ˆì§¸ Nê°œ ê´€ê´‘ì§€ ë°˜í™˜\n",
        "            fallback_df = self.df.head(top_n).copy()\n",
        "            if 'city' not in fallback_df.columns:\n",
        "                fallback_df['city'] = ''\n",
        "            if 'description' not in fallback_df.columns:\n",
        "                fallback_df['description'] = fallback_df['short_description']\n",
        "\n",
        "            return fallback_df[['name', 'city', 'description']].reset_index(drop=True)\n",
        "\n",
        "    def get_detailed_recommendations(self, user_input: Dict[str, Union[str, List[str]]],\n",
        "                                   top_n: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"ìƒì„¸ ì •ë³´ê°€ í¬í•¨ëœ ì¶”ì²œ ê²°ê³¼ ë°˜í™˜\"\"\"\n",
        "        logger.info(f\"ğŸ” ìƒì„¸ ì¶”ì²œ ì •ë³´ ìƒì„± ì¤‘...\")\n",
        "\n",
        "        user_embedding = self._encode_user_input(user_input)\n",
        "        predicted_categories = self._predict_categories(user_embedding)\n",
        "        similarity_scores = self._calculate_similarity_scores(user_embedding)\n",
        "        tag_scores = self._calculate_tag_matching_scores(user_input, predicted_categories)\n",
        "        final_scores = (\n",
        "            self.similarity_weight * similarity_scores +\n",
        "            self.tag_weight * tag_scores\n",
        "        )\n",
        "\n",
        "        top_indices = np.argsort(final_scores)[::-1][:top_n]\n",
        "        recommendations = self.df.iloc[top_indices].copy()\n",
        "\n",
        "        # ì ìˆ˜ ì •ë³´ ì¶”ê°€\n",
        "        recommendations['similarity_score'] = similarity_scores[top_indices]\n",
        "        recommendations['tag_score'] = tag_scores[top_indices]\n",
        "        recommendations['final_score'] = final_scores[top_indices]\n",
        "        recommendations['predicted_season'] = predicted_categories['season']\n",
        "        recommendations['predicted_nature'] = str(predicted_categories['nature'])\n",
        "        recommendations['predicted_vibe'] = str(predicted_categories['vibe'])\n",
        "        recommendations['predicted_target'] = str(predicted_categories['target'])\n",
        "\n",
        "        return recommendations.reset_index(drop=True)\n",
        "\n",
        "    def save_models(self, model_dir: str):\n",
        "        \"\"\"ëª¨ë¸ë“¤ì„ ì§€ì •ëœ ë””ë ‰í† ë¦¬ì— ì €ì¥\"\"\"\n",
        "        logger.info(f\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {model_dir}\")\n",
        "\n",
        "        os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "        # SBERT ì„ë² ë”© ì €ì¥\n",
        "        np.save(os.path.join(model_dir, 'place_embeddings.npy'), self.place_embeddings)\n",
        "\n",
        "        # XGBoost ëª¨ë¸ë“¤ ì €ì¥\n",
        "        models_to_save = {\n",
        "            'season_model': self.season_model,\n",
        "            'nature_model': self.nature_model,\n",
        "            'vibe_model': self.vibe_model,\n",
        "            'target_model': self.target_model\n",
        "        }\n",
        "\n",
        "        for name, model in models_to_save.items():\n",
        "            if model is not None:\n",
        "                joblib.dump(model, os.path.join(model_dir, f'{name}.joblib'))\n",
        "\n",
        "        # ì¸ì½”ë”ë“¤ ì €ì¥\n",
        "        encoders_to_save = {\n",
        "            'season_encoder': self.season_encoder,\n",
        "            'nature_encoder': self.nature_encoder,\n",
        "            'vibe_encoder': self.vibe_encoder,\n",
        "            'target_encoder': self.target_encoder\n",
        "        }\n",
        "\n",
        "        for name, encoder in encoders_to_save.items():\n",
        "            joblib.dump(encoder, os.path.join(model_dir, f'{name}.joblib'))\n",
        "\n",
        "        # ë°ì´í„°í”„ë ˆì„ê³¼ ì„¤ì • ì €ì¥\n",
        "        self.df.to_csv(os.path.join(model_dir, 'processed_data.csv'), index=False)\n",
        "\n",
        "        config = {\n",
        "            'model_name': self.model_name,\n",
        "            'similarity_weight': self.similarity_weight,\n",
        "            'tag_weight': self.tag_weight,\n",
        "            'data_path': self.data_path\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(model_dir, 'config.pickle'), 'wb') as f:\n",
        "            pickle.dump(config, f)\n",
        "\n",
        "        logger.info(\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
        "\n",
        "    def load_models(self, model_dir: str):\n",
        "        \"\"\"ì €ì¥ëœ ëª¨ë¸ë“¤ì„ ë¡œë“œ\"\"\"\n",
        "        logger.info(f\"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_dir}\")\n",
        "\n",
        "        # ì„¤ì • ë¡œë“œ\n",
        "        with open(os.path.join(model_dir, 'config.pickle'), 'rb') as f:\n",
        "            config = pickle.load(f)\n",
        "\n",
        "        self.model_name = config['model_name']\n",
        "        self.similarity_weight = config['similarity_weight']\n",
        "        self.tag_weight = config['tag_weight']\n",
        "\n",
        "        # ë°ì´í„° ë¡œë“œ\n",
        "        self.df = pd.read_csv(os.path.join(model_dir, 'processed_data.csv'))\n",
        "\n",
        "        # nature, vibe, target ì»¬ëŸ¼ì„ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
        "        for col in ['nature', 'vibe', 'target']:\n",
        "            self.df[col] = self.df[col].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "        # SBERT ëª¨ë¸ ë¡œë“œ\n",
        "        self.sbert_model = SentenceTransformer(self.model_name)\n",
        "\n",
        "        # ì„ë² ë”© ë¡œë“œ\n",
        "        self.place_embeddings = np.load(os.path.join(model_dir, 'place_embeddings.npy'))\n",
        "\n",
        "        # XGBoost ëª¨ë¸ë“¤ ë¡œë“œ\n",
        "        model_names = ['season_model', 'nature_model', 'vibe_model', 'target_model']\n",
        "        for name in model_names:\n",
        "            model_path = os.path.join(model_dir, f'{name}.joblib')\n",
        "            if os.path.exists(model_path):\n",
        "                setattr(self, name, joblib.load(model_path))\n",
        "\n",
        "        # ì¸ì½”ë”ë“¤ ë¡œë“œ\n",
        "        encoder_names = ['season_encoder', 'nature_encoder', 'vibe_encoder', 'target_encoder']\n",
        "        for name in encoder_names:\n",
        "            encoder_path = os.path.join(model_dir, f'{name}.joblib')\n",
        "            if os.path.exists(encoder_path):\n",
        "                setattr(self, name, joblib.load(encoder_path))\n",
        "\n",
        "        logger.info(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
        "\n",
        "    def evaluate_models(self) -> Dict[str, float]:\n",
        "        \"\"\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
        "        logger.info(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...\")\n",
        "\n",
        "        X = self.place_embeddings\n",
        "        results = {}\n",
        "\n",
        "        # Season ëª¨ë¸ í‰ê°€\n",
        "        y_season = self.season_encoder.transform(self.df['season'])\n",
        "        season_pred = self.season_model.predict(X)\n",
        "        results['season_accuracy'] = (y_season == season_pred).mean()\n",
        "\n",
        "        # ë‹¤ì¤‘ ë¼ë²¨ ëª¨ë¸ë“¤ í‰ê°€\n",
        "        for label_name in ['nature', 'vibe', 'target']:\n",
        "            encoder = getattr(self, f\"{label_name}_encoder\")\n",
        "            model = getattr(self, f\"{label_name}_model\")\n",
        "\n",
        "            y_true = encoder.transform(self.df[label_name])\n",
        "            y_pred = model.predict(X)\n",
        "\n",
        "            f1 = f1_score(y_true, y_pred, average='micro')\n",
        "            results[f'{label_name}_f1_score'] = f1\n",
        "\n",
        "        logger.info(\"í‰ê°€ ê²°ê³¼:\")\n",
        "        for metric, score in results.items():\n",
        "            logger.info(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "nge5ZfUNojdk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¸°ì¡´ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ì™€ ì™„ì „ í˜¸í™˜ë˜ëŠ” ë˜í¼ í•¨ìˆ˜\n",
        "def recommend_places(user_input: Dict[str, Union[str, List[str]]],\n",
        "                    tfidf_vectorizer=None, season_model=None, nature_model=None,\n",
        "                    vibe_model=None, target_model=None, df=None, top_n: int = 3,\n",
        "                    system: TourismRecommendationSystem = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    ê¸°ì¡´ ì½”ë“œì™€ ì™„ì „ í˜¸í™˜ë˜ëŠ” ì¶”ì²œ í•¨ìˆ˜\n",
        "\n",
        "    ìƒˆë¡œìš´ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ë ¤ë©´ system íŒŒë¼ë¯¸í„°ì— TourismRecommendationSystem ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì „ë‹¬\n",
        "    \"\"\"\n",
        "    if system is None:\n",
        "        raise ValueError(\"TourismRecommendationSystem ì¸ìŠ¤í„´ìŠ¤ë¥¼ system íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "    return system.recommend_places(\n",
        "        user_input=user_input,\n",
        "        tfidf_vectorizer=tfidf_vectorizer,\n",
        "        season_model=season_model,\n",
        "        nature_model=nature_model,\n",
        "        vibe_model=vibe_model,\n",
        "        target_model=target_model,\n",
        "        df=df,\n",
        "        top_n=top_n\n",
        "    )\n",
        "\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ ì½”ë“œ\n",
        "def demo_usage():\n",
        "    \"\"\"ì‚¬ìš© ì˜ˆì‹œ ë°ëª¨ (ì•ˆì „í•œ ì²˜ë¦¬)\"\"\"\n",
        "    print(\"ğŸŒŸ ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œ - SBERT + XGBoost ë²„ì „\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Google Drive ë§ˆìš´íŠ¸ ì‹œë„\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\")\n",
        "\n",
        "        # ì‚¬ìš©ì ì§€ì • CSV íŒŒì¼ ê²½ë¡œ\n",
        "        data_path = \"/content/drive/My Drive/ì¡¸ì—…ë…¼ë¬¸/gangwon_places_100.csv\"\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"â„¹ï¸ Google Colab í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤.\")\n",
        "        data_path = \"/content/drive/My Drive/ì¡¸ì—…ë…¼ë¬¸/gangwon_places_100.csv\"\n",
        "\n",
        "    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}\")\n",
        "        print(\"ğŸ’¡ ì˜¬ë°”ë¥¸ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "        return\n",
        "    else:\n",
        "        print(f\"âœ… ì‹¤ì œ ë°ì´í„° íŒŒì¼ ë°œê²¬: 100ê°œ ê´€ê´‘ì§€\")\n",
        "\n",
        "    try:\n",
        "        # 1. ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì‹¤ì œ 100ê°œ ê´€ê´‘ì§€ ë°ì´í„° ì‚¬ìš©)\n",
        "        print(f\"\\nğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ë°ì´í„°: {data_path})\")\n",
        "        system = TourismRecommendationSystem(data_path)\n",
        "\n",
        "        # 2. ì‚¬ìš©ì ì…ë ¥ ì˜ˆì‹œ\n",
        "        user_input = {\n",
        "            \"season\": \"ê²¨ìš¸\",\n",
        "            \"nature\": [\"ë°”ë‹¤\", \"ìì—°\"],\n",
        "            \"vibe\": [\"ê°ì„±\", \"ì‚°ì±…\"],\n",
        "            \"target\": [\"ì—°ì¸\"]\n",
        "        }\n",
        "\n",
        "        print(f\"\\nğŸ” ì‚¬ìš©ì ì„ í˜¸ë„: {user_input}\")\n",
        "\n",
        "        # 3. ê¸°ì¡´ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ë¡œ ì¶”ì²œ ì‹¤í–‰\n",
        "        print(\"\\nğŸ“‹ ê¸°ì¡´ ì‹œê·¸ë‹ˆì²˜ í˜¸í™˜ í…ŒìŠ¤íŠ¸:\")\n",
        "        recommendations = system.recommend_places(\n",
        "            user_input=user_input,\n",
        "            tfidf_vectorizer=None,  # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
        "            season_model=None,      # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
        "            nature_model=None,      # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
        "            vibe_model=None,        # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
        "            target_model=None,      # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
        "            df=None,               # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
        "            top_n=5  # ğŸ¯ ì¶”ì²œ ê°œìˆ˜: 5ê°œë¡œ ë³€ê²½\n",
        "        )\n",
        "\n",
        "        print(\"ğŸ† ì¶”ì²œ ê²°ê³¼:\")\n",
        "        print(recommendations.to_string(index=False))\n",
        "\n",
        "        # 4. ìƒì„¸ ì¶”ì²œ ê²°ê³¼\n",
        "        print(\"\\nğŸ“Š ìƒì„¸ ì¶”ì²œ ê²°ê³¼:\")\n",
        "        detailed = system.get_detailed_recommendations(user_input, top_n=5)  # ğŸ¯ ìƒì„¸ ì¶”ì²œë„ 5ê°œë¡œ ë³€ê²½\n",
        "        print(detailed[['name', 'season', 'final_score', 'similarity_score', 'tag_score']].to_string(index=False))\n",
        "\n",
        "        # 5. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
        "        print(\"\\nğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥:\")\n",
        "        performance = system.evaluate_models()\n",
        "\n",
        "        # 6. ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸\n",
        "        print(\"\\nğŸ’¾ ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸:\")\n",
        "        try:\n",
        "            system.save_models(\"saved_models\")\n",
        "            print(\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
        "\n",
        "            # ìƒˆ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
        "            print(\"ğŸ”„ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸...\")\n",
        "            new_system = TourismRecommendationSystem.__new__(TourismRecommendationSystem)\n",
        "            new_system.load_models(\"saved_models\")\n",
        "            print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
        "\n",
        "            # ë¡œë“œëœ ì‹œìŠ¤í…œìœ¼ë¡œ ì¶”ì²œ í…ŒìŠ¤íŠ¸\n",
        "            test_rec = new_system.recommend_places(user_input=user_input, top_n=5)  # ğŸ¯ ë¡œë“œ í…ŒìŠ¤íŠ¸ë„ 5ê°œë¡œ ë³€ê²½\n",
        "            print(\"âœ… ë¡œë“œëœ ëª¨ë¸ ì¶”ì²œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ëª¨ë¸ ì €ì¥/ë¡œë“œ ê²½ê³ : {e}\")\n",
        "\n",
        "        print(\"\\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
        "        print(\"ìƒì„¸ ì˜¤ë¥˜:\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_usage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktW3vTSCpdya",
        "outputId": "30cb6435-6608-4742-ac37-64a8a96a8f39"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒŸ ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œ - SBERT + XGBoost ë²„ì „\n",
            "============================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ\n",
            "âœ… ì‹¤ì œ ë°ì´í„° íŒŒì¼ ë°œê²¬: 100ê°œ ê´€ê´‘ì§€\n",
            "\n",
            "ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ë°ì´í„°: /content/drive/My Drive/ì¡¸ì—…ë…¼ë¬¸/gangwon_places_100.csv)\n",
            "\n",
            "ğŸ” ì‚¬ìš©ì ì„ í˜¸ë„: {'season': 'ê²¨ìš¸', 'nature': ['ë°”ë‹¤', 'ìì—°'], 'vibe': ['ê°ì„±', 'ì‚°ì±…'], 'target': ['ì—°ì¸']}\n",
            "\n",
            "ğŸ“‹ ê¸°ì¡´ ì‹œê·¸ë‹ˆì²˜ í˜¸í™˜ í…ŒìŠ¤íŠ¸:\n",
            "ğŸ† ì¶”ì²œ ê²°ê³¼:\n",
            "        name city                                                    description\n",
            "        ì„¤ì•…ì‚°ì±…               ì„¤ì•…ì‚°ì±…ì€(ëŠ”) ê²¨ìš¸ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° (ê°ì„±) ë¶„ìœ„ê¸°ë¡œ ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤.\n",
            "êµ¬ë´‰ì‚° ì „ë§ëŒ€ ì¹´í˜ê±°ë¦¬       êµ¬ë´‰ì‚° ì „ë§ëŒ€ ì¹´í˜ê±°ë¦¬ì€(ëŠ”) ê²¨ìš¸ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° (ê°ì„±) ë¶„ìœ„ê¸°ë¡œ ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤.\n",
            "   ì´ìŠ¹ë§Œë³„ì¥(ê³ ì„±)         ì´ìŠ¹ë§Œë³„ì¥(ê³ ì„±)ì€(ëŠ”) ê²¨ìš¸ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ í˜¸ìˆ˜ ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° (ê°ì„±) ë¶„ìœ„ê¸°ë¡œ ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤.\n",
            "ì‚¬ê·¼ì§„ í•´ì¤‘ê³µì› ì „ë§ëŒ€      ì‚¬ê·¼ì§„ í•´ì¤‘ê³µì› ì „ë§ëŒ€ì€(ëŠ”) ì—¬ë¦„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ë°”ë‹¤ ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° (ê°ì„±) ë¶„ìœ„ê¸°ë¡œ ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤.\n",
            "        ìˆœë‹´ê³„ê³¡               ìˆœë‹´ê³„ê³¡ì€(ëŠ”) ì—¬ë¦„ì— íŠ¹íˆ ì•„ë¦„ë‹¤ì›Œ ì‚° ê²½ê´€ì´ ë›°ì–´ë‚˜ë©° (ê°ì„±) ë¶„ìœ„ê¸°ë¡œ ì—°ì¸ì—ê²Œ ì¶”ì²œë©ë‹ˆë‹¤.\n",
            "\n",
            "ğŸ“Š ìƒì„¸ ì¶”ì²œ ê²°ê³¼:\n",
            "        name season  final_score  similarity_score  tag_score\n",
            "        ì„¤ì•…ì‚°ì±…     ê²¨ìš¸     0.853182          0.755304   1.000000\n",
            "êµ¬ë´‰ì‚° ì „ë§ëŒ€ ì¹´í˜ê±°ë¦¬     ê²¨ìš¸     0.832358          0.739115   0.972222\n",
            "   ì´ìŠ¹ë§Œë³„ì¥(ê³ ì„±)     ê²¨ìš¸     0.790047          0.691745   0.937500\n",
            "ì‚¬ê·¼ì§„ í•´ì¤‘ê³µì› ì „ë§ëŒ€     ì—¬ë¦„     0.717062          0.639547   0.833333\n",
            "        ìˆœë‹´ê³„ê³¡     ì—¬ë¦„     0.710684          0.652066   0.798611\n",
            "\n",
            "ğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥:\n",
            "\n",
            "ğŸ’¾ ëª¨ë¸ ì €ì¥ í…ŒìŠ¤íŠ¸:\n",
            "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\n",
            "ğŸ”„ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸...\n",
            "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\n",
            "âœ… ë¡œë“œëœ ëª¨ë¸ ì¶”ì²œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n",
            "\n",
            "ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cpk66qZXpfcg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}