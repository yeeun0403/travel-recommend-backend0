{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76b5130-236d-4f97-bee8-2c1e15b081bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: requests in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (80.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.53.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjdwl\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
      "WARNING:tensorflow:From C:\\Users\\tjdwl\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ìž„í¬íŠ¸\n",
    "## í•„ìš”í•œ ë¼ì´ë²„ëŸ¬ë¦¬ë“¤ì´ ì—†ëŠ” ê²½ìš° ì•„ëž˜ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜\n",
    "!pip install sentence-transformers xgboost scikit-learn pandas numpy joblib pyyaml tqdm\n",
    "!pip install tf-keras\n",
    "!pip install sentence-transformers\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "import joblib\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import xgboost as xgb\n",
    "\n",
    "## ë¡œê¹… ì„¤ì •\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c82b16f9-6612-417b-bccb-62dbbbcbfda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "   (í´ë”ê°€ ì—†ë‹¤ë©´ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”)\n"
     ]
    }
   ],
   "source": [
    "# í´ë”ê°€ ì´ë¯¸ ë§Œë“¤ì–´ì ¸ ìžˆë‹¤ë©´ ì•„ëž˜ ì½”ë“œëŠ” ì‹¤í–‰í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.\n",
    "# í•„ìš”ì‹œ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.\n",
    "\n",
    "# def create_project_structure():\n",
    "#     \"\"\"í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "#     \n",
    "#     directories = [\n",
    "#         'data/raw',\n",
    "#         'data/processed', \n",
    "#         'data/embeddings',\n",
    "#         'models/xgboost',\n",
    "#         'models/encoders',\n",
    "#         'src',\n",
    "#         'notebooks',\n",
    "#         'saved_models',\n",
    "#         'config',\n",
    "#         'logs'\n",
    "#     ]\n",
    "#     \n",
    "#     for directory in directories:\n",
    "#         Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "#     \n",
    "#     print(\"âœ… í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# create_project_structure()  # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ\n",
    "\n",
    "print(\"ðŸ“ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"   (í´ë”ê°€ ì—†ë‹¤ë©´ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ba1325-1757-4c23-95c2-0ae84b5f2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì„¤ì • íŒŒì¼ ìƒì„±\n",
    "\n",
    "config = {\n",
    "    'model' : {\n",
    "        'sbert_model' : 'snunlp/KR-SBERT-V40K-klueNLI-augSTS',\n",
    "        'embedding_dim' : 768,\n",
    "        'reduced_dim' : 128,\n",
    "        'dimensionality_reduction': 'PCA' ,\n",
    "        'xgboost_params' : {\n",
    "            'max_depth' : 6,\n",
    "            'learning_rate' : 0.1,\n",
    "            'n_estimators' : 100,\n",
    "            'random_state' : 42\n",
    "        }\n",
    "    },\n",
    "    'data' : {\n",
    "        'raw_file' : 'data/raw/gangwon_places_100.xlsx',\n",
    "        'processed_file' : 'data/processed/gangwon_places_100_processed.xlsx',\n",
    "        'embeddings_file' : 'data/embeddings/place_embeddings_pca128.npy'\n",
    "    },\n",
    "    'paths': {\n",
    "        'models' : 'models',\n",
    "        'encoders' : 'models/encoders',\n",
    "        'logs' : 'logs'\n",
    "    }\n",
    "}\n",
    "\n",
    "## config í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "os.makedirs('config', exist_ok=True)\n",
    "\n",
    "## ì„¤ì • íŒŒì¼ ì €ìž¥\n",
    "with open('config/config.yaml', 'w', encoding='utf-8') as f: \n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac4009e-fadd-425f-9ac0-d59c3da4c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì „ì²˜ë¦¬ í´ëž˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "## ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "class DataPreprocessor: \n",
    "    \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ í´ëž˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.season_encoder = None\n",
    "        self.nature_encoder = MultiLabelBinarizer()\n",
    "        self.vibe_encoder = MultiLabelBinarizer()\n",
    "        self.target_encoder = MultiLabelBinarizer()\n",
    "\n",
    "    def parse_multi_label_string(self, text: str) -> List[str]:\n",
    "        \"\"\"ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìžì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "\n",
    "        # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ê³µë°± ì œê±°\n",
    "        items = [item.strip() for item in str(text).split(',')]\n",
    "        return [item for item in items if item] # ë¹ˆ ë¬¸ìžì—´ ì œê±°\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame: \n",
    "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜\"\"\"\n",
    "        # ë³µì‚¬ë³¸ ìƒì„±\n",
    "        processed_df = df.copy()\n",
    "\n",
    "        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n",
    "        required_cols = ['name', 'season', 'nature', 'vibe', 'target', 'short_description']\n",
    "        missing_cols = [col for col in required_cols if col not in processed_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}\")\n",
    "\n",
    "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "        processed_df['short_description'] = processed_df['short_description'].fillna('')\n",
    "        processed_df['season'] = processed_df['season'].fillna('ì‚¬ê³„ì ˆ')\n",
    "        processed_df['nature'] = processed_df['nature'].fillna('')\n",
    "        processed_df['vibe'] = processed_df['vibe'].fillna('')\n",
    "        processed_df['target'] = processed_df['target'].fillna('')\n",
    "\n",
    "        # ë‹¤ì¤‘ ë¼ë²¨ íŒŒì‹±\n",
    "        processed_df['nature_list'] = processed_df['nature'].apply(self.parse_multi_label_string)\n",
    "        processed_df['vibe_list'] = processed_df['vibe'].apply(self.parse_multi_label_string)\n",
    "        processed_df['target_list'] = processed_df['target'].apply(self.parse_multi_label_string)\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ì •ê·œí™”\n",
    "        processed_df['short_description'] = processed_df['short_description'].apply(\n",
    "        lambda x: re.sub(r'[^\\w\\s]', '', str(x)) if pd.notna(x) else ''\n",
    "        )\n",
    "        return processed_df\n",
    "\n",
    "    def fit_encoders(self, df: pd.DataFrame):\n",
    "        \"\"\"ì¸ì½”ë”ë“¤ì„ í•™ìŠµ ë°ì´í„°ì— ë§žì¶¤\"\"\"\n",
    "\n",
    "        # ê³„ì ˆì€ ë‹¨ì¼ ë¼ë²¨ì´ë¯€ë¡œ LabelEncoder ëŒ€ì‹  ì§ì ‘ ì²˜ë¦¬\n",
    "        self.season_categories = sorted(df['season'].unique())\n",
    "\n",
    "        # ë‹¤ì¤‘ ë¼ë²¨ ì¸ì½”ë” í•™ìŠµ\n",
    "        self.nature_encoder.fit(df['nature_list'])\n",
    "        self.vibe_encoder.fit(df['vibe_list'])\n",
    "        self.target_encoder.fit(df['target_list'])\n",
    "\n",
    "        print(f\"ì¸ì½”ë” í•™ìŠµ ì™„ë£Œ\")\n",
    "        print(f\"   - ê³„ì ˆ ì¹´í…Œê³ ë¦¬: {self.season_categories}\")\n",
    "        print(f\"   - ìžì—°í™˜ê²½ ì¹´í…Œê³ ë¦¬: {len(self.nature_encoder.classes_)}ê°œ\")\n",
    "        print(f\"   - ë¶„ìœ„ê¸° ì¹´í…Œê³ ë¦¬: {len(self.vibe_encoder.classes_)}ê°œ\")\n",
    "        print(f\"   - ëŒ€ìƒ ì¹´í…Œê³ ë¦¬: {len(self.target_encoder.classes_)}ê°œ\")\n",
    "\n",
    "    def encode_labels(self, df: pd.DataFrame) -> Dict[str,np.ndarray]:\n",
    "        \"\"\"ë¼ë²¨ë“¤ì„ ì¸ì½”ë”©\"\"\"\n",
    "\n",
    "        # ê³„ì ˆ ì¸ì½”ë”©(ì›-í•« ì¸ì½”ë”©)\n",
    "        season_encoded = np.zeros((len(df), len(self.season_categories)))\n",
    "        for i, season in enumerate(df['season']):\n",
    "            if season in self.season_categories:\n",
    "                season_idx = self.season_categories.index(season)\n",
    "                season_encoded[i, season_idx] = 1\n",
    "\n",
    "        # ë‹¤ì¤‘ ë¼ë²¨ ì¸ì½”ë“±\n",
    "        nature_encoded = self.nature_encoder.transform(df['nature_list'])\n",
    "        vibe_encoded = self.vibe_encoder.transform(df['vibe_list'])\n",
    "        target_encoded = self.target_encoder.transform(df['target_list'])\n",
    "\n",
    "        return{\n",
    "            'season' : season_encoded,\n",
    "            'nature' : nature_encoded,\n",
    "            'vibe' : vibe_encoded,\n",
    "            'target' : target_encoded\n",
    "        }\n",
    "\n",
    "    def save_encoders(self, base_path: str):\n",
    "        \"\"\"ì¸ì½”ë”ë“¤ì„ ì €ìž¥\"\"\"\n",
    "        # ê³„ì ˆ ì¹´í…Œê³ ë¦¬ ì €ìž¥\n",
    "        joblib.dump(self.season_categories, f\"{base_path}/season_encoder.joblib\")\n",
    "\n",
    "        # ë‹¤ì¤‘ ë¼ë²¨ ì¸ì½”ë” ì €ìž¥\n",
    "        joblib.dump(self.nature_encoder, f\"{base_path}/nature_encoder.joblib\")\n",
    "        joblib.dump(self.vibe_encoder, f\"{base_path}/vibe_encoder.joblib\")\n",
    "        joblib.dump(self.target_encoder, f\"{base_path}/target_encoder.joblib\")\n",
    "\n",
    "        print(f\"ì¸ì½”ë” ì €ìž¥ ì™„ë£Œ: {base_path}\")\n",
    "\n",
    "    def load_encoders(self, base_path: str):\n",
    "        \"\"\"ì¸ì½”ë” ë¡œë“œ\"\"\"\n",
    "\n",
    "        self.season_categories = joblib.load(f\"{base_path}/season_encoder.joblib\")\n",
    "        self.nature_encoder = joblib.load(f\"{base_path}/nature_encoder.joblib\")\n",
    "        self.vibe_encoder = joblib.load(f\"{base_path}/vibe_encoder.joblib\")\n",
    "        self.target_encoder = joblib.load(f\"{base_path}/target_encoder.joblib\")\n",
    "\n",
    "        print(f\"ì¸ì½”ë” ë¡œë“œ ì™„ë£Œ: {base_path}\")\n",
    "\n",
    "print(f\"ë°ì´í„° ì „ì²˜ë¦¬ í´ëž˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
    "         \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e775a6e-6fdb-4c22-a1c7-024a7e48df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìž„ë² ë”© ìƒì„± í´ëž˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "## ìž„ë² ë”© ìƒì„± í´ëž˜ìŠ¤ ì •ì˜\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"SBERT ìž„ë² ë”© ìƒì„± ë° ì°¨ì› ì¶•ì†Œ í´ëž˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.dimension_reducer = None\n",
    "        self.reduced_dim = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"SBERT ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        print(f\"SBERT ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}\")\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        print(\"SBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ìž„ë² ë”© ìƒì„±\"\"\"\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "\n",
    "        print(f\"ìž„ë² ë”© ìƒì„± ì¤‘... (ì´ {len(texts)}ê°œ í…ìŠ¤íŠ¸)\")\n",
    "  \n",
    "        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìž„ë² ë”© ìƒì„±(ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)\n",
    "        batch_size = 32\n",
    "        embeddings = []\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch_texts, convert_to_numpy=True)\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        print(f\"ìž„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def fit_dimension_reducer(self, embeddings: np.ndarray, method: str = 'PCA',\n",
    "                              target_dim: int = 128):\n",
    "        \"\"\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "\n",
    "        self.reduced_dim = target_dim\n",
    "\n",
    "        if method =='PCA':\n",
    "            self.dimension_reducer = PCA(n_components=target_dim, random_state=42)\n",
    "        elif method =='TruncatedSVD':\n",
    "            self.dimension_reducer = TruncatedSVD(n_components=target_dim, random_state=42)\n",
    "        else: \n",
    "            raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì°¨ì› ì¶•ì†Œ ë°©ë²•: {method}\")\n",
    "\n",
    "        print(f\"{method}ë¥¼ ì‚¬ìš©í•˜ì—¬ {embeddings.shape[1]}ì°¨ì› -> {target_dim}ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ\")\n",
    "        self.dimension_reducer.fit(embeddings)\n",
    "\n",
    "        # ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ ì¶œë ¥(PCAì˜ ê²½ìš°)\n",
    "        if method =='PCA':\n",
    "            explained_variance_ratio = self.dimension_reducer.explained_variance_ratio_\n",
    "            cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "            print(f\"ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨: {cumulative_variance[-1]:.4f}\")\n",
    "\n",
    "        print(f\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "    def reduce_dimensions(self, embeddings: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ìž„ë² ë”© ì°¨ì› ì¶•ì†Œ\"\"\"\n",
    "\n",
    "        if self.dimension_reducer is None:\n",
    "            raise ValueError(\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        reduced_embeddings = self.dimension_reducer.transform(embeddings)\n",
    "        print(f\"ì°¨ì› ì¶•ì†Œ ì™„ë£Œ: {embeddings.shape} -> {reduced_embeddings.shape}\")\n",
    "\n",
    "        return reduced_embeddings\n",
    "\n",
    "    def save_dimension_reducer(self, filepath: str):\n",
    "        \"\"\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ ì €ìž¥\"\"\"\n",
    "\n",
    "        model_data = {\n",
    "        'reducer': self.dimension_reducer,\n",
    "        'reduced_dim' : self.reduced_dim,\n",
    "        'model_name' : self.model_name\n",
    "        }\n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ ì €ìž¥: {filepath}\")\n",
    "\n",
    "    def load_dimension_reducer(self, filepath: str):\n",
    "        \"\"\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "\n",
    "        model_data = joblib.load(filepath)\n",
    "        self.dimension_reducer = model_data['reducer']\n",
    "        self.reduced_dim = model_data['reduced_dim']\n",
    "        self.model_name = model_data['model_name']\n",
    "\n",
    "        print(f\"ì°¨ì› ì¶•ì†Œ ëª¨ë¸ ë¡œë“œ: {filepath}\")\n",
    "\n",
    "print(\"ìž„ë² ë”© ìƒì„± í´ëž˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232b2813-e02a-4589-9a3e-25c354b01830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost í•™ìŠµ í´ëž˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "### XGBoost í•™ìŠµ í´ëž˜ìŠ¤ ì •ì˜\n",
    "\n",
    "class XGBoostTrainer:\n",
    "    \"\"\"XGBoost ë¶„ë¥˜ê¸° í•™ìŠµ í´ëž˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self, xgb_params: Dict):\n",
    "        self.xgb_params = xgb_params\n",
    "        self.models = {}\n",
    "        self.label_types = ['season', 'nature', 'vibe', 'target']\n",
    "\n",
    "    def train_models(self, feature: np.ndarray, labels: Dict[str, np.ndarray]):\n",
    "        \"\"\"ëª¨ë“  ë¼ë²¨ íƒ€ìž…ì— ëŒ€í•´ ë¶„ë¥˜ê¸° í•™ìŠµ\"\"\"\n",
    "\n",
    "        print(\"XGBoost ëª¨ë¸ í•™ìŠµ ì‹œìž‘...\")\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            print(f\"\\n{label_type} ë¶„ë¥˜ê¸° í•™ìŠµ ì¤‘...\")\n",
    "\n",
    "            y = labels[label_type]\n",
    "\n",
    "            if label_type == 'season':\n",
    "                #ë‹¨ì¼ ë¼ë²¨: ì›-í•«ì—ì„œ í´ëž˜ìŠ¤ ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "                y_single = np.argmax(y, axis=1)\n",
    "\n",
    "                model = xgb.XGBClassifier(**self.xgb_params)\n",
    "                model.fit(features, y_single)\n",
    "                           \n",
    "            else: \n",
    "                #ë‹¤ì¤‘ ë¼ë²¨: OneVsRestClassifier ì‚¬ìš©\n",
    "                model = OneVsRestClassifier(\n",
    "                    xgb.XGBClassifier(**self.xgb_params)\n",
    "                )\n",
    "                model.fit(features, y)\n",
    "\n",
    "            self.models[label_type] = model\n",
    "            print(f\"ëª¨ë“  XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "    def evaluate_models(self, features: np.ndarray, labels: Dict[str,np.ndarray]):\n",
    "        \"\"\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
    "\n",
    "        print(\"\\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€===\")\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            print(f\"\\n[{label_type}] ì„±ëŠ¥ í‰ê°€: \")\n",
    "\n",
    "\n",
    "            y_true = labels[label_type]\n",
    "            model = self.models[label_type]\n",
    "\n",
    "            if label_type == 'season':\n",
    "                # ë‹¨ì¼ ë¼ë²¨ í‰ê°€\n",
    "\n",
    "                y_true_single = np.argmax(y_true, axis=1)\n",
    "                y_pred = model.predict(features)\n",
    "\n",
    "                accuracy = accuracy_score(y_true_single, y_pred)\n",
    "                f1 = f1_score(y_true_single, y_pred, average='weighted')\n",
    "\n",
    "                print(f\"Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "            else: \n",
    "                # ë‹¤ì¤‘ ë¼ë²¨ í‰ê°€\n",
    "                y_pred = model.predict(features)\n",
    "\n",
    "                accuracy = accuracy_score(y_true, y_pred)\n",
    "                f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "                f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "                print(f\"Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"F1-Score (Micro): {f1_micro:.4f}\")\n",
    "                print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
    "                \n",
    "    def save_models(self,base_path: str):\n",
    "        \"\"\"ëª¨ë¸ë“¤ ì €ìž¥\"\"\"\n",
    "        for label_type in self.label_types:\n",
    "            model_path = f\"{base_path}/xgboost/{label_type}_model.joblib\"\n",
    "            joblib.dump(self.models[label_type], model_path)\n",
    "            print(f\"{label_type} ëª¨ë¸ ì €ìž¥: {model_path}\")\n",
    "\n",
    "    def load_models(self,base_path: str):\n",
    "        \"\"\"ëª¨ë¸ë“¤ ë¡œë“œ\"\"\"\n",
    "\n",
    "        for label_type in self.label_types:\n",
    "            model_path = f\"{base_path}/xgboost/{label_type}_model.joblib\"\n",
    "            self.models[label_type] = joblib.load(model_path)\n",
    "            print(f\"{label_type} ëª¨ë¸ ë¡œë“œ: {model_path}\")\n",
    "\n",
    "print(\"XGBoost í•™ìŠµ í´ëž˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5e54007-c9a4-41e9-90c0-cb6ee76bd9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìˆ˜ì •ëœ ì¶”ì²œ ì‹œìŠ¤í…œ í´ëž˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "## ì¶”ì²œ ì‹œìŠ¤í…œ í´ëž˜ìŠ¤ ì •ì˜\n",
    "\n",
    "# ìƒˆë¡œìš´ ì…€ì—ì„œ GangwonPlaceRecommender í´ëž˜ìŠ¤ ìž¬ì •ì˜\n",
    "class GangwonPlaceRecommender:\n",
    "    \"\"\"ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œ ë©”ì¸ í´ëž˜ìŠ¤ (ìˆ˜ì •ëœ ë²„ì „)\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = 'config/config.yaml'):\n",
    "        # ì„¤ì • ë¡œë“œ\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        \n",
    "        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”\n",
    "        self.preprocessor = DataPreprocessor()\n",
    "        self.embedding_generator = EmbeddingGenerator(\n",
    "            self.config['model']['sbert_model']\n",
    "        )\n",
    "        self.xgb_trainer = XGBoostTrainer(\n",
    "            self.config['model']['xgboost_params']\n",
    "        )\n",
    "        \n",
    "        # ë°ì´í„° ì €ìž¥ìš©\n",
    "        self.df = None\n",
    "        self.place_embeddings = None\n",
    "        self.place_names = None\n",
    "        \n",
    "        # íƒœê·¸ ë§¤í•‘ (ìžìœ  ë¬¸ìž¥ íŒŒì‹±ìš©)\n",
    "        self.tag_mapping = {\n",
    "            'season': {\n",
    "                'ë´„': ['ë´„', '3ì›”', '4ì›”', '5ì›”', 'ë²šê½ƒ', 'ê½ƒ'],\n",
    "                'ì—¬ë¦„': ['ì—¬ë¦„', '6ì›”', '7ì›”', '8ì›”', 'ë°”ë‹¤', 'í•´ë³€', 'ì‹œì›', 'ë¬¼'],\n",
    "                'ê°€ì„': ['ê°€ì„', '9ì›”', '10ì›”', '11ì›”', 'ë‹¨í’', 'ì–µìƒˆ', 'ë¹¨ê°„'],\n",
    "                'ê²¨ìš¸': ['ê²¨ìš¸', '12ì›”', '1ì›”', '2ì›”', 'ëˆˆ', 'ìŠ¤í‚¤', 'ì¶”ìš´'],\n",
    "                'ì‚¬ê³„ì ˆ': ['ì‚¬ê³„ì ˆ', 'ì—°ì¤‘', 'ì–¸ì œë‚˜']\n",
    "            },\n",
    "            'nature': {\n",
    "                'ì‚°': ['ì‚°', 'ë“±ì‚°', 'íŠ¸ë ˆí‚¹', 'í•˜ì´í‚¹', 'ì‚°ì±…', 'ì˜¤ë¥´ë§‰'],\n",
    "                'ë°”ë‹¤': ['ë°”ë‹¤', 'í•´ë³€', 'ë°”ë‹·ê°€', 'ìˆ˜ì˜', 'íŒŒë„'],\n",
    "                'í˜¸ìˆ˜': ['í˜¸ìˆ˜', 'ì—°ëª»', 'ë¬¼ê°€', 'ì €ìˆ˜ì§€'],\n",
    "                'ê³„ê³¡': ['ê³„ê³¡', 'ì‹œëƒ‡ë¬¼', 'ê°œìš¸', 'ë¬¼ì†Œë¦¬'],\n",
    "                'ìžì—°': ['ìžì—°', 'ìˆ²', 'ë‚˜ë¬´', 'í’€', 'ì‹ë¬¼'],\n",
    "                'ë„ì‹œ': ['ë„ì‹œ', 'ì‹œë‚´', 'ë²ˆí™”ê°€', 'ìƒì ']\n",
    "            },\n",
    "            'vibe': {\n",
    "                'ê°ì„±': ['ê°ì„±', 'ê°ì„±ì ', 'ë¡œë§¨í‹±', 'ë‚­ë§Œ', 'ì˜ˆìœ'],\n",
    "                'í™œë ¥': ['í™œë ¥', 'í™œê¸°', 'ì‹ ë‚˜ëŠ”', 'ì¦ê±°ìš´', 'ìž¬ë¯¸'],\n",
    "                'íœ´ì‹': ['íœ´ì‹', 'ì‰¬ëŠ”', 'íŽ¸ì•ˆ', 'ì¡°ìš©', 'í‰ì˜¨', 'ížë§'],\n",
    "                'ì‚°ì±…': ['ì‚°ì±…', 'ê±·ê¸°', 'ê±°ë‹ê¸°', 'ì²œì²œížˆ'],\n",
    "                'ëª¨í—˜': ['ëª¨í—˜', 'ìŠ¤ë¦´', 'ë„ì „', 'ìµìŠ¤íŠ¸ë¦¼']\n",
    "            },\n",
    "            'target': {\n",
    "                'ì—°ì¸': ['ì—°ì¸', 'ì»¤í”Œ', 'ë‚¨ì¹œ', 'ì—¬ì¹œ', 'ì• ì¸'],\n",
    "                'ê°€ì¡±': ['ê°€ì¡±', 'ë¶€ëª¨', 'ì•„ì´', 'ìžë…€', 'ì•„ê¸°'],\n",
    "                'ì¹œêµ¬': ['ì¹œêµ¬', 'ì¹œêµ¬ë“¤', 'ë™ë£Œ', 'ê°™ì´'],\n",
    "                'í˜¼ìž': ['í˜¼ìž', 'ë‚˜ë§Œ', 'ë‹¨ë…', 'ì†”ë¡œ']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def parse_user_input(self, user_input: Dict) -> Dict:\n",
    "        \"\"\"ì‚¬ìš©ìž ìž…ë ¥ì„ íŒŒì‹±í•˜ì—¬ í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "        \n",
    "        parsed = {\n",
    "            'season': None,\n",
    "            'nature': [],\n",
    "            'vibe': [],\n",
    "            'target': []\n",
    "        }\n",
    "        \n",
    "        # ìžìœ  ë¬¸ìž¥ ìž…ë ¥ ì²˜ë¦¬\n",
    "        if 'free_text' in user_input:\n",
    "            text = user_input['free_text'].lower()\n",
    "            \n",
    "            # ê° íƒœê·¸ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë§¤ì¹­\n",
    "            for category, tag_dict in self.tag_mapping.items():\n",
    "                for tag, keywords in tag_dict.items():\n",
    "                    if any(keyword in text for keyword in keywords):\n",
    "                        if category == 'season':\n",
    "                            parsed['season'] = tag\n",
    "                        else:\n",
    "                            if tag not in parsed[category]:\n",
    "                                parsed[category].append(tag)\n",
    "        \n",
    "        # ì§ì ‘ íƒœê·¸ ìž…ë ¥ ì²˜ë¦¬\n",
    "        else:\n",
    "            if 'season' in user_input:\n",
    "                parsed['season'] = user_input['season']\n",
    "            \n",
    "            for category in ['nature', 'vibe', 'target']:\n",
    "                if category in user_input:\n",
    "                    if isinstance(user_input[category], list):\n",
    "                        parsed[category] = user_input[category]\n",
    "                    else:\n",
    "                        parsed[category] = [user_input[category]]\n",
    "        \n",
    "        return parsed\n",
    "    \n",
    "    def calculate_hybrid_score(self, user_input: Dict, \n",
    "                             similarity_weight: float = 0.6,\n",
    "                             tag_weight: float = 0.4) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"ìœ ì‚¬ë„ ì ìˆ˜ì™€ íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "        \n",
    "        # ì‚¬ìš©ìž ìž…ë ¥ íŒŒì‹±\n",
    "        parsed_input = self.parse_user_input(user_input)\n",
    "        \n",
    "        # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°\n",
    "        if 'free_text' in user_input:\n",
    "            query_text = user_input['free_text']\n",
    "        else:\n",
    "            # íƒœê·¸ë¥¼ ë¬¸ìž¥ìœ¼ë¡œ ë³€í™˜\n",
    "            query_parts = []\n",
    "            if parsed_input['season']:\n",
    "                query_parts.append(f\"{parsed_input['season']}ì—\")\n",
    "            if parsed_input['target']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['target'])}ì™€\")\n",
    "            if parsed_input['nature']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['nature'])}ì—ì„œ\")\n",
    "            if parsed_input['vibe']:\n",
    "                query_parts.append(f\"{', '.join(parsed_input['vibe'])} ì—¬í–‰\")\n",
    "            \n",
    "            query_text = ' '.join(query_parts)\n",
    "        \n",
    "        # ì¿¼ë¦¬ ìž„ë² ë”© ìƒì„±\n",
    "        if self.embedding_generator.model is None:\n",
    "            self.embedding_generator.load_model()\n",
    "        \n",
    "        query_embedding = self.embedding_generator.model.encode([query_text])\n",
    "        \n",
    "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° - [0] ì¸ë±ìŠ¤ë¡œ 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜\n",
    "        similarity_scores = cosine_similarity(\n",
    "            query_embedding, \n",
    "            self.place_embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # 2. íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°\n",
    "        tag_scores = np.zeros(len(self.df))\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            score = 0\n",
    "            total_weight = 0\n",
    "            \n",
    "            # ê³„ì ˆ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.3)\n",
    "            if parsed_input['season'] and row['season'] == parsed_input['season']:\n",
    "                score += 0.3\n",
    "            total_weight += 0.3\n",
    "            \n",
    "            # ìžì—°í™˜ê²½ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.25)\n",
    "            if parsed_input['nature']:\n",
    "                nature_match = len(set(parsed_input['nature']) & set(row['nature_list']))\n",
    "                if nature_match > 0:\n",
    "                    score += 0.25 * (nature_match / len(parsed_input['nature']))\n",
    "            total_weight += 0.25\n",
    "            \n",
    "            # ë¶„ìœ„ê¸° ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.25)\n",
    "            if parsed_input['vibe']:\n",
    "                vibe_match = len(set(parsed_input['vibe']) & set(row['vibe_list']))\n",
    "                if vibe_match > 0:\n",
    "                    score += 0.25 * (vibe_match / len(parsed_input['vibe']))\n",
    "            total_weight += 0.25\n",
    "            \n",
    "            # ëŒ€ìƒ ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.2)\n",
    "            if parsed_input['target']:\n",
    "                target_match = len(set(parsed_input['target']) & set(row['target_list']))\n",
    "                if target_match > 0:\n",
    "                    score += 0.2 * (target_match / len(parsed_input['target']))\n",
    "            total_weight += 0.2\n",
    "            \n",
    "            # ì •ê·œí™”\n",
    "            tag_scores[idx] = score / total_weight if total_weight > 0 else 0\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°\n",
    "        hybrid_scores = (similarity_weight * similarity_scores + \n",
    "                        tag_weight * tag_scores)\n",
    "        \n",
    "        return hybrid_scores, similarity_scores, tag_scores\n",
    "    \n",
    "    def recommend_places(self, user_input: Dict, top_k: int = 10) -> Dict:\n",
    "        \"\"\"ê´€ê´‘ì§€ ì¶”ì²œ ë©”ì¸ í•¨ìˆ˜\"\"\"\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°\n",
    "        hybrid_scores, similarity_scores, tag_scores = self.calculate_hybrid_score(user_input)\n",
    "        \n",
    "        # ìƒìœ„ kê°œ ì¶”ì²œì§€ ì„ íƒ\n",
    "        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "        \n",
    "        # ì¶”ì²œ ê²°ê³¼ êµ¬ì„±\n",
    "        recommendations = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            place_info = {\n",
    "                'name': self.df.iloc[idx]['name'],\n",
    "                'season': self.df.iloc[idx]['season'],\n",
    "                'nature': self.df.iloc[idx]['nature_list'],\n",
    "                'vibe': self.df.iloc[idx]['vibe_list'],\n",
    "                'target': self.df.iloc[idx]['target_list'],\n",
    "                'description': self.df.iloc[idx]['short_description'],\n",
    "                'hybrid_score': float(hybrid_scores[idx]),\n",
    "                'similarity_score': float(similarity_scores[idx]),\n",
    "                'tag_score': float(tag_scores[idx])\n",
    "            }\n",
    "            recommendations.append(place_info)\n",
    "        \n",
    "        # íŒŒì‹±ëœ ì‚¬ìš©ìž ìž…ë ¥ ì •ë³´ ì¶”ê°€\n",
    "        parsed_input = self.parse_user_input(user_input)\n",
    "        \n",
    "        result = {\n",
    "            'user_input': user_input,\n",
    "            'parsed_input': parsed_input,\n",
    "            'recommendations': recommendations,\n",
    "            'total_places': len(self.df)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"âœ… ìˆ˜ì •ëœ ì¶”ì²œ ì‹œìŠ¤í…œ í´ëž˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e88c2e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_PATHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124më°ì´í„° íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m df \u001b[38;5;241m=\u001b[39m load_data(DATA_PATHS)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Loaded data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows, columns=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DATA_PATHS' is not defined"
     ]
    }
   ],
   "source": [
    "# [PATCH] LOAD DATA (Excel/CSV fallback) + schema check\n",
    "import pandas as pd, os\n",
    "\n",
    "def load_data(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            if p.endswith(\".xlsx\"):\n",
    "                df = pd.read_excel(p)\n",
    "            else:\n",
    "                df = pd.read_csv(p)\n",
    "            # schema validation\n",
    "            missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "            assert not missing, f\"ìŠ¤í‚¤ë§ˆ ëˆ„ë½ ì»¬ëŸ¼: {missing}\"\n",
    "            for c in [\"name\",\"short_description\",\"address\",\"full_address\"]:\n",
    "                df[c] = df[c].fillna(\"\").astype(str)\n",
    "            return df\n",
    "    raise FileNotFoundError(\"ë°ì´í„° íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "df = load_data(DATA_PATHS)\n",
    "print(f\"âœ… Loaded data: {len(df):,} rows, columns={list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52475d3-0a3c-43c4-8849-7588f03fb598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "\n",
    "# # ì¶”ì²œ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "# recommender = GangwonPlaceRecommender()\n",
    "\n",
    "# # ì‹¤ì œ ë°ì´í„° ë¡œë“œ(ì—…ë¡œë“œëœ CSV íŒŒì¼ ì‚¬ìš©)\n",
    "# print(\"=== ì‹¤ì œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬===\")\n",
    "\n",
    "# # ì—…ë¡œë“œëœ íŒŒì¼ì„ data/rawë¡œ ë³µì‚¬ (íŒŒì¼ì´ í˜„ìž¬ ë””ë ‰í† ë¦¬ì— ìžˆëŠ” ê²½ìš°)\n",
    "# if os.path.exists('gangwon_places_100.xlsx'):\n",
    "#     import shutil\n",
    "#     shutil.copy('gangwon_places_100.csv', 'data/raw/gangwon_places_100.xlsx')\n",
    "#     print(\"âœ… ì—…ë¡œë“œëœ CSV íŒŒì¼ì„ data/rawë¡œ ë³µì‚¬ ì™„ë£Œ\")\n",
    "\n",
    "# # CSV íŒŒì¼ ë¡œë“œ\n",
    "# df = pd.read_csv('data/raw/gangwon_places_100.xlsx', encoding='utf-8')\n",
    "# print(f\"ì›ë³¸ ë°ì´í„°: {df.shape}\")\n",
    "# print(f\"ì»¬ëŸ¼: {df.columns.tolist()}\")\n",
    "\n",
    "# # ì¶”ê°€ ì»¬ëŸ¼ ì •ë³´ ì¶œë ¥\n",
    "# print(f\"\\n ì‹¤ì œ ë°ì´í„° ì •ë³´:\")\n",
    "# print(f\"ì´ ê´€ê´‘ì§€ ìˆ˜: {len(df)}\")\n",
    "# print(f\"ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}\")\n",
    "\n",
    "# # ê° ì¹´í…Œê³ ë¦¬ë³„ ê³ ìœ ê°’ í™•ì¸\n",
    "# categorical_columns = ['season', 'nature', 'vibe', 'target']\n",
    "# for col in categorical_columns:\n",
    "#     if col in df.columns:\n",
    "#         unique_values = df[col].dropna().unique()\n",
    "#         print(f\"-{col} ì¹´í…Œê³ ë¦¬: {len(unique_values)}ê°œ ì¢…ë¥˜\")\n",
    "#         print(f\" ì˜ˆì‹œ: {list(unique_values)[:5]}\")\n",
    "\n",
    "# # ë°ì´í„° ì „ì²˜ë¦¬\n",
    "# processed_df = recommender.preprocessor.preprocess_data(df)\n",
    "# print(f\"\\n ì „ì²˜ë¦¬ ëœ ë°ì´í„°: {processed_df.shape}\")\n",
    "\n",
    "# # ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸\n",
    "# print(f\"\\n ì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ (ìƒìœ„ 3ê°œ):\")\n",
    "# for idx,row in processed_df.head(3).iterrows():\n",
    "#     print(f\"\\n{idx+1}. {row['name']}\")\n",
    "#     print(f\"   ê³„ì ˆ: {row['season']}\")\n",
    "#     print(f\"   ìžì—°í™˜ê²½ (ë¦¬ìŠ¤íŠ¸): {row['nature_list']}\")\n",
    "#     print(f\"   ë¶„ìœ„ê¸° (ë¦¬ìŠ¤íŠ¸): {row['vibe_list']}\")\n",
    "#     print(f\"   ëŒ€ìƒ (ë¦¬ìŠ¤íŠ¸): {row['target_list']}\")\n",
    "#     print(f\"   ì„¤ëª…: {row['short_description'][:50]}...\")\n",
    "\n",
    "# # ì¸ì½”ë” í•™ìŠµ\n",
    "# recommender.preprocessor.fit_encoders(processed_df)\n",
    "\n",
    "# # ë¼ë²¨ ì¸ì½”ë”©\n",
    "# encoded_labels = recommender.preprocessor.encode_labels(processed_df)\n",
    "\n",
    "# # ì¸ì½”ë”© ê²°ê³¼ í™•ì¸\n",
    "# print(f\"\\n ì¸ì½”ë”© ê²°ê³¼:\") \n",
    "\n",
    "# # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ìž¥\n",
    "# processed_df.to_csv('data/processed/gangwon_places_100_processed.xlsx', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# # ì¶”ì²œ ì‹œìŠ¤í…œì— ë°ì´í„° ì €ìž¥\n",
    "# recommender.df = processed_df\n",
    "# recommender.place_names = processed_df['name'].tolist()\n",
    "\n",
    "## ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ì¶”ì²œ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "recommender = GangwonPlaceRecommender()\n",
    "\n",
    "print(\"=== ì‹¤ì œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ===\")\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Excel íŒŒì¼ ë¡œë“œ (data/raw/gangwon_places_100.xlsx)\n",
    "file_path = 'data/raw/gangwon_places_100.xlsx'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "\n",
    "print(f\"ðŸ“Š Excel íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}\")\n",
    "df = pd.read_excel(file_path)\n",
    "print(f\"âœ… Excel íŒŒì¼ ë¡œë“œ ì„±ê³µ!\")\n",
    "\n",
    "print(f\"\\nì›ë³¸ ë°ì´í„°: {df.shape}\")\n",
    "print(f\"ì»¬ëŸ¼: {df.columns.tolist()}\")\n",
    "\n",
    "# ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "print(f\"\\nðŸ“‹ ìƒ˜í”Œ ë°ì´í„° (ì²« 3í–‰):\")\n",
    "for idx, row in df.head(3).iterrows():\n",
    "    print(f\"\\n{idx+1}. {row['name']}\")\n",
    "    print(f\"   ê³„ì ˆ: {row['season']}\")\n",
    "    print(f\"   ìžì—°: {row['nature']}\")\n",
    "    print(f\"   ë¶„ìœ„ê¸°: {row['vibe']}\")\n",
    "    print(f\"   ëŒ€ìƒ: {row['target']}\")\n",
    "\n",
    "# ë°ì´í„° ì •ë³´\n",
    "print(f\"\\nðŸ“Š ì‹¤ì œ ë°ì´í„° ì •ë³´:\")\n",
    "print(f\"ì´ ê´€ê´‘ì§€ ìˆ˜: {len(df)}\")\n",
    "print(f\"ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}\")\n",
    "\n",
    "# ê° ì¹´í…Œê³ ë¦¬ë³„ ê³ ìœ ê°’ í™•ì¸\n",
    "categorical_columns = ['season', 'nature', 'vibe', 'target']\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        unique_values = df[col].dropna().unique()\n",
    "        print(f\"\\n- {col} ì¹´í…Œê³ ë¦¬: {len(unique_values)}ê°œ ì¢…ë¥˜\")\n",
    "        print(f\"  ì˜ˆì‹œ: {list(unique_values)[:5]}\")\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œìž‘\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "processed_df = recommender.preprocessor.preprocess_data(df)\n",
    "print(f\"\\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {processed_df.shape}\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸\n",
    "print(f\"\\nðŸ“‹ ì „ì²˜ë¦¬ ê²°ê³¼ ìƒ˜í”Œ (ìƒìœ„ 3ê°œ):\")\n",
    "for idx, row in processed_df.head(3).iterrows():\n",
    "    print(f\"\\n{idx+1}. {row['name']}\")\n",
    "    print(f\"   ê³„ì ˆ: {row['season']}\")\n",
    "    print(f\"   ìžì—°í™˜ê²½ (ë¦¬ìŠ¤íŠ¸): {row['nature_list']}\")\n",
    "    print(f\"   ë¶„ìœ„ê¸° (ë¦¬ìŠ¤íŠ¸): {row['vibe_list']}\")\n",
    "    print(f\"   ëŒ€ìƒ (ë¦¬ìŠ¤íŠ¸): {row['target_list']}\")\n",
    "    print(f\"   ì„¤ëª…: {row['short_description'][:50]}...\")\n",
    "\n",
    "# ì¸ì½”ë” í•™ìŠµ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ“ ì¸ì½”ë” í•™ìŠµ ì‹œìž‘\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommender.preprocessor.fit_encoders(processed_df)\n",
    "\n",
    "# ë¼ë²¨ ì¸ì½”ë”©\n",
    "print(\"\\nðŸ”¢ ë¼ë²¨ ì¸ì½”ë”© ì¤‘...\")\n",
    "encoded_labels = recommender.preprocessor.encode_labels(processed_df)\n",
    "\n",
    "# ì¸ì½”ë”© ê²°ê³¼ í™•ì¸\n",
    "print(f\"\\nâœ… ì¸ì½”ë”© ê²°ê³¼:\")\n",
    "for key, value in encoded_labels.items():\n",
    "    print(f\"   - {key}: {value.shape}\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ìž¥\n",
    "print(\"\\nðŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ìž¥ ì¤‘...\")\n",
    "processed_df.to_csv('data/processed/gangwon_places_100_processed.csv', \n",
    "                    index=False, \n",
    "                    encoding='utf-8-sig')\n",
    "print(\"âœ… ì €ìž¥ ì™„ë£Œ: data/processed/gangwon_places_100_processed.csv\")\n",
    "\n",
    "# ì¶”ì²œ ì‹œìŠ¤í…œì— ë°ì´í„° ì €ìž¥\n",
    "recommender.df = processed_df\n",
    "recommender.place_names = processed_df['name'].tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ… ì´ {len(recommender.df)}ê°œ ê´€ê´‘ì§€ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(f\"âœ… ë¡œë“œëœ íŒŒì¼: {file_path}\")\n",
    "print(f\"âœ… ì¸ì½”ë” í•™ìŠµ ì™„ë£Œ: {len(encoded_labels)}ê°œ ì¹´í…Œê³ ë¦¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ae69c-d465-4196-acac-111ee6df6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SBERT ìž„ë² ë”© ìƒì„±(768ì°¨ì› ìœ ì§€)\n",
    "print(\"\\n SBERT ìž„ë² ë”© ìƒì„± ë° ì°¨ì› ì¶•ì†Œ\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "texts = processed_df['short_description'].tolist()\n",
    "\n",
    "# SBERT ìž„ë² ë”© ìƒì„±\n",
    "embeddings = recommender.embedding_generator.generate_embeddings(texts)\n",
    "\n",
    "print(f\"ðŸ“Š ìž„ë² ë”© í˜•íƒœ: {embeddings.shape}\")\n",
    "print(f\"ðŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "\"\"\"\n",
    "# ì°¨ì› ì¶•ì†Œ ëª¨ë¸ í•™ìŠµ\n",
    "recommender.embedding_generator.fit_dimension_reducer(\n",
    "    embeddings,\n",
    "    method = recommender.config['model']['dimensionality_reduction'],\n",
    "    target_dim = recommender.config['model']['reduced_dim']\n",
    ")\n",
    "# ì°¨ì› ì¶•ì†Œ ì ìš©\n",
    "reduced_embeddings = recommender.embedding_generator.reduce_dimensions(embeddings)\n",
    "\"\"\"\n",
    "# ì°¨ì› ì¶•ì†Œ ì—†ì´ ì›ë³¸ 768ì°¨ì› ì‚¬ìš©\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "np.save('data/embeddings/place_embeddings_full768.npy', embeddings)\n",
    "\n",
    "# # ìž„ë² ë”© ì €ìž¥ \n",
    "# np.save('data/embeddings/place_embeddings_pca128.npy', reduced_embeddings)\n",
    "\n",
    "# ì¶”ì²œ ì‹œìŠ¤í…œì— ìž„ë² ë”© ì €ìž¥\n",
    "recommender.place_embeddings = embeddings\n",
    "\n",
    "\n",
    "print(\"âœ… 768ì°¨ì› ìž„ë² ë”© ìƒì„± ë° ì €ìž¥ ì™„ë£Œ\")\n",
    "print(f\"   íŒŒì¼ ì €ìž¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72f8fd-1306-4750-9edc-c8373bcfb6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGBoost ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
    "print(\"\\n === XGBoost ëª¨ë¸ í•™ìŠµ===\")\n",
    "\n",
    "# íŠ¹ì„±ê³¼ ë¼ë²¨ ì¤€ë¹„\n",
    "features = embeddings\n",
    "labels = encoded_labels\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "recommender.xgb_trainer.train_models(features, labels)\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€ \n",
    "recommender.xgb_trainer.evaluate_models(features, labels)\n",
    "\n",
    "print(\"\\n=== XGBoost ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e37bf-50e5-4faf-937b-0394617108ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ëª¨ë¸ ë° ì¸ì½”ë” ì €ìž¥\n",
    "print(\"\\n ëª¨ë¸ ë° ì¸ì½”ë” ì €ìž¥\")\n",
    "\n",
    "# í´ë” ìƒì„±\n",
    "os.makedirs('models/xgboost', exist_ok=True)\n",
    "os.makedirs('models/encoders', exist_ok=True)\n",
    "\n",
    "# ì¸ì½”ë” ì €ìž¥\n",
    "recommender.preprocessor.save_encoders('models/encoders')\n",
    "\n",
    "# XGBoost ëª¨ë¸ ì €ìž¥\n",
    "recommender.xgb_trainer.save_models('models')\n",
    "\n",
    "print(\" ëª¨ë“  ëª¨ë¸ ë° ì¸ì½”ë” ì €ìž¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0eba0b-7140-4112-af28-ae7eb2adb4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n=== ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: íƒœê·¸ ê¸°ë°˜ ìž…ë ¥\n",
    "test_input_1 = {\n",
    "    \"season\": \"ì—¬ë¦„\",\n",
    "    \"nature\": [\"ë°”ë‹¤\", \"ìžì—°\"],\n",
    "    \"vibe\": [\"íœ´ì‹\", \"ê°ì„±\"],\n",
    "    \"target\": [\"ì—°ì¸\"]\n",
    "}\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: íƒœê·¸ ê¸°ë°˜ ìž…ë ¥\")\n",
    "print(f\"ìž…ë ¥: {test_input_1}\")\n",
    "\n",
    "result_1 = recommender.recommend_places(test_input_1, top_k=5)\n",
    "\n",
    "print(f\"\\n íŒŒì‹±ëœ ìž…ë ¥: {result_1['parsed_input']}\")\n",
    "print(f\"ì´ {result_1['total_places']}ê°œ ê´€ê´‘ì§€ ì¤‘ ìƒìœ„ 5ê°œ ì¶”ì²œ:\")\n",
    "\n",
    "for i, place in enumerate(result_1['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ì„¤ëª…: {place['description']}\")\n",
    "    print(f\"   íƒœê·¸: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ={place['hybrid_score']:.4f}, ìœ ì‚¬ë„={place['similarity_score']:.4f}, íƒœê·¸={place['tag_score']:.4f}\")\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ìžìœ  ë¬¸ìž¥ ìž…ë ¥\n",
    "test_input_2 = {\n",
    "    \"fress_text\": \"ê²¨ìš¸ì— ê°€ì¡±ê³¼ í•¨ê»˜ ìŠ¤í‚¤ë¥¼ íƒ€ê³  ì‹¶ì–´ìš”\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" *50)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ìžìœ  ë¬¸ìž¥ ìž…ë ¥\")\n",
    "print(f\"ìž…ë ¥ : {test_input_2}\")\n",
    "\n",
    "result_2 = recommender.recommend_places(test_input_2, top_k=5)\n",
    "\n",
    "for i, place in enumerate(result_2['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ì„¤ëª…: {place['description']}\")\n",
    "    print(f\"   íƒœê·¸: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   ì ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ={place['hybrid_score']:.4f}, ìœ ì‚¬ë„={place['similarity_score']:.4f}, íƒœê·¸={place['tag_score']:.4f}\")\n",
    "\n",
    "print(\"\\n ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4911af44-7e05-479f-9303-c1566010e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ëª¨ë¸ ë¡œë“œ ë° ìž¬ì‚¬ìš© í…ŒìŠ¤íŠ¸\n",
    "print(f\"\\n === ëª¨ë¸ ë¡œë“œ ë° ìž¬ì‚¬ìš© í…ŒìŠ¤íŠ¸===\")\n",
    "# ìƒˆë¡œìš´ ì¶”ì²œ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìˆ˜ì •ëœ í´ëž˜ìŠ¤ ì‚¬ìš©)\n",
    "new_recommender = GangwonPlaceRecommender()\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "new_recommender.df = pd.read_csv('data/processed/gangwon_places_100_processed.csv')\n",
    "new_recommender.df = new_recommender.df.reset_index(drop=True)  # ì¸ë±ìŠ¤ ë¦¬ì…‹\n",
    "\n",
    "# ìž„ë² ë”© ë¡œë“œ\n",
    "new_recommender.place_embeddings = np.load('data/embeddings/place_embeddings_full768.npy')\n",
    "\n",
    "# ì¸ì½”ë” ë¡œë“œ\n",
    "new_recommender.preprocessor.load_encoders('models/encoders')\n",
    "\n",
    "# XGBoost ëª¨ë¸ ë¡œë“œ\n",
    "new_recommender.xgb_trainer.load_models('models')\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "test_input_3 = {\n",
    "    \"free_text\": \"ë´„ì— í˜¼ìž ì¡°ìš©í•œ ì‚°ì—ì„œ ížë§í•˜ê³  ì‹¶ì–´ìš”\"\n",
    "}\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: ìˆ˜ì •ëœ ëª¨ë¸ë¡œ ì¶”ì²œ\")\n",
    "print(f\"ìž…ë ¥: {test_input_3}\")\n",
    "\n",
    "result_3 = new_recommender.recommend_places(test_input_3, top_k=3)\n",
    "\n",
    "print(f\"\\níŒŒì‹±ëœ ìž…ë ¥: {result_3['parsed_input']}\")\n",
    "print(f\"ìƒìœ„ 3ê°œ ì¶”ì²œ:\")\n",
    "\n",
    "for i, place in enumerate(result_3['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ì„¤ëª…: {place['description']}\")\n",
    "    print(f\"   íƒœê·¸: {place['season']} | {place['nature']} | {place['vibe']} | {place['target']}\")\n",
    "    print(f\"   í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: {place['hybrid_score']:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… ìˆ˜ì •ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46069c-0d8d-4e6f-89ed-786967358f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš© ì¶”ì²œ í•¨ìˆ˜\n",
    "def simple_recommend_test(recommender, user_input, top_k=3):\n",
    "    \"\"\"ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš© ì¶”ì²œ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # íŒŒì‹±ëœ ìž…ë ¥\n",
    "    parsed_input = recommender.parse_user_input(user_input)\n",
    "    \n",
    "    # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    if 'free_text' in user_input:\n",
    "        query_text = user_input['free_text']\n",
    "    else:\n",
    "        query_parts = []\n",
    "        if parsed_input['season']:\n",
    "            query_parts.append(f\"{parsed_input['season']}ì—\")\n",
    "        if parsed_input['nature']:\n",
    "            query_parts.append(f\"{', '.join(parsed_input['nature'])}ì—ì„œ\")\n",
    "        if parsed_input['vibe']:\n",
    "            query_parts.append(f\"{', '.join(parsed_input['vibe'])} ì—¬í–‰\")\n",
    "        query_text = ' '.join(query_parts)\n",
    "    \n",
    "    # ì¿¼ë¦¬ ìž„ë² ë”© ìƒì„±\n",
    "    if recommender.embedding_generator.model is None:\n",
    "        recommender.embedding_generator.load_model()\n",
    "    \n",
    "    query_embedding = recommender.embedding_generator.model.encode([query_text])\n",
    "    \n",
    "    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarity_scores = cosine_similarity(query_embedding, recommender.place_embeddings)[0]\n",
    "    \n",
    "    # ìƒìœ„ ì¶”ì²œì§€ ì„ íƒ\n",
    "    top_indices = np.argsort(similarity_scores)[::-1][:top_k]\n",
    "    \n",
    "    # ê²°ê³¼ êµ¬ì„±\n",
    "    recommendations = []\n",
    "    for idx in top_indices:\n",
    "        place_info = {\n",
    "            'name': recommender.df.iloc[idx]['name'],\n",
    "            'description': recommender.df.iloc[idx]['short_description'],\n",
    "            'similarity_score': float(similarity_scores[idx])\n",
    "        }\n",
    "        recommendations.append(place_info)\n",
    "    \n",
    "    return {\n",
    "        'parsed_input': parsed_input,\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "test_input_3 = {\n",
    "    \"free_text\": \"ë´„ì— í˜¼ìž ì¡°ìš©í•œ ì‚°ì—ì„œ ížë§í•˜ê³  ì‹¶ì–´ìš”\"\n",
    "}\n",
    "\n",
    "print(\"=== ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ===\")\n",
    "print(f\"ìž…ë ¥: {test_input_3}\")\n",
    "\n",
    "result_3 = simple_recommend_test(new_recommender, test_input_3, top_k=3)\n",
    "\n",
    "print(f\"\\níŒŒì‹±ëœ ìž…ë ¥: {result_3['parsed_input']}\")\n",
    "print(f\"ìƒìœ„ 3ê°œ ì¶”ì²œ:\")\n",
    "\n",
    "for i, place in enumerate(result_3['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ì„¤ëª…: {place['description']}\")\n",
    "    print(f\"   ìœ ì‚¬ë„ ì ìˆ˜: {place['similarity_score']:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e897c-9917-4559-aa67-ec8c98367711",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flask API ì—°ë™ì„ ìœ„í•œ JSON ë³€í™˜ í•¨ìˆ˜\n",
    "\n",
    "def create_api_response(recommendation_result: Dict) -> Dict:\n",
    "    \"\"\"Flask API ì‘ë‹µì„ ìœ„í•œ JSON í˜•íƒœë¡œ ë³€í™˜\"\"\"\n",
    "\n",
    "    api_response = {\n",
    "        \n",
    "        'status': 'success',\n",
    "        'data' : {\n",
    "        'user_input': recommendation_result['user_input'],\n",
    "        'parsed_input': recommendation_result['parsed_input'],\n",
    "        'total_places': recommendation_result['total_places'],\n",
    "        'recommendations':[]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for place in recommendation_result['recommendations']:\n",
    "        place_data = {\n",
    "            'name': place['name'],\n",
    "            'description': place['description'],\n",
    "            'tags': {\n",
    "                'season': place['season'],\n",
    "                'nature': place['nature'],\n",
    "                'vibe': place['vibe'],\n",
    "                'target': place['target']\n",
    "            },\n",
    "            'scores' :{\n",
    "                'hybrid': round(place['hybrid_score'], 4),\n",
    "                'similarity': round(place['similarity_score'], 4),\n",
    "                'tag_match': round(place['tag_score'], 4)\n",
    "            }\n",
    "        }\n",
    "        api_response['data']['recommendations'].append(place_data)\n",
    "\n",
    "        return api_response\n",
    "\n",
    "print(\"Flask API ì—°ë™ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581073d5-364b-4cd7-839b-02231836a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì‚¬ìš©ìž ì •ì˜ ì¶”ì²œ í•¨ìˆ˜(Flask APIìš©)\n",
    "\n",
    "def recommend_places_api(user_input: Union[Dict, str], top_k: int = 10) -> Dict:\n",
    "    \"\"\"\n",
    "    Flask APIì—ì„œ ì‚¬ìš©í•  ì¶”ì²œ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        user_input: ì‚¬ìš©ìž ìž…ë ¥ (Dict ë˜ëŠ” JSON ë¬¸ìžì—´)\n",
    "        top_k: ì¶”ì²œí•  ê´€ê´‘ì§€ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "        API ì‘ë‹µ í˜•íƒœì˜ Dict\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ë¬¸ìžì—´ì¸ ê²½ìš° JSON íŒŒì‹±\n",
    "        if isinstance(user_input, str):\n",
    "            user_input = json.loads(user_input)\n",
    "\n",
    "        # ìž…ë ¥ ê²€ì¦\n",
    "        if not isinstance(user_input, dict):\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': 'ìž˜ëª»ëœ ìž…ë ¥ ë°©ì‹ìž…ë‹ˆë‹¤.',\n",
    "                'data': None\n",
    "            }\n",
    "        # ì¶”ì²œ ì‹¤í–‰\n",
    "        result = recommender.recommend_places(user_input, top_k= top_k)\n",
    "\n",
    "        # API ì‘ë‹µ ìƒì„±\n",
    "        api_response = create_api_response(result)\n",
    "\n",
    "        return api_response\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'message': f'ì¶”ì²œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',\n",
    "            'data': None\n",
    "        }\n",
    "# API í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\n=== API í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===\")\n",
    "\n",
    "# JSON ë¬¸ìžì—´ ìž…ë ¥ í…ŒìŠ¤íŠ¸\n",
    "json_input = '{\"free_text\": \"ì—¬ë¦„ì— ë°”ë‹¤ì—ì„œ ì„œí•‘í•˜ê³  ì‹¶ì–´ìš”\"}'\n",
    "api_result = recommend_places_api(json_input, top_k=3)\n",
    "\n",
    "print(\"JSON ë¬¸ìžì—´ ìž…ë ¥ í…ŒìŠ¤íŠ¸:\")\n",
    "print(f\"Status: {api_result['status']}\")\n",
    "if api_result['status'] == 'success':\n",
    "    print(f\"ì¶”ì²œ ê²°ê³¼: {len(api_result['data']['recommendations'])}ê°œ\")\n",
    "    for i, place in enumerate(api_result['data']['recommendations']):\n",
    "        print(f\"  {i+1}. {place['name']} (ì ìˆ˜: {place['scores']['hybrid']})\")\n",
    "\n",
    "print(\"\\n API í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17e0ce-fe1e-4adf-ab0c-0adfc3de8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "\n",
    "print(\"\\n=== ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤===\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: ë³µí•© íƒœê·¸ ìž…ë ¥\n",
    "test_input_4 = {\n",
    "    \"season\": \"ê°€ì„\",\n",
    "    \"nature\": [\"ì‚°\", \"ìžì—°\"],\n",
    "    \"vibe\": [\"ê°ì„±\", \"íœ´ì‹\"],\n",
    "    \"target\": [\"í˜¼ìž\"]\n",
    "}\n",
    "\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 4: ë³µí•© íƒœê·¸ ìž…ë ¥\")\n",
    "print(f\"ìž…ë ¥: {test_input_4}\")\n",
    "\n",
    "result_4 = recommender.recommend_places(test_input_4, top_k=3)\n",
    "\n",
    "print(f\"\\níŒŒì‹±ëœ ìž…ë ¥: {result_4['parsed_input']}\")\n",
    "print(f\"ìƒìœ„ 3ê°œ ì¶”ì²œ:\")\n",
    "\n",
    "for i, place in enumerate(result_4['recommendations']):\n",
    "    print(f\"\\n{i+1}. {place['name']}\")\n",
    "    print(f\"   ì„¤ëª…: {place['description'][:100]}...\")\n",
    "    print(f\"   ì ìˆ˜: {place['hybrid_score']:.4f}\")\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: ë‹¤ì–‘í•œ ìžìœ  ë¬¸ìž¥ ìž…ë ¥\n",
    "test_cases = [\n",
    "    \"ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜ ì‹ ë‚˜ëŠ” ì—¬ë¦„ íœ´ê°€ë¥¼ ë³´ë‚´ê³  ì‹¶ì–´ìš”\",\n",
    "    \"ì—°ì¸ê³¼ ë¡œë§¨í‹±í•œ ê°€ì„ ë°ì´íŠ¸ ìž¥ì†Œë¥¼ ì°¾ê³  ìžˆì–´ìš”\",\n",
    "    \"ê°€ì¡±ê³¼ í•¨ê»˜ ì•ˆì „í•˜ê³  êµìœ¡ì ì¸ ê³³ì„ ê°€ê³  ì‹¶ìŠµë‹ˆë‹¤\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 5: ë‹¤ì–‘í•œ ìžìœ  ë¬¸ìž¥ ìž…ë ¥\")\n",
    "\n",
    "for i, test_text in enumerate(test_cases):\n",
    "    print(f\"\\nðŸ“ í…ŒìŠ¤íŠ¸ {i+1}: {test_text}\")\n",
    "    \n",
    "    test_input = {\"free_text\": test_text}\n",
    "    result = recommender.recommend_places(test_input, top_k=2)\n",
    "    \n",
    "    print(f\"íŒŒì‹±ëœ ìž…ë ¥: {result['parsed_input']}\")\n",
    "    print(f\"ì¶”ì²œ ê²°ê³¼:\")\n",
    "    for j, place in enumerate(result['recommendations']):\n",
    "        print(f\"  {j+1}. {place['name']} (ì ìˆ˜: {place['hybrid_score']:.4f})\")\n",
    "\n",
    "print(\"\\nâœ… ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e763b5-818c-49f4-b574-20ae1d164340",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™”\n",
    "print(\"\\n === ì„±ëŠ¥ ë¶„ì„ ===\")\n",
    "\n",
    "#ì¶”ì²œ ì ìˆ˜ ë¶„í¬ ë¶„ì„\n",
    "def analyze_recommendation_scores():\n",
    "    \"\"\"ì¶”ì²œ ì ìˆ˜ ë¶„í¬ ë¶„ì„\"\"\"\n",
    "\n",
    "    # ìƒ˜í”Œ ë°ì´í„° ìž…ë ¥ë“¤\n",
    "    sample_inputs = [\n",
    "        {\"season\": \"ì—¬ë¦„\", \"nature\": [\"ë°”ë‹¤\"], \"vibe\": [\"íœ´ì‹\"], \"target\": [\"ì—°ì¸\"]},\n",
    "        {\"season\": \"ê²¨ìš¸\", \"nature\": [\"ì‚°\"], \"vibe\": [\"ëª¨í—˜\"], \"target\": [\"ì¹œêµ¬\"]},\n",
    "        {\"season\": \"ë´„\", \"nature\": [\"ìžì—°\"], \"vibe\": [\"ê°ì„±\"], \"target\": [\"í˜¼ìž\"]},\n",
    "        {\"free_text\": \"ê°€ì„ì— ë‹¨í’ ë³´ëŸ¬ ê°€ê³  ì‹¶ì–´ìš”\"},\n",
    "        {\"free_text\": \"ìŠ¤í‚¤ìž¥ì—ì„œ ìŠ¤ë¦´ ë„˜ì¹˜ëŠ” ê²¨ìš¸ì„ ë³´ë‚´ê³  ì‹¶ìŠµë‹ˆë‹¤\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ“Š ì¶”ì²œ ì ìˆ˜ ë¶„í¬ ë¶„ì„:\")\n",
    "\n",
    "    for i, test_input in enumerate(sample_inputs):\n",
    "        result = recommender.recommend_places(test_input, top_k=5)\n",
    "\n",
    "        hybrid_scores = [place['hybrid_score'] for place in result ['recommendations']]\n",
    "        similarity_scores = [place['similarity_score'] for place in result['recommendations']]\n",
    "        tag_scores = [place['tag_score'] for place in result['recommendations']]\n",
    "        \n",
    "        print(f\"\\ní…ŒìŠ¤íŠ¸ {i+1}: {test_input}\")\n",
    "        print(f\"  í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ë²”ìœ„: {min(hybrid_scores):.4f} ~ {max(hybrid_scores):.4f}\")\n",
    "        print(f\"  ìœ ì‚¬ë„ ì ìˆ˜ í‰ê· : {np.mean(similarity_scores):.4f}\")\n",
    "        print(f\"  íƒœê·¸ ë§¤ì¹­ ì ìˆ˜ í‰ê· : {np.mean(tag_scores):.4f}\")\n",
    "\n",
    "analyze_recommendation_scores()\n",
    "\n",
    "# ì‹œìŠ¤í…œ ì„±ëŠ¥ ì •ë³´\n",
    "print(f\"\\nðŸ”§ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì •ë³´:\")\n",
    "print(f\"- ì „ì²´ ê´€ê´‘ì§€ ìˆ˜: {len(recommender.df)}\")\n",
    "print(f\"- ìž„ë² ë”© ì°¨ì›: {recommender.place_embeddings.shape[1]}\")\n",
    "print(f\"- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {recommender.place_embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"- í•™ìŠµëœ ëª¨ë¸ ìˆ˜: {len(recommender.xgb_trainer.models)}\")\n",
    "\n",
    "print(\"\\nâœ… ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48edfebd-8839-4121-a71f-76235ffa8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ìµœì¢… ì •ë¦¬ ë° ì‚¬ìš©ë²• ì•ˆë‚´\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ‰ ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“ ìƒì„±ëœ íŒŒì¼ êµ¬ì¡°:\")\n",
    "print(\"\"\"\n",
    "project_root/\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ raw/gangwon_places_100.xlsx                 # ì›ë³¸ ë°ì´í„°\n",
    "â”‚   â”œâ”€â”€ processed/gangwon_places_100_processed.csv # ì „ì²˜ë¦¬ëœ ë°ì´í„°\n",
    "â”‚   â””â”€â”€ embeddings/place_embeddings_full768.npy    # 768ì°¨ì› ìž„ë² ë”©\n",
    "â”œâ”€â”€ models/\n",
    "â”‚   â”œâ”€â”€ xgboost/\n",
    "â”‚   â”‚   â”œâ”€â”€ season_model.joblib                    # ê³„ì ˆ ë¶„ë¥˜ ëª¨ë¸\n",
    "â”‚   â”‚   â”œâ”€â”€ nature_model.joblib                    # ìžì—°í™˜ê²½ ë¶„ë¥˜ ëª¨ë¸\n",
    "â”‚   â”‚   â”œâ”€â”€ vibe_model.joblib                      # ë¶„ìœ„ê¸° ë¶„ë¥˜ ëª¨ë¸\n",
    "â”‚   â”‚   â””â”€â”€ target_model.joblib                    # ëŒ€ìƒ ë¶„ë¥˜ ëª¨ë¸\n",
    "â”‚   â””â”€â”€ encoders/\n",
    "â”‚       â”œâ”€â”€ season_encoder.joblib                  # ê³„ì ˆ ì¸ì½”ë”\n",
    "â”‚       â”œâ”€â”€ nature_encoder.joblib                  # ìžì—°í™˜ê²½ ì¸ì½”ë”\n",
    "â”‚       â”œâ”€â”€ vibe_encoder.joblib                    # ë¶„ìœ„ê¸° ì¸ì½”ë”\n",
    "â”‚       â””â”€â”€ target_encoder.joblib                  # ëŒ€ìƒ ì¸ì½”ë”\n",
    "â””â”€â”€ config/config.yaml                             # ì„¤ì • íŒŒì¼\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸš€ ì‚¬ìš©ë²•:\")\n",
    "print(\"\"\"\n",
    "1. íƒœê·¸ ê¸°ë°˜ ì¶”ì²œ:\n",
    "   user_input = {\n",
    "       \"season\": \"ì—¬ë¦„\",\n",
    "       \"nature\": [\"ë°”ë‹¤\", \"ìžì—°\"],\n",
    "       \"vibe\": [\"ê°ì„±\", \"íœ´ì‹\"],\n",
    "       \"target\": [\"ì—°ì¸\"]\n",
    "   }\n",
    "   result = recommender.recommend_places(user_input, top_k=5)\n",
    "\n",
    "2. ìžìœ  ë¬¸ìž¥ ê¸°ë°˜ ì¶”ì²œ:\n",
    "   user_input = {\n",
    "       \"free_text\": \"ê²¨ìš¸ì— ê°€ì¡±ê³¼ í•¨ê»˜ ìŠ¤í‚¤ë¥¼ íƒ€ê³  ì‹¶ì–´ìš”\"\n",
    "   }\n",
    "   result = recommender.recommend_places(user_input, top_k=5)\n",
    "\n",
    "3. Flask API ì—°ë™:\n",
    "   api_response = recommend_places_api(user_input, top_k=10)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâš™ï¸ ì£¼ìš” ê¸°ëŠ¥:\")\n",
    "print(\"\"\"\n",
    "âœ… SBERT ê¸°ë°˜ í•œêµ­ì–´ ìž„ë² ë”© ìƒì„± (768ì°¨ì› ìœ ì§€)\n",
    "âœ… XGBoost ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ (season, nature, vibe, target)\n",
    "âœ… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ 60% + íƒœê·¸ 40%)\n",
    "âœ… ìžìœ  ë¬¸ìž¥ ìž…ë ¥ íŒŒì‹± ë° íƒœê·¸ ì¶”ì¶œ\n",
    "âœ… ëª¨ë¸ ë° ì¸ì½”ë” ì €ìž¥/ë¡œë“œ\n",
    "âœ… Flask API ì—°ë™ ì¤€ë¹„\n",
    "âœ… JSON ìž…ì¶œë ¥ ì§€ì›\n",
    "âœ… ì„±ëŠ¥ ë¶„ì„ ë„êµ¬\n",
    "âœ… ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì§€ì›\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸ“Š ì„±ëŠ¥ ì§€í‘œ:\")\n",
    "print(f\"- ë°ì´í„°: {len(recommender.df)}ê°œ ê´€ê´‘ì§€\")\n",
    "print(f\"- ìž„ë² ë”© ì°¨ì›: {recommender.place_embeddings.shape[1]}ì°¨ì›\")\n",
    "print(f\"- ëª¨ë¸ íƒ€ìž…: XGBoost (season: ë‹¨ì¼ë¼ë²¨, nature/vibe/target: ë‹¤ì¤‘ë¼ë²¨)\")\n",
    "print(f\"- ì¶”ì²œ ë°©ì‹: í•˜ì´ë¸Œë¦¬ë“œ (ìœ ì‚¬ë„ 60% + íƒœê·¸ 40%)\")\n",
    "print(f\"- ì§€ì› ìž…ë ¥: íƒœê·¸ ê¸°ë°˜ + ìžìœ  ë¬¸ìž¥ ìž…ë ¥\")\n",
    "\n",
    "print(\"\\nðŸ”„ ëª¨ë¸ ìž¬ì‚¬ìš©:\")\n",
    "print(\"\"\"\n",
    "# ì €ìž¥ëœ ëª¨ë¸ ë¡œë“œ\n",
    "new_recommender = GangwonPlaceRecommender()\n",
    "new_recommender.df = pd.read_csv('data/processed/gangwon_places_100_processed.xlsx')\n",
    "new_recommender.place_embeddings = np.load('data/embeddings/place_embeddings_full768.npy')\n",
    "new_recommender.preprocessor.load_encoders('models/encoders')\n",
    "new_recommender.xgb_trainer.load_models('models')\n",
    "\n",
    "# ì¶”ì²œ ì‹¤í–‰\n",
    "result = new_recommender.recommend_places(user_input, top_k=5)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸ’¡ ì¶”ê°€ í™œìš© ë°©ì•ˆ:\")\n",
    "print(\"\"\"\n",
    "1. ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì—°ë™:\n",
    "   - Flask/Django ë°±ì—”ë“œì— recommend_places_api() í•¨ìˆ˜ í™œìš©\n",
    "   - REST API ì—”ë“œí¬ì¸íŠ¸ êµ¬ì„±\n",
    "   - ì‹¤ì‹œê°„ ì¶”ì²œ ì„œë¹„ìŠ¤ ì œê³µ\n",
    "\n",
    "2. ëª¨ë°”ì¼ ì•± ì—°ë™:\n",
    "   - JSON í˜•íƒœì˜ API ì‘ë‹µ í™œìš©\n",
    "   - ì‚¬ìš©ìž ìž…ë ¥ íŒŒì‹± ê¸°ëŠ¥ í™œìš©\n",
    "   - ì˜¤í”„ë¼ì¸ ëª¨ë¸ ë°°í¬ ê°€ëŠ¥\n",
    "\n",
    "3. ì„±ëŠ¥ ìµœì í™”:\n",
    "   - ìž„ë² ë”© ìºì‹±ìœ¼ë¡œ ì‘ë‹µ ì†ë„ í–¥ìƒ\n",
    "   - ë°°ì¹˜ ì¶”ì²œ ì²˜ë¦¬\n",
    "   - ëª¨ë¸ ì••ì¶• ë° ê²½ëŸ‰í™”\n",
    "\n",
    "4. ê¸°ëŠ¥ í™•ìž¥:\n",
    "   - ì‚¬ìš©ìž í”¼ë“œë°± í•™ìŠµ\n",
    "   - í˜‘ì—… í•„í„°ë§ ì¶”ê°€\n",
    "   - ê°œì¸í™” ì¶”ì²œ êµ¬í˜„\n",
    "   - ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ\n",
    "   - ì§€ì—­ë³„ í•„í„°ë§ ê¸°ëŠ¥\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸ“ ì£¼ì˜ì‚¬í•­:\")\n",
    "print(\"\"\"\n",
    "- ì²« ì‹¤í–‰ ì‹œ SBERT ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤\n",
    "- GPU ì‚¬ìš© ì‹œ ë” ë¹ ë¥¸ ìž„ë² ë”© ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤\n",
    "- ì‹¤ì œ ì„œë¹„ìŠ¤ ë°°í¬ ì‹œ ë³´ì•ˆ ë° ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ê°•í™”í•˜ì„¸ìš”\n",
    "- ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œ ëª¨ë¸ ìž¬í•™ìŠµì´ í•„ìš”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤\n",
    "- ì¶”ì²œ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì •ê¸°ì ì¸ ëª¨ë¸ íŠœë‹ì„ ê¶Œìž¥í•©ë‹ˆë‹¤\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸŒŸ ì¶”ì²œ ì‹œìŠ¤í…œ íŠ¹ì§•:\")\n",
    "print(\"\"\"\n",
    "- í•œêµ­ì–´ íŠ¹í™” SBERT ëª¨ë¸ ì‚¬ìš© (snunlp/KR-SBERT-V40K-klueNLI-augSTS)\n",
    "- í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ (ì˜ë¯¸ì  ìœ ì‚¬ë„ + íƒœê·¸ ë§¤ì¹­)\n",
    "- ìžìœ  ë¬¸ìž¥ ìž…ë ¥ ì§€ì›ìœ¼ë¡œ ì‚¬ìš©ìž íŽ¸ì˜ì„± í–¥ìƒ\n",
    "- ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ë¡œ ì •í™•í•œ íƒœê·¸ ì˜ˆì¸¡\n",
    "- ëª¨ë¸ ì €ìž¥/ë¡œë“œ ê¸°ëŠ¥ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ìš´ì˜\n",
    "- Flask API ì—°ë™ìœ¼ë¡œ ì›¹ ì„œë¹„ìŠ¤ í™•ìž¥ ê°€ëŠ¥\n",
    "- ì„±ëŠ¥ ë¶„ì„ ë„êµ¬ë¡œ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ ì¶”ì²œ ì‹œìŠ¤í…œ ì„±ëŠ¥:\")\n",
    "print(\"\"\"\n",
    "- ìž„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (60% ê°€ì¤‘ì¹˜)\n",
    "- íƒœê·¸ ë§¤ì¹­ ê¸°ë°˜ ì •í™•ë„ í–¥ìƒ (40% ê°€ì¤‘ì¹˜)\n",
    "- ê³„ì ˆ, ìžì—°í™˜ê²½, ë¶„ìœ„ê¸°, ëŒ€ìƒë³„ ì„¸ë¶„í™”ëœ ì¶”ì²œ\n",
    "- ìžìœ  ë¬¸ìž¥ íŒŒì‹±ìœ¼ë¡œ ìžì—°ìŠ¤ëŸ¬ìš´ ì‚¬ìš©ìž ê²½í—˜\n",
    "- ìƒìœ„ Kê°œ ì¶”ì²œìœ¼ë¡œ ë‹¤ì–‘í•œ ì„ íƒì§€ ì œê³µ\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸš€ ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"   ì´ì œ ë‹¤ì–‘í•œ ì‚¬ìš©ìž ìž…ë ¥ì— ëŒ€í•´ ì •í™•í•œ ê´€ê´‘ì§€ ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "print(\"   Flask API ì—°ë™ì„ í†µí•´ ì›¹ ì„œë¹„ìŠ¤ë¡œ í™•ìž¥í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"   ëª¨ë“  ì˜¤ë¥˜ê°€ ìˆ˜ì •ë˜ì–´ ì•ˆì •ì ìœ¼ë¡œ ìž‘ë™í•©ë‹ˆë‹¤.\")\n",
    "print(\"   íƒœê·¸ ê¸°ë°˜ ì¶”ì²œê³¼ ìžìœ  ë¬¸ìž¥ ìž…ë ¥ì„ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“š ì¶”ê°€ í•™ìŠµ ìžë£Œ:\")\n",
    "print(\"\"\"\n",
    "- SBERT ëª¨ë¸ ìƒì„¸ ì •ë³´: https://huggingface.co/snunlp/KR-SBERT-V40K-klueNLI-augSTS\n",
    "- XGBoost ê³µì‹ ë¬¸ì„œ: https://xgboost.readthedocs.io/\n",
    "- Scikit-learn ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜: https://scikit-learn.org/stable/modules/multiclass.html\n",
    "- Flask API ê°œë°œ ê°€ì´ë“œ: https://flask.palletsprojects.com/\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸ”— ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"\"\"\n",
    "1. ì›¹ ì¸í„°íŽ˜ì´ìŠ¤ ê°œë°œ (HTML/CSS/JavaScript)\n",
    "2. Flask/Django ë°±ì—”ë“œ API êµ¬ì¶•\n",
    "3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (PostgreSQL/MySQL)\n",
    "4. ì‚¬ìš©ìž í”¼ë“œë°± ìˆ˜ì§‘ ì‹œìŠ¤í…œ\n",
    "5. ì¶”ì²œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ\n",
    "6. ëª¨ë°”ì¼ ì•± ì—°ë™\n",
    "7. ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâœ¨ ì™„ë£Œëœ ê¸°ëŠ¥ë“¤:\")\n",
    "print(\"\"\"\n",
    "âœ… ë°ì´í„° ì „ì²˜ë¦¬ ë° ì •ì œ\n",
    "âœ… SBERT ìž„ë² ë”© ìƒì„± (768ì°¨ì›)\n",
    "âœ… XGBoost ë‹¤ì¤‘ ë¼ë²¨ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n",
    "âœ… í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„\n",
    "âœ… ìžìœ  ë¬¸ìž¥ ìž…ë ¥ íŒŒì‹± ì‹œìŠ¤í…œ\n",
    "âœ… íƒœê·¸ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ\n",
    "âœ… ëª¨ë¸ ì €ìž¥/ë¡œë“œ ê¸°ëŠ¥\n",
    "âœ… Flask API ì—°ë™ ì¤€ë¹„\n",
    "âœ… ì„±ëŠ¥ ë¶„ì„ ë„êµ¬\n",
    "âœ… ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
    "âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë””ë²„ê¹…\n",
    "âœ… ì™„ì „í•œ ë¬¸ì„œí™”\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸŽŠ ì¶•í•˜í•©ë‹ˆë‹¤! ê°•ì›ë„ ê´€ê´‘ì§€ ì¶”ì²œ ì‹œìŠ¤í…œì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"ì´ì œ ì‹¤ì œ ì‚¬ìš©ìžë“¤ì—ê²Œ ì •í™•í•˜ê³  ìœ ìš©í•œ ê´€ê´‘ì§€ ì¶”ì²œì„ ì œê³µí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ================================\n",
    "# ë³´ë„ˆìŠ¤: ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì œ\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì œ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ì˜ˆì œ 1: ê°„ë‹¨í•œ ì¶”ì²œ\n",
    "print(\"\\nðŸ“ ì˜ˆì œ 1: ê°„ë‹¨í•œ ì¶”ì²œ\")\n",
    "simple_input = {\"free_text\": \"ë´„ì— ì‚°ì—ì„œ ížë§\"}\n",
    "simple_result = recommender.recommend_places(simple_input, top_k=3)\n",
    "print(f\"ìž…ë ¥: {simple_input['free_text']}\")\n",
    "print(\"ì¶”ì²œ ê²°ê³¼:\")\n",
    "for i, place in enumerate(simple_result['recommendations']):\n",
    "    print(f\"  {i+1}. {place['name']} (ì ìˆ˜: {place['hybrid_score']:.3f})\")\n",
    "\n",
    "# ì˜ˆì œ 2: íƒœê·¸ ì¡°í•© ì¶”ì²œ\n",
    "print(\"\\nðŸ“ ì˜ˆì œ 2: íƒœê·¸ ì¡°í•© ì¶”ì²œ\")\n",
    "tag_input = {\"season\": \"ì—¬ë¦„\", \"nature\": [\"ë°”ë‹¤\"], \"target\": [\"ê°€ì¡±\"]}\n",
    "tag_result = recommender.recommend_places(tag_input, top_k=3)\n",
    "print(f\"ìž…ë ¥: {tag_input}\")\n",
    "print(\"ì¶”ì²œ ê²°ê³¼:\")\n",
    "for i, place in enumerate(tag_result['recommendations']):\n",
    "    print(f\"  {i+1}. {place['name']} (ì ìˆ˜: {place['hybrid_score']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ë§ˆìŒê» ì‚¬ìš©í•˜ì„¸ìš”!\")\n",
    "print(\"=\"*60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea4517-59d3-4c5f-be74-c3a011ebe00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìž„í¬íŠ¸ ë° ì„¤ì •\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ìž„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295921da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PATCH] CONFIG for 1,000 items\n",
    "DATA_PATHS = [\n",
    "    \"/mnt/data/gangwon_places_1000.xlsx\",\n",
    "    \"/mnt/data/gangwon_places_1000.csv\",\n",
    "    \"/mnt/data/gangwon_places_100.xlsx\",\n",
    "    \"/mnt/data/gangwon_places_100.csv\",\n",
    "]\n",
    "EMB_CACHE_PATH = \"/mnt/data/embeddings_sbert_v1.parquet\"\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = \"cuda\"  # ì—†ìœ¼ë©´ \"cpu\"\n",
    "\n",
    "REQUIRED_COLS = [\"name\",\"season\",\"nature\",\"vibe\",\"target\",\"fee\",\"parking\",\n",
    "                 \"address\",\"open_time\",\"latitude\",\"longitude\",\"full_address\",\"short_description\"]\n",
    "\n",
    "print(\"âœ… CONFIG loaded (1000-ready)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0681300d-3041-4586-bcdb-18ed2b28cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ë°ì´í„° ë¡œë“œ (data/raw/gangwon_places_100.xlsx)\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Excel íŒŒì¼ ë¡œë“œ\n",
    "df = pd.read_excel('data/raw/gangwon_places_100.xlsx')\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}\")\n",
    "print(f\"ì»¬ëŸ¼: {df.columns.tolist()}\")\n",
    "print(f\"\\nìƒ˜í”Œ ë°ì´í„° (ì²« 3ê°œ):\")\n",
    "print(df.head(3)[['name', 'season', 'nature', 'vibe']])\n",
    "\n",
    "# ê¸°ë³¸ ì „ì²˜ë¦¬\n",
    "def preprocess_tags(value):\n",
    "    \"\"\"íƒœê·¸ ë¬¸ìžì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return []\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    return [item.strip() for item in str(value).split(',') if item.strip()]\n",
    "\n",
    "# íƒœê·¸ ì»¬ëŸ¼ ì „ì²˜ë¦¬\n",
    "for col in ['nature', 'vibe', 'target']:\n",
    "    df[col] = df[col].apply(preprocess_tags)\n",
    "\n",
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "df['short_description'] = df['short_description'].fillna('')\n",
    "df['season'] = df['season'].fillna('ì‚¬ê³„ì ˆ')\n",
    "\n",
    "print(f\"\\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "print(f\"Nature ìƒ˜í”Œ: {df['nature'].iloc[0]}\")\n",
    "print(f\"Vibe ìƒ˜í”Œ: {df['vibe'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c25cdf-d9b5-4be1-bbfd-d77c7d11b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì„¤ëª… í…ìŠ¤íŠ¸ ê°•í™”\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ”§ ë°ì´í„° ì¦ê°• ì¤‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class DataAugmenter:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì¦ê°• í´ëž˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def augment_description(self, row):\n",
    "        \"\"\"ì„¤ëª… í…ìŠ¤íŠ¸ì— íƒœê·¸ ì •ë³´ ì¶”ê°€\"\"\"\n",
    "        original = str(row['short_description'])\n",
    "        \n",
    "        # ê³„ì ˆ ì •ë³´ ì¶”ê°€\n",
    "        season_text = f\"ì´ê³³ì€ {row['season']}ì— íŠ¹ížˆ ì•„ë¦„ë‹µìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        # ìžì—°í™˜ê²½ ì •ë³´ ì¶”ê°€\n",
    "        if row['nature']:\n",
    "            nature_text = f\"{', '.join(row['nature'])} ê²½ê´€ì„ ì¦ê¸¸ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\"\n",
    "        else:\n",
    "            nature_text = \"\"\n",
    "        \n",
    "        # ë¶„ìœ„ê¸° ì •ë³´ ì¶”ê°€\n",
    "        if row['vibe']:\n",
    "            vibe_text = f\"{', '.join(row['vibe'])} ë¶„ìœ„ê¸°ë¡œ ì¢‹ìŠµë‹ˆë‹¤.\"\n",
    "        else:\n",
    "            vibe_text = \"\"\n",
    "        \n",
    "        # ëŒ€ìƒ ì •ë³´ ì¶”ê°€\n",
    "        if row['target']:\n",
    "            target_text = f\"{', '.join(row['target'])}ì—ê²Œ ì¶”ì²œí•©ë‹ˆë‹¤.\"\n",
    "        else:\n",
    "            target_text = \"\"\n",
    "        \n",
    "        # ëª¨ë“  ì •ë³´ ê²°í•©\n",
    "        augmented = f\"{original} {season_text} {nature_text} {vibe_text} {target_text}\"\n",
    "        \n",
    "        return augmented.strip()\n",
    "\n",
    "# ì¦ê°• ì ìš©\n",
    "augmenter = DataAugmenter()\n",
    "df['enhanced_description'] = df.apply(augmenter.augment_description, axis=1)\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ì¦ê°• ì™„ë£Œ!\")\n",
    "print(f\"\\nì›ë³¸ ì„¤ëª… ìƒ˜í”Œ:\")\n",
    "print(df['short_description'].iloc[0][:100] + \"...\")\n",
    "print(f\"\\nì¦ê°•ëœ ì„¤ëª… ìƒ˜í”Œ:\")\n",
    "print(df['enhanced_description'].iloc[0][:150] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2576874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PATCH] Multi-label targets (vibe/target)\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def split_labels(col):\n",
    "    return df[col].fillna(\"\").astype(str).apply(\n",
    "        lambda s: [t.strip() for t in s.replace(\"/\",\",\").split(\",\") if t.strip()]\n",
    "    )\n",
    "\n",
    "y_vibe_list   = split_labels(\"vibe\")\n",
    "y_target_list = split_labels(\"target\")\n",
    "\n",
    "mlb_vibe   = MultiLabelBinarizer().fit(y_vibe_list)\n",
    "mlb_target = MultiLabelBinarizer().fit(y_target_list)\n",
    "\n",
    "Y_vibe   = mlb_vibe.transform(y_vibe_list)\n",
    "Y_target = mlb_target.transform(y_target_list)\n",
    "\n",
    "import numpy as np\n",
    "Y = np.hstack([Y_vibe, Y_target])\n",
    "\n",
    "print(\"âœ… Labels prepared\")\n",
    "print(\"vibe classes:\", mlb_vibe.classes_)\n",
    "print(\"target classes:\", mlb_target.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PATCH] Train/Test split with multilabel stratification (if available)\n",
    "import numpy as np\n",
    "try:\n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    train_idx, test_idx = next(mskf.split(X, Y))\n",
    "    print(\"âœ… Split: MultilabelStratifiedKFold\")\n",
    "except Exception as e:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_idx, test_idx = train_test_split(np.arange(len(X)), test_size=0.2, random_state=42)\n",
    "    print(\"âš ï¸ Fallback: train_test_split (install iterstrat for better balance)\")\n",
    "\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PATCH] SBERT embedding with caching\n",
    "import torch, numpy as np, hashlib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL_NAME = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    "sbert = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "\n",
    "def _hash_texts(texts):\n",
    "    h = hashlib.sha256()\n",
    "    for t in texts:\n",
    "        h.update(t.encode(\"utf-8\"))\n",
    "    return h.hexdigest()[:16]\n",
    "\n",
    "texts = (df[\"name\"] + \" \" + df[\"short_description\"]).str.strip().tolist()\n",
    "data_hash = _hash_texts(texts)\n",
    "\n",
    "need_encode = True\n",
    "if os.path.exists(EMB_CACHE_PATH):\n",
    "    import pandas as pd\n",
    "    cache = pd.read_parquet(EMB_CACHE_PATH)\n",
    "    if {\"data_hash\",\"model\",\"dim\"}.issubset(cache.columns):\n",
    "        row = cache.iloc[0]\n",
    "        if row[\"data_hash\"] == data_hash and row[\"model\"] == MODEL_NAME:\n",
    "            X = np.stack(cache[\"embeddings\"].iloc[0])\n",
    "            need_encode = False\n",
    "            print(\"âœ… Loaded embeddings from cache:\", EMB_CACHE_PATH)\n",
    "\n",
    "if need_encode:\n",
    "    print(\"â³ Encoding with SBERT (first run, will cache)...\")\n",
    "    all_vecs = []\n",
    "    sbert.max_seq_length = 256\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), BATCH_SIZE):\n",
    "            batch = texts[i:i+BATCH_SIZE]\n",
    "            vecs = sbert.encode(batch, batch_size=BATCH_SIZE, convert_to_numpy=True, normalize_embeddings=True)\n",
    "            all_vecs.append(vecs)\n",
    "    X = np.vstack(all_vecs)\n",
    "    import pandas as pd\n",
    "    pd.DataFrame({\n",
    "        \"data_hash\":[data_hash],\n",
    "        \"model\":[MODEL_NAME],\n",
    "        \"dim\":[X.shape[1]],\n",
    "        \"embeddings\":[X],\n",
    "    }).to_parquet(EMB_CACHE_PATH, index=False)\n",
    "    print(\"âœ… Cached embeddings to:\", EMB_CACHE_PATH)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff4a53f-cb34-42df-a241-c665f4fbb107",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì¶”ê°€ í”¼ì²˜ ìƒì„±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ”¨ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¤‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í´ëž˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def create_statistical_features(self, df):\n",
    "        \"\"\"í†µê³„ì  í”¼ì²˜\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            # íƒœê·¸ ê°œìˆ˜\n",
    "            nature_count = len(row['nature'])\n",
    "            vibe_count = len(row['vibe'])\n",
    "            target_count = len(row['target'])\n",
    "            total_tags = nature_count + vibe_count + target_count\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ê¸¸ì´\n",
    "            desc_length = len(str(row['short_description']))\n",
    "            enhanced_length = len(str(row['enhanced_description']))\n",
    "            \n",
    "            # ê³ ìœ  ë‹¨ì–´ ìˆ˜\n",
    "            words = str(row['enhanced_description']).split()\n",
    "            unique_words = len(set(words))\n",
    "            \n",
    "            features.append([\n",
    "                nature_count,\n",
    "                vibe_count,\n",
    "                target_count,\n",
    "                total_tags,\n",
    "                desc_length,\n",
    "                enhanced_length,\n",
    "                unique_words,\n",
    "                enhanced_length / desc_length if desc_length > 0 else 1\n",
    "            ])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def create_tag_combination_features(self, df):\n",
    "        \"\"\"íƒœê·¸ ì¡°í•© í”¼ì²˜ (One-Hot)\"\"\"\n",
    "        # Nature + Vibe ì¡°í•©\n",
    "        combinations = []\n",
    "        for idx, row in df.iterrows():\n",
    "            combo = [f\"{n}_{v}\" for n in row['nature'] for v in row['vibe']]\n",
    "            combinations.append(combo if combo else ['ì—†ìŒ'])\n",
    "        \n",
    "        mlb = MultiLabelBinarizer()\n",
    "        combo_features = mlb.fit_transform(combinations)\n",
    "        \n",
    "        return combo_features\n",
    "    \n",
    "    def combine_all_features(self, embeddings, statistical, combinations):\n",
    "        \"\"\"ëª¨ë“  í”¼ì²˜ ê²°í•©\"\"\"\n",
    "        # PCAë¡œ ìž„ë² ë”© ì¶•ì†Œ (ì¶”ê°€ ì •ë³´ë¡œ)\n",
    "        pca = PCA(n_components=64)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "        \n",
    "        # ëª¨ë“  í”¼ì²˜ ê²°í•©\n",
    "        final_features = np.concatenate([\n",
    "            embeddings,           # ì›ë³¸ ìž„ë² ë”©\n",
    "            reduced_embeddings,   # ì¶•ì†Œ ìž„ë² ë”©\n",
    "            statistical,          # í†µê³„ í”¼ì²˜\n",
    "            combinations          # ì¡°í•© í”¼ì²˜\n",
    "        ], axis=1)\n",
    "        \n",
    "        print(f\"âœ… í”¼ì²˜ ê²°í•© ì™„ë£Œ!\")\n",
    "        print(f\"  - ì›ë³¸ ìž„ë² ë”©: {embeddings.shape[1]}ì°¨ì›\")\n",
    "        print(f\"  - ì¶•ì†Œ ìž„ë² ë”©: {reduced_embeddings.shape[1]}ì°¨ì›\")\n",
    "        print(f\"  - í†µê³„ í”¼ì²˜: {statistical.shape[1]}ì°¨ì›\")\n",
    "        print(f\"  - ì¡°í•© í”¼ì²˜: {combinations.shape[1]}ì°¨ì›\")\n",
    "        print(f\"  - ìµœì¢… í”¼ì²˜: {final_features.shape[1]}ì°¨ì›\")\n",
    "        \n",
    "        return final_features, pca\n",
    "\n",
    "# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰\n",
    "engineer = FeatureEngineer()\n",
    "\n",
    "statistical_features = engineer.create_statistical_features(df)\n",
    "print(f\"âœ… í†µê³„ í”¼ì²˜ ìƒì„±: {statistical_features.shape}\")\n",
    "\n",
    "combination_features = engineer.create_tag_combination_features(df)\n",
    "print(f\"âœ… ì¡°í•© í”¼ì²˜ ìƒì„±: {combination_features.shape}\")\n",
    "\n",
    "enhanced_features, pca_model = engineer.combine_all_features(\n",
    "    place_embeddings,\n",
    "    statistical_features,\n",
    "    combination_features\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ ìµœì¢… í”¼ì²˜ ì™„ì„±: {enhanced_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d39b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PATCH] Train One-vs-Rest XGBoost\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = OneVsRestClassifier(\n",
    "    XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.08,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        tree_method=\"hist\",  # GPUë©´ 'gpu_hist'\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"âœ… Model trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PATCH] Evaluation (Micro/Macro F1) + threshold control\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "proba = clf.predict_proba(X_test)\n",
    "threshold = 0.5\n",
    "Y_pred = (proba >= threshold).astype(int)\n",
    "\n",
    "micro = f1_score(Y_test, Y_pred, average=\"micro\")\n",
    "macro = f1_score(Y_test, Y_pred, average=\"macro\")\n",
    "\n",
    "print(f\"Micro-F1: {micro:.4f}\")\n",
    "print(f\"Macro-F1: {macro:.4f}\")\n",
    "\n",
    "# Optional: quick threshold sweep for best micro-F1\n",
    "best_t, best_m = threshold, micro\n",
    "for t in [0.35,0.4,0.45,0.5,0.55,0.6]:\n",
    "    Yp = (proba >= t).astype(int)\n",
    "    m  = f1_score(Y_test, Yp, average=\"micro\")\n",
    "    if m > best_m:\n",
    "        best_t, best_m = t, m\n",
    "print(f\"Best Micro-F1 in sweep: {best_m:.4f} @ threshold={best_t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf75fb-e75c-4914-bff9-31360a3cc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ê°œì„ ëœ ì¶”ì²œ ì‹œìŠ¤í…œ í´ëž˜ìŠ¤\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ ê°œì„ ëœ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶• ì¤‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class EnhancedRecommendationSystem:\n",
    "    \"\"\"ê°œì„ ëœ ì¶”ì²œ ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self, df, embeddings, models, encoders, \n",
    "                 sbert_model, pca_model, engineer):\n",
    "        self.df = df\n",
    "        self.place_embeddings = embeddings\n",
    "        self.season_model = models.get('season')\n",
    "        self.nature_model = models.get('nature')\n",
    "        self.vibe_model = models.get('vibe')\n",
    "        self.target_model = models.get('target')\n",
    "        self.season_encoder = encoders['season']\n",
    "        self.nature_encoder = encoders['nature']\n",
    "        self.vibe_encoder = encoders['vibe']\n",
    "        self.target_encoder = encoders['target']\n",
    "        self.sbert_model = sbert_model\n",
    "        self.pca_model = pca_model\n",
    "        self.engineer = engineer\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜\n",
    "        self.similarity_weight = 0.5\n",
    "        self.tag_weight = 0.3\n",
    "        self.predicted_weight = 0.2\n",
    "    \n",
    "    def encode_user_query(self, user_input: Dict) -> np.ndarray:\n",
    "        \"\"\"ì‚¬ìš©ìž ìž…ë ¥ì„ ìž„ë² ë”©ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "        # í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        text_parts = []\n",
    "        \n",
    "        if 'season' in user_input and user_input['season']:\n",
    "            text_parts.extend([user_input['season']] * 3)\n",
    "        \n",
    "        for key in ['nature', 'vibe', 'target']:\n",
    "            if key in user_input:\n",
    "                values = user_input[key]\n",
    "                if isinstance(values, list):\n",
    "                    text_parts.extend(values * 2)\n",
    "                else:\n",
    "                    text_parts.extend([values] * 2)\n",
    "        \n",
    "        query_text = ' '.join(text_parts) if text_parts else \"ê´€ê´‘ì§€\"\n",
    "        \n",
    "        # ìž„ë² ë”© ìƒì„±\n",
    "        query_embedding = self.sbert_model.encode(\n",
    "            [query_text],\n",
    "            normalize_embeddings=True\n",
    "        )[0]\n",
    "        \n",
    "        return query_embedding\n",
    "    \n",
    "    def calculate_advanced_scores(self, user_input: Dict, \n",
    "                                  user_embedding: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ê³ ê¸‰ ìŠ¤ì½”ì–´ë§\"\"\"\n",
    "        \n",
    "        # 1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
    "        similarity_scores = cosine_similarity(\n",
    "            user_embedding.reshape(1, -1),\n",
    "            self.place_embeddings[:, :len(user_embedding)]  # ìž„ë² ë”© ì°¨ì› ë§žì¶”ê¸°\n",
    "        )[0]\n",
    "        \n",
    "        # 2. íƒœê·¸ ë§¤ì¹­ ìŠ¤ì½”ì–´\n",
    "        tag_scores = self._calculate_tag_scores(user_input)\n",
    "        \n",
    "        # 3. ì˜ˆì¸¡ ê¸°ë°˜ ìŠ¤ì½”ì–´ (XGBoost)\n",
    "        predicted_scores = self._calculate_predicted_scores(user_embedding)\n",
    "        \n",
    "        # 4. ìµœì¢… ìŠ¤ì½”ì–´ (ê°€ì¤‘ í‰ê· )\n",
    "        final_scores = (\n",
    "            self.similarity_weight * similarity_scores +\n",
    "            self.tag_weight * tag_scores +\n",
    "            self.predicted_weight * predicted_scores\n",
    "        )\n",
    "        \n",
    "        return final_scores, similarity_scores, tag_scores, predicted_scores\n",
    "    \n",
    "    def _calculate_tag_scores(self, user_input: Dict) -> np.ndarray:\n",
    "        \"\"\"ê°œì„ ëœ íƒœê·¸ ë§¤ì¹­ ìŠ¤ì½”ì–´\"\"\"\n",
    "        scores = np.zeros(len(self.df))\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            score = 0.0\n",
    "            \n",
    "            # Season ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.3)\n",
    "            if user_input.get('season') == row['season']:\n",
    "                score += 0.3\n",
    "            \n",
    "            # Nature ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.25) - Jaccard + F1\n",
    "            if 'nature' in user_input and user_input['nature']:\n",
    "                user_nature = set(user_input['nature'] if isinstance(user_input['nature'], list) \n",
    "                                else [user_input['nature']])\n",
    "                place_nature = set(row['nature'])\n",
    "                \n",
    "                if user_nature and place_nature:\n",
    "                    intersection = len(user_nature & place_nature)\n",
    "                    union = len(user_nature | place_nature)\n",
    "                    jaccard = intersection / union if union > 0 else 0\n",
    "                    \n",
    "                    precision = intersection / len(place_nature) if place_nature else 0\n",
    "                    recall = intersection / len(user_nature) if user_nature else 0\n",
    "                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                    \n",
    "                    score += 0.25 * (0.6 * jaccard + 0.4 * f1)\n",
    "            \n",
    "            # Vibe ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.25)\n",
    "            if 'vibe' in user_input and user_input['vibe']:\n",
    "                user_vibe = set(user_input['vibe'] if isinstance(user_input['vibe'], list) \n",
    "                              else [user_input['vibe']])\n",
    "                place_vibe = set(row['vibe'])\n",
    "                \n",
    "                if user_vibe and place_vibe:\n",
    "                    intersection = len(user_vibe & place_vibe)\n",
    "                    union = len(user_vibe | place_vibe)\n",
    "                    jaccard = intersection / union if union > 0 else 0\n",
    "                    score += 0.25 * jaccard\n",
    "            \n",
    "            # Target ë§¤ì¹­ (ê°€ì¤‘ì¹˜ 0.2)\n",
    "            if 'target' in user_input and user_input['target']:\n",
    "                user_target = set(user_input['target'] if isinstance(user_input['target'], list) \n",
    "                                else [user_input['target']])\n",
    "                place_target = set(row['target'])\n",
    "                \n",
    "                if user_target and place_target:\n",
    "                    intersection = len(user_target & place_target)\n",
    "                    score += 0.2 * (intersection / len(user_target))\n",
    "            \n",
    "            scores[idx] = score\n",
    "        \n",
    "        # ì •ê·œí™”\n",
    "        if scores.max() > 0:\n",
    "            scores = scores / scores.max()\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def _calculate_predicted_scores(self, user_embedding: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"XGBoost ì˜ˆì¸¡ ê¸°ë°˜ ìŠ¤ì½”ì–´\"\"\"\n",
    "        # ê°„ë‹¨ížˆ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ë³µìž¡í•œ ë¡œì§ ê°€ëŠ¥)\n",
    "        return np.ones(len(self.df)) * 0.5\n",
    "    \n",
    "    def recommend(self, user_input: Dict, top_n: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"ì¶”ì²œ ì‹¤í–‰\"\"\"\n",
    "        print(f\"\\nðŸŽ¯ ì¶”ì²œ ìƒì„± ì¤‘...\")\n",
    "        print(f\"ì‚¬ìš©ìž ìž…ë ¥: {user_input}\")\n",
    "        \n",
    "        # ì‚¬ìš©ìž ì¿¼ë¦¬ ìž„ë² ë”©\n",
    "        user_embedding = self.encode_user_query(user_input)\n",
    "        \n",
    "        # ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "        final_scores, sim_scores, tag_scores, pred_scores = \\\n",
    "            self.calculate_advanced_scores(user_input, user_embedding)\n",
    "        \n",
    "        # ìƒìœ„ Nê°œ ì„ íƒ\n",
    "        top_indices = np.argsort(final_scores)[::-1][:top_n]\n",
    "        \n",
    "        # ê²°ê³¼ DataFrame ìƒì„±\n",
    "        recommendations = self.df.iloc[top_indices].copy()\n",
    "        recommendations['final_score'] = final_scores[top_indices]\n",
    "        recommendations['similarity_score'] = sim_scores[top_indices]\n",
    "        recommendations['tag_score'] = tag_scores[top_indices]\n",
    "        \n",
    "        print(f\"âœ… ì¶”ì²œ ì™„ë£Œ! ìƒìœ„ {top_n}ê°œ ì„ ì •\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "recommender = EnhancedRecommendationSystem(\n",
    "    df=df,\n",
    "    embeddings=enhanced_features,\n",
    "    models={'season': season_model, 'nature': models['nature'], \n",
    "            'vibe': models['vibe'], 'target': models['target']},\n",
    "    encoders={'season': season_encoder, 'nature': nature_encoder,\n",
    "              'vibe': vibe_encoder, 'target': target_encoder},\n",
    "    sbert_model=ensemble_embedder.primary_model,\n",
    "    pca_model=pca_model,\n",
    "    engineer=engineer\n",
    ")\n",
    "\n",
    "print(\"âœ… ê°œì„ ëœ ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209309e8-c4ae-4a91-9a98-f9143aacae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ê°œì„ ëœ ëª¨ë¸ ì €ìž¥\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ’¾ ëª¨ë¸ ì €ìž¥ ì¤‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# ì €ìž¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs('models/enhanced', exist_ok=True)\n",
    "\n",
    "# 1. ìž„ë² ë”© ì €ìž¥\n",
    "np.save('models/enhanced/enhanced_embeddings.npy', enhanced_features)\n",
    "print(\"âœ… ìž„ë² ë”© ì €ìž¥ ì™„ë£Œ\")\n",
    "\n",
    "# 2. XGBoost ëª¨ë¸ ì €ìž¥\n",
    "joblib.dump(season_model, 'models/enhanced/season_model.joblib')\n",
    "joblib.dump(models['nature'], 'models/enhanced/nature_model.joblib')\n",
    "joblib.dump(models['vibe'], 'models/enhanced/vibe_model.joblib')\n",
    "joblib.dump(models['target'], 'models/enhanced/target_model.joblib')\n",
    "print(\"âœ… XGBoost ëª¨ë¸ ì €ìž¥ ì™„ë£Œ\")\n",
    "\n",
    "# 3. ì¸ì½”ë” ì €ìž¥\n",
    "joblib.dump(season_encoder, 'models/enhanced/season_encoder.joblib')\n",
    "joblib.dump(nature_encoder, 'models/enhanced/nature_encoder.joblib')\n",
    "joblib.dump(vibe_encoder, 'models/enhanced/vibe_encoder.joblib')\n",
    "joblib.dump(target_encoder, 'models/enhanced/target_encoder.joblib')\n",
    "print(\"âœ… ì¸ì½”ë” ì €ìž¥ ì™„ë£Œ\")\n",
    "\n",
    "# 4. PCA ëª¨ë¸ ì €ìž¥\n",
    "joblib.dump(pca_model, 'models/enhanced/pca_model.joblib')\n",
    "print(\"âœ… PCA ëª¨ë¸ ì €ìž¥ ì™„ë£Œ\")\n",
    "\n",
    "# 5. ë°ì´í„°í”„ë ˆìž„ ì €ìž¥\n",
    "df.to_csv('models/enhanced/processed_data.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… ë°ì´í„° ì €ìž¥ ì™„ë£Œ\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ ëª¨ë“  ëª¨ë¸ ì €ìž¥ ì™„ë£Œ!\")\n",
    "print(f\"ì €ìž¥ ìœ„ì¹˜: models/enhanced/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10534e49-f06c-4e01-b2f8-db777d97cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ì¢…í•© ë¹„êµ ì‹œìŠ¤í…œ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ”¬ ì¶”ì²œ ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51275afe-65ff-4e64-b4d5-cffcd4c5c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ê¸°ë³¸ ì¶”ì²œ ì‹œìŠ¤í…œ - ê°œì„  ì „\n",
    "class BasicRecommendationSystem:\n",
    "    \"\"\"ê¸°ë³¸ ì¶”ì²œ ì‹œìŠ¤í…œ (ë¹„êµìš©)\"\"\"\n",
    "    \n",
    "    def __init__(self, df, sbert_model):\n",
    "        self.df = df\n",
    "        self.sbert_model = sbert_model\n",
    "        \n",
    "        # ê¸°ë³¸ ìž„ë² ë”© ìƒì„± (ë‹¨ìˆœ)\n",
    "        descriptions = df['short_description'].fillna('').astype(str).tolist()\n",
    "        print(\"ðŸ“ ê¸°ë³¸ ìž„ë² ë”© ìƒì„± ì¤‘...\")\n",
    "        self.place_embeddings = sbert_model.encode(\n",
    "            descriptions,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        print(f\"âœ… ê¸°ë³¸ ìž„ë² ë”© ì™„ë£Œ: {self.place_embeddings.shape}\")\n",
    "    \n",
    "    def recommend(self, user_input: Dict, top_n: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"ê¸°ë³¸ ì¶”ì²œ (ë‹¨ìˆœ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë§Œ ì‚¬ìš©)\"\"\"\n",
    "        \n",
    "        # ì‚¬ìš©ìž ì¿¼ë¦¬ ìƒì„±\n",
    "        query_parts = []\n",
    "        if 'season' in user_input:\n",
    "            query_parts.append(user_input['season'])\n",
    "        for key in ['nature', 'vibe', 'target']:\n",
    "            if key in user_input:\n",
    "                values = user_input[key]\n",
    "                if isinstance(values, list):\n",
    "                    query_parts.extend(values)\n",
    "                else:\n",
    "                    query_parts.append(values)\n",
    "        \n",
    "        query_text = ' '.join(query_parts) if query_parts else \"ê´€ê´‘ì§€\"\n",
    "        \n",
    "        # ì¿¼ë¦¬ ìž„ë² ë”©\n",
    "        query_embedding = self.sbert_model.encode(\n",
    "            [query_text],\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        \n",
    "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë§Œ ì‚¬ìš©\n",
    "        similarities = cosine_similarity(query_embedding, self.place_embeddings)[0]\n",
    "        \n",
    "        # ìƒìœ„ Nê°œ ì„ íƒ\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "        recommendations = self.df.iloc[top_indices].copy()\n",
    "        recommendations['score'] = similarities[top_indices]\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "print(\"\\nðŸ“¦ ê¸°ë³¸ ì¶”ì²œ ì‹œìŠ¤í…œ ì¤€ë¹„ ì¤‘...\")\n",
    "basic_system = BasicRecommendationSystem(df, ensemble_embedder.primary_model)\n",
    "print(\"âœ… ê¸°ë³¸ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94203fdd-f48a-4856-9176-77ef373f4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì„±ëŠ¥ í‰ê°€ ë©”íŠ¸ë¦­\n",
    "class RecommendationEvaluator:\n",
    "    \"\"\"ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ í´ëž˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def evaluate_system(self, system, test_cases: List[Dict], \n",
    "                       system_name: str = \"System\") -> Dict:\n",
    "        \"\"\"ì‹œìŠ¤í…œ ì¢…í•© í‰ê°€\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸ” {system_name} í‰ê°€ ì¤‘...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        results = {\n",
    "            'system_name': system_name,\n",
    "            'precision_at_3': [],\n",
    "            'precision_at_5': [],\n",
    "            'recall_at_3': [],\n",
    "            'recall_at_5': [],\n",
    "            'ndcg_at_5': [],\n",
    "            'mrr': [],\n",
    "            'diversity': [],\n",
    "            'avg_time': [],\n",
    "            'tag_match_rate': []\n",
    "        }\n",
    "        \n",
    "        for i, test in enumerate(test_cases, 1):\n",
    "            print(f\"\\ní…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}/{len(test_cases)}: {test['name']}\")\n",
    "            \n",
    "            # ì¶”ì²œ ì‹œê°„ ì¸¡ì •\n",
    "            start_time = time.time()\n",
    "            recommendations = system.recommend(test['input'], top_n=5)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            results['avg_time'].append(elapsed_time)\n",
    "            \n",
    "            # Ground Truth ìƒì„± (ì‹¤ì œë¡œëŠ” ì‚¬ìš©ìž í”¼ë“œë°± ë°ì´í„° ì‚¬ìš©)\n",
    "            ground_truth = self._generate_ground_truth(test['input'])\n",
    "            \n",
    "            # ê° ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "            precision_3 = self._precision_at_k(recommendations, ground_truth, 3)\n",
    "            precision_5 = self._precision_at_k(recommendations, ground_truth, 5)\n",
    "            recall_3 = self._recall_at_k(recommendations, ground_truth, 3)\n",
    "            recall_5 = self._recall_at_k(recommendations, ground_truth, 5)\n",
    "            ndcg = self._ndcg_at_k(recommendations, ground_truth, 5)\n",
    "            mrr = self._mrr(recommendations, ground_truth)\n",
    "            diversity = self._diversity_score(recommendations)\n",
    "            tag_match = self._tag_match_rate(recommendations, test['input'])\n",
    "            \n",
    "            results['precision_at_3'].append(precision_3)\n",
    "            results['precision_at_5'].append(precision_5)\n",
    "            results['recall_at_3'].append(recall_3)\n",
    "            results['recall_at_5'].append(recall_5)\n",
    "            results['ndcg_at_5'].append(ndcg)\n",
    "            results['mrr'].append(mrr)\n",
    "            results['diversity'].append(diversity)\n",
    "            results['tag_match_rate'].append(tag_match)\n",
    "            \n",
    "            print(f\"  â±ï¸  ì‹œê°„: {elapsed_time:.4f}ì´ˆ\")\n",
    "            print(f\"  ðŸ“Š Precision@5: {precision_5:.3f}\")\n",
    "            print(f\"  ðŸ“Š íƒœê·¸ ë§¤ì¹­ë¥ : {tag_match:.3f}\")\n",
    "        \n",
    "        # í‰ê·  ê³„ì‚°\n",
    "        summary = {\n",
    "            'system_name': system_name,\n",
    "            'avg_precision_at_3': np.mean(results['precision_at_3']),\n",
    "            'avg_precision_at_5': np.mean(results['precision_at_5']),\n",
    "            'avg_recall_at_3': np.mean(results['recall_at_3']),\n",
    "            'avg_recall_at_5': np.mean(results['recall_at_5']),\n",
    "            'avg_ndcg_at_5': np.mean(results['ndcg_at_5']),\n",
    "            'avg_mrr': np.mean(results['mrr']),\n",
    "            'avg_diversity': np.mean(results['diversity']),\n",
    "            'avg_time': np.mean(results['avg_time']),\n",
    "            'avg_tag_match_rate': np.mean(results['tag_match_rate'])\n",
    "        }\n",
    "        \n",
    "        return summary, results\n",
    "    \n",
    "    def _generate_ground_truth(self, user_input: Dict) -> List[str]:\n",
    "        \"\"\"Ground Truth ìƒì„± (ì‹¤ì œ ì •ë‹µ ë°ì´í„°)\"\"\"\n",
    "        # ì‹¤ì œë¡œëŠ” ì‚¬ìš©ìž í”¼ë“œë°± ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ,\n",
    "        # ì—¬ê¸°ì„œëŠ” íƒœê·¸ê°€ ì •í™•ížˆ ì¼ì¹˜í•˜ëŠ” ìž¥ì†Œë“¤ì„ ì •ë‹µìœ¼ë¡œ ê°„ì£¼\n",
    "        \n",
    "        relevant_places = []\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            match_score = 0\n",
    "            \n",
    "            # Season ë§¤ì¹­\n",
    "            if 'season' in user_input and row['season'] == user_input['season']:\n",
    "                match_score += 1\n",
    "            \n",
    "            # Nature ë§¤ì¹­\n",
    "            if 'nature' in user_input:\n",
    "                user_nature = set(user_input['nature'] if isinstance(user_input['nature'], list) \n",
    "                                else [user_input['nature']])\n",
    "                place_nature = set(row['nature'])\n",
    "                if user_nature & place_nature:\n",
    "                    match_score += len(user_nature & place_nature)\n",
    "            \n",
    "            # Vibe ë§¤ì¹­\n",
    "            if 'vibe' in user_input:\n",
    "                user_vibe = set(user_input['vibe'] if isinstance(user_input['vibe'], list) \n",
    "                              else [user_input['vibe']])\n",
    "                place_vibe = set(row['vibe'])\n",
    "                if user_vibe & place_vibe:\n",
    "                    match_score += len(user_vibe & place_vibe)\n",
    "            \n",
    "            # Target ë§¤ì¹­\n",
    "            if 'target' in user_input:\n",
    "                user_target = set(user_input['target'] if isinstance(user_input['target'], list) \n",
    "                                else [user_input['target']])\n",
    "                place_target = set(row['target'])\n",
    "                if user_target & place_target:\n",
    "                    match_score += 1\n",
    "            \n",
    "            # ë§¤ì¹­ ì ìˆ˜ê°€ 2 ì´ìƒì´ë©´ ê´€ë ¨ ìžˆëŠ” ìž¥ì†Œë¡œ ê°„ì£¼\n",
    "            if match_score >= 2:\n",
    "                relevant_places.append(row['name'])\n",
    "        \n",
    "        return relevant_places[:10]  # ìµœëŒ€ 10ê°œ\n",
    "    \n",
    "    def _precision_at_k(self, recommendations: pd.DataFrame, \n",
    "                       ground_truth: List[str], k: int) -> float:\n",
    "        \"\"\"Precision@K\"\"\"\n",
    "        if len(recommendations) < k:\n",
    "            k = len(recommendations)\n",
    "        \n",
    "        top_k_names = recommendations['name'].iloc[:k].tolist()\n",
    "        relevant_in_top_k = len([name for name in top_k_names if name in ground_truth])\n",
    "        \n",
    "        return relevant_in_top_k / k if k > 0 else 0.0\n",
    "    \n",
    "    def _recall_at_k(self, recommendations: pd.DataFrame, \n",
    "                    ground_truth: List[str], k: int) -> float:\n",
    "        \"\"\"Recall@K\"\"\"\n",
    "        if not ground_truth:\n",
    "            return 0.0\n",
    "        \n",
    "        top_k_names = recommendations['name'].iloc[:k].tolist()\n",
    "        relevant_in_top_k = len([name for name in top_k_names if name in ground_truth])\n",
    "        \n",
    "        return relevant_in_top_k / len(ground_truth)\n",
    "    \n",
    "    def _ndcg_at_k(self, recommendations: pd.DataFrame, \n",
    "                   ground_truth: List[str], k: int) -> float:\n",
    "        \"\"\"NDCG@K (Normalized Discounted Cumulative Gain)\"\"\"\n",
    "        top_k_names = recommendations['name'].iloc[:k].tolist()\n",
    "        \n",
    "        # DCG ê³„ì‚°\n",
    "        dcg = sum([\n",
    "            (1.0 if name in ground_truth else 0.0) / np.log2(i + 2)\n",
    "            for i, name in enumerate(top_k_names)\n",
    "        ])\n",
    "        \n",
    "        # IDCG ê³„ì‚° (ì´ìƒì ì¸ ìˆœì„œ)\n",
    "        ideal_length = min(len(ground_truth), k)\n",
    "        idcg = sum([1.0 / np.log2(i + 2) for i in range(ideal_length)])\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    def _mrr(self, recommendations: pd.DataFrame, ground_truth: List[str]) -> float:\n",
    "        \"\"\"MRR (Mean Reciprocal Rank)\"\"\"\n",
    "        for i, name in enumerate(recommendations['name']):\n",
    "            if name in ground_truth:\n",
    "                return 1.0 / (i + 1)\n",
    "        return 0.0\n",
    "    \n",
    "    def _diversity_score(self, recommendations: pd.DataFrame) -> float:\n",
    "        \"\"\"ì¶”ì²œ ë‹¤ì–‘ì„± ì ìˆ˜\"\"\"\n",
    "        all_tags = set()\n",
    "        \n",
    "        for idx, row in recommendations.iterrows():\n",
    "            all_tags.update(row.get('nature', []))\n",
    "            all_tags.update(row.get('vibe', []))\n",
    "            all_tags.update(row.get('target', []))\n",
    "        \n",
    "        # ê³ ìœ  íƒœê·¸ ìˆ˜ / (ì¶”ì²œ ìˆ˜ * í‰ê·  íƒœê·¸ ìˆ˜)\n",
    "        avg_tags_per_place = 3  # ëŒ€ëžµì ì¸ í‰ê· \n",
    "        max_possible_tags = len(recommendations) * avg_tags_per_place\n",
    "        \n",
    "        return len(all_tags) / max_possible_tags if max_possible_tags > 0 else 0.0\n",
    "    \n",
    "    def _tag_match_rate(self, recommendations: pd.DataFrame, \n",
    "                       user_input: Dict) -> float:\n",
    "        \"\"\"íƒœê·¸ ë§¤ì¹­ë¥  (ì‚¬ìš©ìž ìž…ë ¥ê³¼ ì¶”ì²œ ê²°ê³¼ì˜ íƒœê·¸ ì¼ì¹˜ë„)\"\"\"\n",
    "        total_matches = 0\n",
    "        total_possible = 0\n",
    "        \n",
    "        for idx, row in recommendations.iterrows():\n",
    "            # Season\n",
    "            if 'season' in user_input:\n",
    "                total_possible += 1\n",
    "                if row['season'] == user_input['season']:\n",
    "                    total_matches += 1\n",
    "            \n",
    "            # Nature, Vibe, Target\n",
    "            for key in ['nature', 'vibe', 'target']:\n",
    "                if key in user_input and user_input[key]:\n",
    "                    user_tags = set(user_input[key] if isinstance(user_input[key], list) \n",
    "                                  else [user_input[key]])\n",
    "                    place_tags = set(row.get(key, []))\n",
    "                    \n",
    "                    if user_tags:\n",
    "                        total_possible += len(user_tags)\n",
    "                        total_matches += len(user_tags & place_tags)\n",
    "        \n",
    "        return total_matches / total_possible if total_possible > 0 else 0.0\n",
    "\n",
    "# í‰ê°€ìž ìƒì„±\n",
    "evaluator = RecommendationEvaluator(df)\n",
    "print(\"âœ… í‰ê°€ìž ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b6f6b-876d-4a09-93e7-fd3b9a96592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"ê²¨ìš¸ ë°”ë‹¤ ë°ì´íŠ¸\",\n",
    "        \"input\": {\n",
    "            \"season\": \"ê²¨ìš¸\",\n",
    "            \"nature\": [\"ë°”ë‹¤\"],\n",
    "            \"vibe\": [\"ê°ì„±\", \"ì‚°ì±…\"],\n",
    "            \"target\": [\"ì—°ì¸\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ì—¬ë¦„ ê°€ì¡± í•´ë³€ íœ´ê°€\",\n",
    "        \"input\": {\n",
    "            \"season\": \"ì—¬ë¦„\",\n",
    "            \"nature\": [\"ë°”ë‹¤\", \"ìžì—°\"],\n",
    "            \"vibe\": [\"ížë§\", \"ì•¡í‹°ë¹„í‹°\"],\n",
    "            \"target\": [\"ê°€ì¡±\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ê°€ì„ ì‚° ížë§\",\n",
    "        \"input\": {\n",
    "            \"season\": \"ê°€ì„\",\n",
    "            \"nature\": [\"ì‚°\"],\n",
    "            \"vibe\": [\"ì¡°ìš©í•œ\", \"ížë§\"],\n",
    "            \"target\": [\"ì¹œêµ¬\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ë´„ ìžì—° ì‚°ì±…\",\n",
    "        \"input\": {\n",
    "            \"season\": \"ë´„\",\n",
    "            \"nature\": [\"ìžì—°\", \"ì‚°\"],\n",
    "            \"vibe\": [\"ì‚°ì±…\"],\n",
    "            \"target\": [\"ì—°ì¸\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ì‚¬ê³„ì ˆ ê°ì„± ì—¬í–‰\",\n",
    "        \"input\": {\n",
    "            \"season\": \"ì‚¬ê³„ì ˆ\",\n",
    "            \"nature\": [\"í˜¸ìˆ˜\", \"ìžì—°\"],\n",
    "            \"vibe\": [\"ê°ì„±\", \"ì‚¬ì§„ëª…ì†Œ\"],\n",
    "            \"target\": [\"ì¹œêµ¬\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ì—¬ë¦„ ìŠ¤ë¦´ ëª¨í—˜\",\n",
    "        \"input\": {\n",
    "            \"season\": \"ì—¬ë¦„\",\n",
    "            \"nature\": [\"ì‚°\", \"ë°”ë‹¤\"],\n",
    "            \"vibe\": [\"ìŠ¤ë¦´\", \"ì•¡í‹°ë¹„í‹°\"],\n",
    "            \"target\": [\"ì¹œêµ¬\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ê²¨ìš¸ ê°€ì¡± ìŠ¤í‚¤\",\n",
    "        \"input\": {\n",
    "            \"season\": \"ê²¨ìš¸\",\n",
    "            \"nature\": [\"ì‚°\"],\n",
    "            \"vibe\": [\"ì•¡í‹°ë¹„í‹°\", \"ìŠ¤ë¦´\"],\n",
    "            \"target\": [\"ê°€ì¡±\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ê°€ì„ ì—­ì‚¬ íƒë°©\",\n",
    "        \"input\": {\n",
    "            \"season\": \"ê°€ì„\",\n",
    "            \"nature\": [\"ìžì—°\"],\n",
    "            \"vibe\": [\"ì¡°ìš©í•œ\"],\n",
    "            \"target\": [\"ê°€ì¡±\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nâœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¤€ë¹„ ì™„ë£Œ: {len(test_cases)}ê°œ\")\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    print(f\"  {i}. {test['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17361792-28dc-48e5-9fdd-91891c8e5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ê¸°ë³¸ ì‹œìŠ¤í…œ vs ê°œì„  ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âš”ï¸  ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. ê¸°ë³¸ ì‹œìŠ¤í…œ í‰ê°€\n",
    "print(\"\\nðŸ”µ [1/2] ê¸°ë³¸ ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ ì¤‘...\")\n",
    "basic_summary, basic_details = evaluator.evaluate_system(\n",
    "    basic_system, \n",
    "    test_cases, \n",
    "    system_name=\"ê¸°ë³¸ ì‹œìŠ¤í…œ\"\n",
    ")\n",
    "\n",
    "# 2. ê°œì„  ì‹œìŠ¤í…œ í‰ê°€\n",
    "print(\"\\nðŸŸ¢ [2/2] ê°œì„  ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ ì¤‘...\")\n",
    "enhanced_summary, enhanced_details = evaluator.evaluate_system(\n",
    "    recommender, \n",
    "    test_cases, \n",
    "    system_name=\"ê°œì„  ì‹œìŠ¤í…œ\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1881d9-8bd3-4bc2-81be-b602d66f2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ê²°ê³¼ ë¹„êµí‘œ ìƒì„±\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "comparison_df = pd.DataFrame([basic_summary, enhanced_summary])\n",
    "comparison_df = comparison_df.set_index('system_name')\n",
    "\n",
    "# ê°œì„ ìœ¨ ê³„ì‚°\n",
    "improvement = {}\n",
    "for col in comparison_df.columns:\n",
    "    basic_val = comparison_df.loc['ê¸°ë³¸ ì‹œìŠ¤í…œ', col]\n",
    "    enhanced_val = comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', col]\n",
    "    \n",
    "    if col == 'avg_time':\n",
    "        # ì‹œê°„ì€ ê°ì†Œê°€ ì¢‹ìŒ\n",
    "        improvement[col] = ((basic_val - enhanced_val) / basic_val * 100)\n",
    "    else:\n",
    "        # ë‚˜ë¨¸ì§€ëŠ” ì¦ê°€ê°€ ì¢‹ìŒ\n",
    "        improvement[col] = ((enhanced_val - basic_val) / basic_val * 100) if basic_val > 0 else 0\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\nðŸ“‹ ì£¼ìš” ë©”íŠ¸ë¦­ ë¹„êµ:\\n\")\n",
    "print(f\"{'ë©”íŠ¸ë¦­':<30} {'ê¸°ë³¸ ì‹œìŠ¤í…œ':>15} {'ê°œì„  ì‹œìŠ¤í…œ':>15} {'ê°œì„ ìœ¨':>15}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metric_names = {\n",
    "    'avg_precision_at_5': 'Precision@5',\n",
    "    'avg_recall_at_5': 'Recall@5',\n",
    "    'avg_ndcg_at_5': 'NDCG@5',\n",
    "    'avg_mrr': 'MRR',\n",
    "    'avg_diversity': 'ë‹¤ì–‘ì„±',\n",
    "    'avg_tag_match_rate': 'íƒœê·¸ ë§¤ì¹­ë¥ ',\n",
    "    'avg_time': 'í‰ê·  ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)'\n",
    "}\n",
    "\n",
    "for col, name in metric_names.items():\n",
    "    basic_val = comparison_df.loc['ê¸°ë³¸ ì‹œìŠ¤í…œ', col]\n",
    "    enhanced_val = comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', col]\n",
    "    improve = improvement[col]\n",
    "    \n",
    "    if col == 'avg_time':\n",
    "        print(f\"{name:<30} {basic_val:>15.4f} {enhanced_val:>15.4f} {improve:>14.1f}%â†“\")\n",
    "    else:\n",
    "        print(f\"{name:<30} {basic_val:>15.4f} {enhanced_val:>15.4f} {improve:>14.1f}%â†‘\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ì „ì²´ ì„±ëŠ¥ í–¥ìƒ ê³„ì‚°\n",
    "key_metrics = ['avg_precision_at_5', 'avg_recall_at_5', 'avg_ndcg_at_5', 'avg_tag_match_rate']\n",
    "avg_improvement = np.mean([improvement[m] for m in key_metrics])\n",
    "\n",
    "print(f\"\\nðŸŽ¯ ì¢…í•© ì„±ëŠ¥ í–¥ìƒ: {avg_improvement:.1f}%\")\n",
    "\n",
    "# ì„±ëŠ¥ ë“±ê¸‰ ë§¤ê¸°ê¸°\n",
    "if avg_improvement >= 30:\n",
    "    grade = \"ðŸ† íƒì›”í•œ ê°œì„ \"\n",
    "elif avg_improvement >= 20:\n",
    "    grade = \"ðŸ¥‡ ìš°ìˆ˜í•œ ê°œì„ \"\n",
    "elif avg_improvement >= 10:\n",
    "    grade = \"ðŸ¥ˆ ì¢‹ì€ ê°œì„ \"\n",
    "else:\n",
    "    grade = \"ðŸ¥‰ ë³´í†µ ê°œì„ \"\n",
    "\n",
    "print(f\"í‰ê°€: {grade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1499c-3469-4df6-a8f1-13524f6cd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 1. ë©”íŠ¸ë¦­ë³„ ë¹„êµ ê·¸ëž˜í”„\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('ðŸ”¬ ì¶”ì²œ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ', fontsize=20, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = [\n",
    "    ('avg_precision_at_5', 'Precision@5', axes[0, 0]),\n",
    "    ('avg_recall_at_5', 'Recall@5', axes[0, 1]),\n",
    "    ('avg_ndcg_at_5', 'NDCG@5', axes[0, 2]),\n",
    "    ('avg_mrr', 'MRR', axes[1, 0]),\n",
    "    ('avg_diversity', 'ë‹¤ì–‘ì„±', axes[1, 1]),\n",
    "    ('avg_tag_match_rate', 'íƒœê·¸ ë§¤ì¹­ë¥ ', axes[1, 2])\n",
    "]\n",
    "\n",
    "for col, title, ax in metrics_to_plot:\n",
    "    values = [comparison_df.loc['ê¸°ë³¸ ì‹œìŠ¤í…œ', col], \n",
    "              comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', col]]\n",
    "    colors = ['#3498db', '#2ecc71']\n",
    "    \n",
    "    bars = ax.bar(['ê¸°ë³¸', 'ê°œì„ '], values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('ì ìˆ˜', fontsize=12)\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # ê°’ í‘œì‹œ\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{values[i]:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # ê°œì„ ìœ¨ í‘œì‹œ\n",
    "    improve_pct = improvement[col]\n",
    "    ax.text(0.5, 0.95, f'ê°œì„ : {improve_pct:+.1f}%',\n",
    "            transform=ax.transAxes,\n",
    "            ha='center', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5),\n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ… ê·¸ëž˜í”„ ì €ìž¥: performance_comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "# 2. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë³„ ì„±ëŠ¥ ë¹„êµ\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(test_cases))\n",
    "width = 0.35\n",
    "\n",
    "basic_scores = basic_details['precision_at_5']\n",
    "enhanced_scores = enhanced_details['precision_at_5']\n",
    "\n",
    "bars1 = ax.bar(x - width/2, basic_scores, width, label='ê¸°ë³¸ ì‹œìŠ¤í…œ', \n",
    "               color='#3498db', alpha=0.7, edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, enhanced_scores, width, label='ê°œì„  ì‹œìŠ¤í…œ', \n",
    "               color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Precision@5', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ðŸ“Š í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë³„ Precision@5 ë¹„êµ', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"TC{i+1}\" for i in range(len(test_cases))], rotation=0)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ê°’ í‘œì‹œ\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_case_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ… ê·¸ëž˜í”„ ì €ìž¥: test_case_comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "# 3. ë ˆì´ë” ì°¨íŠ¸\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "categories = ['Precision', 'Recall', 'NDCG', 'MRR', 'ë‹¤ì–‘ì„±', 'íƒœê·¸ë§¤ì¹­']\n",
    "metrics_radar = ['avg_precision_at_5', 'avg_recall_at_5', 'avg_ndcg_at_5', \n",
    "                 'avg_mrr', 'avg_diversity', 'avg_tag_match_rate']\n",
    "\n",
    "basic_values = [comparison_df.loc['ê¸°ë³¸ ì‹œìŠ¤í…œ', m] for m in metrics_radar]\n",
    "enhanced_values = [comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', m] for m in metrics_radar]\n",
    "\n",
    "# ê°ë„ ì„¤ì •\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "basic_values += basic_values[:1]\n",
    "enhanced_values += enhanced_values[:1]\n",
    "angles += angles[:1]\n",
    "\n",
    "ax.plot(angles, basic_values, 'o-', linewidth=2, label='ê¸°ë³¸ ì‹œìŠ¤í…œ', color='#3498db')\n",
    "ax.fill(angles, basic_values, alpha=0.25, color='#3498db')\n",
    "ax.plot(angles, enhanced_values, 'o-', linewidth=2, label='ê°œì„  ì‹œìŠ¤í…œ', color='#2ecc71')\n",
    "ax.fill(angles, enhanced_values, alpha=0.25, color='#2ecc71')\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('ðŸŽ¯ ì¢…í•© ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('radar_chart.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ… ê·¸ëž˜í”„ ì €ìž¥: radar_chart.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ì‹œê°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e60980-7d5c-41eb-a6db-748585954d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## êµ¬ì²´ì ì¸ ì¶”ì²œ ê²°ê³¼ ë¹„êµ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ” ìƒì„¸ ì‚¬ë¡€ ë¶„ì„\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ìƒì„¸ ë¹„êµ\n",
    "test_case = test_cases[0]\n",
    "\n",
    "print(f\"\\nðŸ“Œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {test_case['name']}\")\n",
    "print(f\"ìž…ë ¥: {test_case['input']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ðŸ”µ ê¸°ë³¸ ì‹œìŠ¤í…œ ì¶”ì²œ ê²°ê³¼:\")\n",
    "print(\"-\"*80)\n",
    "basic_recs = basic_system.recommend(test_case['input'], top_n=5)\n",
    "for i, (idx, row) in enumerate(basic_recs.iterrows(), 1):\n",
    "    print(f\"\\n{i}. {row['name']}\")\n",
    "    print(f\"   ê³„ì ˆ: {row['season']}\")\n",
    "    print(f\"   ìžì—°: {', '.join(row['nature'])}\")\n",
    "    print(f\"   ë¶„ìœ„ê¸°: {', '.join(row['vibe'])}\")\n",
    "    print(f\"   ì ìˆ˜: {row['score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ðŸŸ¢ ê°œì„  ì‹œìŠ¤í…œ ì¶”ì²œ ê²°ê³¼:\")\n",
    "print(\"-\"*80)\n",
    "enhanced_recs = recommender.recommend(test_case['input'], top_n=5)\n",
    "for i, (idx, row) in enumerate(enhanced_recs.iterrows(), 1):\n",
    "    print(f\"\\n{i}. {row['name']}\")\n",
    "    print(f\"   ê³„ì ˆ: {row['season']}\")\n",
    "    print(f\"   ìžì—°: {', '.join(row['nature'])}\")\n",
    "    print(f\"   ë¶„ìœ„ê¸°: {', '.join(row['vibe'])}\")\n",
    "    print(f\"   ëŒ€ìƒ: {', '.join(row['target']) if row['target'] else 'ì •ë³´ì—†ìŒ'}\")\n",
    "    print(f\"   ìµœì¢… ì ìˆ˜: {row['final_score']:.4f}\")\n",
    "    print(f\"   (ìœ ì‚¬ë„: {row['similarity_score']:.3f}, íƒœê·¸: {row['tag_score']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ðŸ“Š ë¹„êµ ë¶„ì„:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# íƒœê·¸ ë§¤ì¹­ ë¶„ì„\n",
    "def analyze_tag_matching(recs, user_input):\n",
    "    matches = {'season': 0, 'nature': 0, 'vibe': 0, 'target': 0}\n",
    "    total = len(recs)\n",
    "    \n",
    "    for idx, row in recs.iterrows():\n",
    "        if row['season'] == user_input.get('season'):\n",
    "            matches['season'] += 1\n",
    "        \n",
    "        for key in ['nature', 'vibe', 'target']:\n",
    "            if key in user_input and user_input[key]:\n",
    "                user_tags = set(user_input[key] if isinstance(user_input[key], list) \n",
    "                              else [user_input[key]])\n",
    "                place_tags = set(row.get(key, []))\n",
    "                if user_tags & place_tags:\n",
    "                    matches[key] += 1\n",
    "    \n",
    "    return {k: v/total for k, v in matches.items()}\n",
    "\n",
    "basic_matches = analyze_tag_matching(basic_recs, test_case['input'])\n",
    "enhanced_matches = analyze_tag_matching(enhanced_recs, test_case['input'])\n",
    "\n",
    "print(\"\\níƒœê·¸ ë§¤ì¹­ë¥  ë¹„êµ:\")\n",
    "print(f\"{'ì¹´í…Œê³ ë¦¬':<15} {'ê¸°ë³¸ ì‹œìŠ¤í…œ':>15} {'ê°œì„  ì‹œìŠ¤í…œ':>15} {'ê°œì„ ':>15}\")\n",
    "print(\"-\"*65)\n",
    "for key in ['season', 'nature', 'vibe', 'target']:\n",
    "    basic_val = basic_matches[key]\n",
    "    enhanced_val = enhanced_matches[key]\n",
    "    improve = ((enhanced_val - basic_val) / basic_val * 100) if basic_val > 0 else 0\n",
    "    print(f\"{key:<15} {basic_val:>14.1%} {enhanced_val:>14.1%} {improve:>13.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5788bb8-e3bb-467e-8d80-1206c47ac0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ ìµœì¢… ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                        ðŸ† ì„±ëŠ¥ í‰ê°€ ìµœì¢… ìš”ì•½                               â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                           â•‘\n",
    "â•‘  ðŸ“Š ì£¼ìš” ë©”íŠ¸ë¦­ ê°œì„ ìœ¨:                                                     â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â•‘\n",
    "â•‘  â€¢ Precision@5        : {improvement['avg_precision_at_5']:>6.1f}% â†‘                                  â•‘\n",
    "â•‘  â€¢ Recall@5           : {improvement['avg_recall_at_5']:>6.1f}% â†‘                                  â•‘\n",
    "â•‘  â€¢ NDCG@5             : {improvement['avg_ndcg_at_5']:>6.1f}% â†‘                                  â•‘\n",
    "â•‘  â€¢ íƒœê·¸ ë§¤ì¹­ë¥          : {improvement['avg_tag_match_rate']:>6.1f}% â†‘                                  â•‘\n",
    "â•‘                                                                           â•‘\n",
    "â•‘  âš¡ ì„±ëŠ¥ ì§€í‘œ:                                                              â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â•‘\n",
    "â•‘  â€¢ í‰ê·  ì²˜ë¦¬ ì‹œê°„      : {comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', 'avg_time']:.4f}ì´ˆ                                    â•‘\n",
    "â•‘  â€¢ ì¶”ì²œ ë‹¤ì–‘ì„±         : {comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', 'avg_diversity']:.3f}                                       â•‘\n",
    "â•‘  â€¢ MRR                : {comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', 'avg_mrr']:.3f}                                       â•‘\n",
    "â•‘                                                                           â•‘\n",
    "â•‘  ðŸŽ¯ ì¢…í•© í‰ê°€:                                                             â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â•‘\n",
    "â•‘  â€¢ ì „ì²´ ì„±ëŠ¥ í–¥ìƒ      : {avg_improvement:>6.1f}%                                        â•‘\n",
    "â•‘  â€¢ í‰ê°€ ë“±ê¸‰           : {grade:<20}                          â•‘\n",
    "â•‘                                                                           â•‘\n",
    "â•‘  ðŸ’¡ ì£¼ìš” ê°œì„  ì‚¬í•­:                                                         â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â•‘\n",
    "â•‘  âœ“ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì„¤ëª… í…ìŠ¤íŠ¸ í’ˆì§ˆ í–¥ìƒ                                      â•‘\n",
    "â•‘  âœ“ ì•™ìƒë¸” ìž„ë² ë”©ìœ¼ë¡œ ì˜ë¯¸ í‘œí˜„ë ¥ ì¦ê°€                                        â•‘\n",
    "â•‘  âœ“ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ìœ¼ë¡œ ë¶„ë¥˜ ì •í™•ë„ ê°œì„                                        â•‘\n",
    "â•‘  âœ“ ê³ ê¸‰ ìŠ¤ì½”ì–´ë§ìœ¼ë¡œ íƒœê·¸ ë§¤ì¹­ ì •í™•ë„ í–¥ìƒ                                    â•‘\n",
    "â•‘  âœ“ XGBoost ìµœì í™”ë¡œ ì˜ˆì¸¡ ì„±ëŠ¥ ê°œì„                                           â•‘\n",
    "â•‘                                                                           â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "# CSVë¡œ ìƒì„¸ ê²°ê³¼ ì €ìž¥\n",
    "results_summary = pd.DataFrame({\n",
    "    'ì‹œìŠ¤í…œ': ['ê¸°ë³¸', 'ê°œì„ '],\n",
    "    'Precision@5': [comparison_df.loc['ê¸°ë³¸ ì‹œìŠ¤í…œ', 'avg_precision_at_5'],\n",
    "                   comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', 'avg_precision_at_5']],\n",
    "    'Recall@5': [comparison_df.loc['ê¸°ë³¸ ì‹œìŠ¤í…œ', 'avg_recall_at_5'],\n",
    "                comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', 'avg_recall_at_5']],\n",
    "    'NDCG@5': [comparison_df.loc['ê¸°ë³¸ ì‹œìŠ¤í…œ', 'avg_ndcg_at_5'],\n",
    "              comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', 'avg_ndcg_at_5']],\n",
    "    'MRR': [comparison_df.loc['ê¸°ë³¸ ì‹œìŠ¤í…œ', 'avg_mrr'],\n",
    "           comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', 'avg_mrr']],\n",
    "    'ë‹¤ì–‘ì„±': [comparison_df.loc['ê¸°ë³¸ ì‹œìŠ¤í…œ', 'avg_diversity'],\n",
    "             comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', 'avg_diversity']],\n",
    "    'íƒœê·¸ë§¤ì¹­ë¥ ': [comparison_df.loc['ê¸°ë³¸ ì‹œìŠ¤í…œ', 'avg_tag_match_rate'],\n",
    "                comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', 'avg_tag_match_rate']],\n",
    "    'ì²˜ë¦¬ì‹œê°„': [comparison_df.loc['ê¸°ë³¸ ì‹œìŠ¤í…œ', 'avg_time'],\n",
    "               comparison_df.loc['ê°œì„  ì‹œìŠ¤í…œ', 'avg_time']]\n",
    "})\n",
    "\n",
    "results_summary.to_csv('performance_evaluation_results.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\nðŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ìž¥: performance_evaluation_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa689f-b48a-46bc-95cc-837d9e2dc1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c713fe9-f49c-4a0e-8a6e-b7324a5b616f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12144d19-9b6d-4193-ac40-6cae0a6893aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7002184-dca5-4230-a0d2-f28940fd372d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40396e88-2cb1-4912-917a-91d91336e07c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
