{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00861740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdwl\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tjdwl\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 라이브러리 설치 및 임포트\n",
    "import os, sys, json, math, time, warnings\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "try:\n",
    "    import umap  # Optional\n",
    "except Exception:\n",
    "    umap = None\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    SentenceTransformer = None\n",
    "    warnings.warn(f\"sentence-transformers 불러오기 실패: {e}\")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    XGBClassifier = None\n",
    "    warnings.warn(f\"xgboost 불러오기 실패: {e}\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# 디렉터리 준비\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "os.makedirs('models/encoders', exist_ok=True)\n",
    "os.makedirs('models/checkpoints', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed782e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 설정 파일 생성\n",
    "config: Dict[str, Any] = {\n",
    "    \"data\": {\n",
    "        \"raw_file\": \"data/raw/gangwon_places_1000.xlsx\",\n",
    "        \"processed_file\": \"data/processed/gangwon_places_1000.csv\",\n",
    "        \"embeddings_file\": \"data/embeddings/place_embeddings.npy\",\n",
    "        \"reduced_embeddings_file\": \"data/embeddings/place_embeddings_pca128.npy\",\n",
    "    },\n",
    "    \"text\": {\n",
    "        \"fields_for_embedding\": [\"name\", \"short_description\"],\n",
    "        \"max_tokens_per_field\": 64\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"sbert_model\": \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\",\n",
    "        \"dimensionality_reduction\": \"PCA\",   # 'PCA' or 'UMAP'\n",
    "        \"reduced_dim\": 128,\n",
    "        \"xgb_params\": {\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.08,\n",
    "            \"subsample\": 0.9,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "            \"reg_lambda\": 1.0,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"n_jobs\": -1,\n",
    "            \"tree_method\": \"hist\"\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948de1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 데이터 전처리 함수 정의\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"name\",\"season\",\"nature\",\"vibe\",\"target\",\"fee\",\"parking\",\"address\",\n",
    "    \"open_time\",\"latitude\",\"longitude\",\"full_address\",\"short_description\"\n",
    "]\n",
    "\n",
    "def _split_labels(raw: Any) -> List[str]:\n",
    "    if pd.isna(raw):\n",
    "        return []\n",
    "    s = str(raw).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    for sep in [',',';','/','|']:\n",
    "        s = s.replace(sep, ' ')\n",
    "    parts = [p.strip() for p in s.split() if p.strip()]\n",
    "    return parts\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, max_labels_per_tag: int = 5):\n",
    "        self.max_labels_per_tag = max_labels_per_tag\n",
    "        self.encoders: Dict[str, MultiLabelBinarizer] = {}\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        missing = [c for c in EXPECTED_COLUMNS if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"스키마 불일치: 누락 컬럼 {missing}\")\n",
    "        df = df[EXPECTED_COLUMNS].copy()\n",
    "\n",
    "        for c in [\"name\",\"short_description\",\"address\",\"full_address\",\"open_time\",\"fee\",\"parking\",\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "        for c in [\"latitude\",\"longitude\"]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "        df.drop_duplicates(subset=[\"name\",\"address\"], inplace=True)\n",
    "\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            df[tag] = df[tag].apply(_split_labels).apply(lambda arr: arr[: self.max_labels_per_tag])\n",
    "        return df\n",
    "\n",
    "    def fit_encoders(self, df: pd.DataFrame) -> None:\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit(df[tag].tolist())\n",
    "            self.encoders[tag] = mlb\n",
    "        for tag, mlb in self.encoders.items():\n",
    "            np.save(f\"models/encoders/{tag}_classes.npy\", mlb.classes_)\n",
    "\n",
    "    def load_encoders(self) -> None:\n",
    "        enc = {}\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            path = f\"models/encoders/{tag}_classes.npy\"\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"인코더 파일이 없습니다: {path}\")\n",
    "            classes = np.load(path, allow_pickle=True)\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit([classes.tolist()])\n",
    "            enc[tag] = mlb\n",
    "        self.encoders = enc\n",
    "\n",
    "    def encode_labels(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        if not self.encoders:\n",
    "            self.load_encoders()\n",
    "        y = {}\n",
    "        for tag, mlb in self.encoders.items():\n",
    "            y[tag] = mlb.transform(df[tag].tolist())\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125e2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 임베딩 생성 클래스 정의\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: Optional[str] = None):\n",
    "        self.model_name = model_name or config[\"model\"][\"sbert_model\"]\n",
    "        self.model = None\n",
    "        self.dimension_reducer = None\n",
    "        self.reduced_dim = config[\"model\"][\"reduced_dim\"]\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model is not None:\n",
    "            return\n",
    "        if SentenceTransformer is None:\n",
    "            raise RuntimeError(\"SentenceTransformer를 불러올 수 없습니다.\")\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "    def _concat_text_fields(self, row: pd.Series, fields: List[str]) -> str:\n",
    "        parts = []\n",
    "        for f in fields:\n",
    "            val = str(row.get(f, \"\")).strip()\n",
    "            if val and val.lower() != \"nan\":\n",
    "                parts.append(val)\n",
    "        return \" \".join(parts)\n",
    "\n",
    "    def build_texts(self, df: pd.DataFrame, fields: Optional[List[str]] = None) -> List[str]:\n",
    "        fields = fields or config[\"text\"][\"fields_for_embedding\"]\n",
    "        return [self._concat_text_fields(row, fields) for _, row in df.iterrows()]\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str], cache_path: str) -> np.ndarray:\n",
    "        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "        if os.path.exists(cache_path):\n",
    "            embs = np.load(cache_path)\n",
    "            print(f\"🔁 임베딩 캐시 로드: {cache_path} {embs.shape}\")\n",
    "            return embs\n",
    "        self.load_model()\n",
    "        embs = self.model.encode(texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        np.save(cache_path, embs)\n",
    "        print(f\"✅ 임베딩 저장: {cache_path} {embs.shape}\")\n",
    "        return embs\n",
    "\n",
    "    def fit_dimension_reducer(self, X: np.ndarray, method: str = \"PCA\", n_components: int = 128):\n",
    "        method = (method or \"PCA\").upper()\n",
    "        if method == \"UMAP\":\n",
    "            if umap is None:\n",
    "                warnings.warn(\"UMAP이 설치되어 있지 않아 PCA로 대체합니다.\")\n",
    "            else:\n",
    "                self.dimension_reducer = umap.UMAP(\n",
    "                    n_components=n_components, n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=RANDOM_STATE\n",
    "                )\n",
    "                self.reduced_dim = n_components\n",
    "                self.dimension_reducer.fit(X)\n",
    "                return\n",
    "        self.dimension_reducer = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "        self.reduced_dim = n_components\n",
    "        self.dimension_reducer.fit(X)\n",
    "\n",
    "    def reduce_dimensions(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.dimension_reducer is None:\n",
    "            raise RuntimeError(\"dimension_reducer가 없습니다. fit_dimension_reducer 먼저 호출하세요.\")\n",
    "        return self.dimension_reducer.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b5b7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 실제 csv 파일 로드 및 검증\n",
    "def load_and_validate_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"XLSX/CSV 자동 판별 + 13컬럼 검증 + processed 저장\"\"\"\n",
    "    candidates = [\n",
    "        config[\"data\"][\"raw_file\"],\n",
    "        \"data/raw/gangwon_places_1000.xlsx\",\n",
    "        file_path\n",
    "    ]\n",
    "    src = next((p for p in candidates if p and os.path.exists(p)), None)\n",
    "    if src is None:\n",
    "        raise FileNotFoundError(\"데이터 파일을 찾을 수 없습니다. data/raw에 1000개 파일을 두세요.\")\n",
    "\n",
    "    if src.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df = pd.read_excel(src)\n",
    "    else:\n",
    "        df = pd.read_csv(src)\n",
    "\n",
    "    missing = [c for c in EXPECTED_COLUMNS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"스키마 불일치: 누락 컬럼 {missing}\")\n",
    "    df = df[EXPECTED_COLUMNS].copy()\n",
    "\n",
    "    out = config[\"data\"][\"processed_file\"]\n",
    "    os.makedirs(os.path.dirname(out), exist_ok=True)\n",
    "    df.to_csv(out, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ 데이터 저장: {out} ({len(df):,} rows)\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15a9f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## XGBoost 트레이너\n",
    "class XGBoostTrainer:\n",
    "    def __init__(self, params: Optional[Dict[str, Any]] = None):\n",
    "        if XGBClassifier is None:\n",
    "            raise RuntimeError(\"XGBoost를 불러올 수 없습니다.\")\n",
    "        self.params = params or config[\"model\"][\"xgb_params\"]\n",
    "        self.models: Dict[str, Any] = {}\n",
    "\n",
    "    def train_models(self, X: np.ndarray, y_dict: Dict[str, np.ndarray]) -> None:\n",
    "        self.models = {}\n",
    "        for tag, y in y_dict.items():\n",
    "            clf = XGBClassifier(**self.params)\n",
    "            X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=(y.sum(axis=1)>0)\n",
    "            )\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            self.models[tag] = clf\n",
    "\n",
    "    def evaluate_models(self, X_test: np.ndarray, y_true: Dict[str, np.ndarray]) -> Dict[str, Any]:\n",
    "        report = {}\n",
    "        for tag, clf in self.models.items():\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                y_pred = (clf.predict_proba(X_test) > 0.5).astype(int)\n",
    "            else:\n",
    "                y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "            micro_f1 = f1_score(y_true[tag], y_pred, average=\"micro\", zero_division=0)\n",
    "            samples_f1 = f1_score(y_true[tag], y_pred, average=\"samples\", zero_division=0)\n",
    "            report[tag] = {\"micro_f1\": micro_f1, \"samples_f1\": samples_f1}\n",
    "        return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b403ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 추천기 (공개 메서드 이름 유지)\n",
    "class GangwonPlaceRecommender:\n",
    "    def __init__(self, cfg: Dict[str, Any] = None):\n",
    "        self.config = cfg or config\n",
    "        self.dp = DataPreprocessor()\n",
    "        self.eg = EmbeddingGenerator()\n",
    "        self.trainer = None\n",
    "        self.df: Optional[pd.DataFrame] = None\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        self.reduced: Optional[np.ndarray] = None\n",
    "\n",
    "    def prepare(self) -> None:\n",
    "        df = load_and_validate_csv(self.config[\"data\"][\"raw_file\"])\n",
    "        df = self.dp.preprocess_data(df)\n",
    "        self.dp.fit_encoders(df)\n",
    "        self.df = df\n",
    "\n",
    "        texts = self.eg.build_texts(df, self.config[\"text\"][\"fields_for_embedding\"])\n",
    "        self.embeddings = self.eg.generate_embeddings(texts, self.config[\"data\"][\"embeddings_file\"])\n",
    "\n",
    "        method = self.config[\"model\"][\"dimensionality_reduction\"]\n",
    "        n_comp = self.config[\"model\"][\"reduced_dim\"]\n",
    "        self.eg.fit_dimension_reducer(self.embeddings, method=method, n_components=n_comp)\n",
    "        self.reduced = self.eg.reduce_dimensions(self.embeddings)\n",
    "        np.save(self.config[\"data\"][\"reduced_embeddings_file\"], self.reduced)\n",
    "\n",
    "        y = self.dp.encode_labels(df)\n",
    "        self.trainer = XGBoostTrainer(self.config[\"model\"][\"xgb_params\"])\n",
    "        self.trainer.train_models(self.reduced, y)\n",
    "\n",
    "    def recommend_places(self, query_text: str, top_k: int = 10) -> pd.DataFrame:\n",
    "        if self.df is None or self.reduced is None:\n",
    "            raise RuntimeError(\"모델이 준비되지 않았습니다. prepare()를 먼저 호출하세요.\")\n",
    "        q_emb = self.eg.generate_embeddings([query_text], cache_path=\"data/embeddings/_tmp_query.npy\")\n",
    "        q_red = self.eg.reduce_dimensions(q_emb)[0]\n",
    "\n",
    "        a = self.reduced / (np.linalg.norm(self.reduced, axis=1, keepdims=True) + 1e-9)\n",
    "        b = q_red / (np.linalg.norm(q_red) + 1e-9)\n",
    "        sims = a @ b\n",
    "\n",
    "        idx = np.argsort(-sims)[:top_k]\n",
    "        out = self.df.iloc[idx].copy()\n",
    "        out[\"score\"] = sims[idx]\n",
    "        return out.reset_index(drop=True)\n",
    "\n",
    "    def recommend_places_api(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        query = payload.get(\"q\") or payload.get(\"query\") or \"\"\n",
    "        k = int(payload.get(\"k\", 10))\n",
    "        rec = self.recommend_places(query, top_k=k)\n",
    "        return {\"results\": rec.to_dict(orient=\"records\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ea2ed8e-adc8-4624-a563-4a439e91175d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'benchmark_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 벤치마크 실행\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m rec, summary \u001b[38;5;241m=\u001b[39m benchmark_pipeline()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 준비(임베딩+차원축소+학습) 시간: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprepare_time_sec\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 홀드아웃 재학습 시간: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretrain_time_sec\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'benchmark_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "## 실행 예시: 벤치마크 + 추천 결과 출력 (표준 Jupyter 표시)\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 벤치마크 실행\n",
    "rec, summary = benchmark_pipeline()\n",
    "\n",
    "print(f\"✅ 준비(임베딩+차원축소+학습) 시간: {summary['prepare_time_sec']:.2f}s\")\n",
    "print(f\"✅ 홀드아웃 재학습 시간: {summary['retrain_time_sec']:.2f}s\")\n",
    "print(\"✅ 평가 리포트(마이크로/샘플 F1):\")\n",
    "for tag, rpt in summary[\"eval_report\"].items():\n",
    "    print(f\"  - {tag}: micro_f1={rpt['micro_f1']:.4f}, samples_f1={rpt['samples_f1']:.4f}\")\n",
    "\n",
    "# 추천 질의 응답 시간(ms) 표\n",
    "rec_times_df = pd.DataFrame(summary[\"recommend_times\"]).assign(ms=lambda d: d[\"elapsed_sec\"]*1000)\\\n",
    "                                                      .drop(columns=[\"elapsed_sec\"])\n",
    "print(\"\\n[추천 질의 응답 시간(ms)]\")\n",
    "display(rec_times_df)\n",
    "\n",
    "# 결과 저장(원하면 첨부/공유 가능)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "rec_times_path = \"outputs/recommend_latency_ms.csv\"\n",
    "rec_times_df.to_csv(rec_times_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"📁 저장: {rec_times_path}\")\n",
    "\n",
    "# 각 예시 질의의 Top-K 추천 미리보기 테이블 표시 + 저장\n",
    "for q, df_topk in summary[\"examples\"]:\n",
    "    print(f\"\\n[{q}] Top 추천 미리보기\")\n",
    "    display(df_topk)\n",
    "    out_path = f\"outputs/preview_{q.replace(' ','_')}.csv\"\n",
    "    df_topk.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"📁 저장: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "703c65a9-b13b-47de-987a-bb5c282b8d04",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'caas_jupyter_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## 실행 예시: 벤치마크 + 추천 결과 출력\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaas_jupyter_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display_dataframe_to_user\n\u001b[0;32m      3\u001b[0m rec, summary \u001b[38;5;241m=\u001b[39m benchmark_pipeline()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 준비(임베딩+차원축소+학습) 시간: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprepare_time_sec\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'caas_jupyter_tools'"
     ]
    }
   ],
   "source": [
    "## 실행 예시: 벤치마크 + 추천 결과 출력\n",
    "from caas_jupyter_tools import display_dataframe_to_user\n",
    "rec, summary = benchmark_pipeline()\n",
    "\n",
    "print(f\"✅ 준비(임베딩+차원축소+학습) 시간: {summary['prepare_time_sec']:.2f}s\")\n",
    "print(f\"✅ 홀드아웃 재학습 시간: {summary['retrain_time_sec']:.2f}s\")\n",
    "print(\"✅ 평가 리포트(마이크로/샘플 F1):\")\n",
    "for tag, rpt in summary[\"eval_report\"].items():\n",
    "    print(f\"  - {tag}: micro_f1={rpt['micro_f1']:.4f}, samples_f1={rpt['samples_f1']:.4f}\")\n",
    "\n",
    "import pandas as pd\n",
    "rec_times_df = pd.DataFrame(summary[\"recommend_times\"])\n",
    "display_dataframe_to_user(\"추천 질의 응답 시간(ms)\", (rec_times_df.assign(ms=(rec_times_df[\"elapsed_sec\"]*1000)).drop(columns=[\"elapsed_sec\"])))\n",
    "\n",
    "# 각 예시 쿼리의 Top-K 추천 미리보기 테이블도 띄우기\n",
    "for q, df in summary[\"examples\"]:\n",
    "    display_dataframe_to_user(f\"[{q}] Top 추천 미리보기\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c74853ec-fbbc-4ed4-8087-e6a96633b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 저장: data/processed/gangwon_places_1000.csv (1,000 rows)\n",
      "🔁 임베딩 캐시 로드: data/embeddings/place_embeddings.npy (923, 768)\n",
      "🔁 임베딩 캐시 로드: data/embeddings/_tmp_query.npy (1, 768)\n",
      "🔁 임베딩 캐시 로드: data/embeddings/_tmp_query.npy (1, 768)\n",
      "🔁 임베딩 캐시 로드: data/embeddings/_tmp_query.npy (1, 768)\n",
      "🔁 임베딩 캐시 로드: data/embeddings/_tmp_query.npy (1, 768)\n",
      "✅ 준비(임베딩+차원축소+학습) 시간: 25.60s\n",
      "✅ 홀드아웃 재학습 시간: 20.25s\n",
      "✅ 평가 리포트(마이크로/샘플 F1):\n",
      "  - season: micro_f1=0.9426, samples_f1=0.9315\n",
      "  - nature: micro_f1=0.7083, samples_f1=0.6807\n",
      "  - vibe: micro_f1=0.8262, samples_f1=0.8568\n",
      "  - target: micro_f1=0.9100, samples_f1=0.8973\n",
      "\n",
      "[추천 질의 응답 시간(ms)]\n",
      "      query     ms\n",
      "가을 감성 바다 카페 8.8387\n",
      "   가족 산책 코스 3.6357\n",
      "    스릴 액티비티 2.6373\n",
      "      역사 유적 4.5844\n",
      "📁 저장: outputs/recommend_latency_ms.csv\n",
      "\n",
      "[가을 감성 바다 카페] Top 추천 미리보기\n",
      "       name                      address    score\n",
      "   경포 아쿠아리움         강원특별자치도 강릉시 난설헌로 131 0.508496\n",
      "      자작도해변  강원특별자치도 고성군 죽왕면 자작도선사길(죽왕면) 0.507921\n",
      "   작은후진해수욕장    강원특별자치도 삼척시 새천년도로 467(교동) 0.499675\n",
      "쏠비치삼척 오션플레이  강원특별자치도 삼척시 수로부인길 453 (갈천동) 0.458971\n",
      "      갯마을해변 강원특별자치도 양양군 현남면 안남애길 48(남애리) 0.456861\n",
      "📁 저장: outputs/preview_가을_감성_바다_카페.csv\n",
      "\n",
      "[가족 산책 코스] Top 추천 미리보기\n",
      "       name                      address    score\n",
      "   경포 아쿠아리움         강원특별자치도 강릉시 난설헌로 131 0.508496\n",
      "      자작도해변  강원특별자치도 고성군 죽왕면 자작도선사길(죽왕면) 0.507921\n",
      "   작은후진해수욕장    강원특별자치도 삼척시 새천년도로 467(교동) 0.499675\n",
      "쏠비치삼척 오션플레이  강원특별자치도 삼척시 수로부인길 453 (갈천동) 0.458971\n",
      "      갯마을해변 강원특별자치도 양양군 현남면 안남애길 48(남애리) 0.456861\n",
      "📁 저장: outputs/preview_가족_산책_코스.csv\n",
      "\n",
      "[스릴 액티비티] Top 추천 미리보기\n",
      "       name                      address    score\n",
      "   경포 아쿠아리움         강원특별자치도 강릉시 난설헌로 131 0.508496\n",
      "      자작도해변  강원특별자치도 고성군 죽왕면 자작도선사길(죽왕면) 0.507921\n",
      "   작은후진해수욕장    강원특별자치도 삼척시 새천년도로 467(교동) 0.499675\n",
      "쏠비치삼척 오션플레이  강원특별자치도 삼척시 수로부인길 453 (갈천동) 0.458971\n",
      "      갯마을해변 강원특별자치도 양양군 현남면 안남애길 48(남애리) 0.456861\n",
      "📁 저장: outputs/preview_스릴_액티비티.csv\n",
      "\n",
      "[역사 유적] Top 추천 미리보기\n",
      "       name                      address    score\n",
      "   경포 아쿠아리움         강원특별자치도 강릉시 난설헌로 131 0.508496\n",
      "      자작도해변  강원특별자치도 고성군 죽왕면 자작도선사길(죽왕면) 0.507921\n",
      "   작은후진해수욕장    강원특별자치도 삼척시 새천년도로 467(교동) 0.499675\n",
      "쏠비치삼척 오션플레이  강원특별자치도 삼척시 수로부인길 453 (갈천동) 0.458971\n",
      "      갯마을해변 강원특별자치도 양양군 현남면 안남애길 48(남애리) 0.456861\n",
      "📁 저장: outputs/preview_역사_유적.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Gangwon Recommendation System — Upgraded (1000개 + 차원축소 + 벤치마크)\n",
    "# ※ 함수/클래스/공개 메서드 이름은 기존과 호환되도록 유지\n",
    "# ============================================================\n",
    "\n",
    "# --- Imports / Dirs / Constants --------------------------------------------\n",
    "import os, sys, json, math, time, warnings\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "try:\n",
    "    import umap  # Optional\n",
    "except Exception:\n",
    "    umap = None\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    SentenceTransformer = None\n",
    "    warnings.warn(f\"sentence-transformers 불러오기 실패: {e}\")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    XGBClassifier = None\n",
    "    warnings.warn(f\"xgboost 불러오기 실패: {e}\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# 디렉터리 준비\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "os.makedirs('models/encoders', exist_ok=True)\n",
    "os.makedirs('models/checkpoints', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# --- Config -----------------------------------------------------------------\n",
    "config: Dict[str, Any] = {\n",
    "    \"data\": {\n",
    "        \"raw_file\": \"data/raw/gangwon_places_1000.xlsx\",          # 우선 사용\n",
    "        \"processed_file\": \"data/processed/gangwon_places_1000.csv\",\n",
    "        \"embeddings_file\": \"data/embeddings/place_embeddings.npy\",\n",
    "        \"reduced_embeddings_file\": \"data/embeddings/place_embeddings_pca128.npy\",\n",
    "    },\n",
    "    \"text\": {\n",
    "        \"fields_for_embedding\": [\"name\", \"short_description\"],     # 임베딩에 사용할 필드\n",
    "        \"max_tokens_per_field\": 64\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"sbert_model\": \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\",\n",
    "        \"dimensionality_reduction\": \"PCA\",   # 'PCA' or 'UMAP'\n",
    "        \"reduced_dim\": 128,\n",
    "        \"xgb_params\": {\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.08,\n",
    "            \"subsample\": 0.9,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "            \"reg_lambda\": 1.0,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"n_jobs\": -1,\n",
    "            \"tree_method\": \"hist\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Schema / Preprocessor ---------------------------------------------------\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"name\",\"season\",\"nature\",\"vibe\",\"target\",\"fee\",\"parking\",\"address\",\n",
    "    \"open_time\",\"latitude\",\"longitude\",\"full_address\",\"short_description\"\n",
    "]\n",
    "\n",
    "def _split_labels(raw: Any) -> List[str]:\n",
    "    if pd.isna(raw):\n",
    "        return []\n",
    "    s = str(raw).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    for sep in [',',';','/','|']:\n",
    "        s = s.replace(sep, ' ')\n",
    "    parts = [p.strip() for p in s.split() if p.strip()]\n",
    "    return parts\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, max_labels_per_tag: int = 5):\n",
    "        self.max_labels_per_tag = max_labels_per_tag\n",
    "        self.encoders: Dict[str, MultiLabelBinarizer] = {}\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # 스키마 검증/정렬\n",
    "        missing = [c for c in EXPECTED_COLUMNS if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"스키마 불일치: 누락 컬럼 {missing}\")\n",
    "        df = df[EXPECTED_COLUMNS].copy()\n",
    "\n",
    "        # 문자열 클린\n",
    "        for c in [\"name\",\"short_description\",\"address\",\"full_address\",\"open_time\",\"fee\",\"parking\",\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "        # 위/경도 숫자화\n",
    "        for c in [\"latitude\",\"longitude\"]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "        # 중복 제거\n",
    "        df.drop_duplicates(subset=[\"name\",\"address\"], inplace=True)\n",
    "\n",
    "        # 라벨 파싱(다중라벨)\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            df[tag] = df[tag].apply(_split_labels).apply(lambda arr: arr[: self.max_labels_per_tag])\n",
    "        return df\n",
    "\n",
    "    def fit_encoders(self, df: pd.DataFrame) -> None:\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit(df[tag].tolist())\n",
    "            self.encoders[tag] = mlb\n",
    "        # 저장\n",
    "        for tag, mlb in self.encoders.items():\n",
    "            np.save(f\"models/encoders/{tag}_classes.npy\", mlb.classes_)\n",
    "\n",
    "    def load_encoders(self) -> None:\n",
    "        enc = {}\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            path = f\"models/encoders/{tag}_classes.npy\"\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"인코더 파일이 없습니다: {path}\")\n",
    "            classes = np.load(path, allow_pickle=True)\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit([classes.tolist()])  # 동일 클래스 세트로 맞춤\n",
    "            enc[tag] = mlb\n",
    "        self.encoders = enc\n",
    "\n",
    "    def encode_labels(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        if not self.encoders:\n",
    "            self.load_encoders()\n",
    "        y = {}\n",
    "        for tag, mlb in self.encoders.items():\n",
    "            y[tag] = mlb.transform(df[tag].tolist())\n",
    "        return y\n",
    "\n",
    "# --- Embedding Generator -----------------------------------------------------\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: Optional[str] = None):\n",
    "        self.model_name = model_name or config[\"model\"][\"sbert_model\"]\n",
    "        self.model = None\n",
    "        self.dimension_reducer = None\n",
    "        self.reduced_dim = config[\"model\"][\"reduced_dim\"]\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model is not None:\n",
    "            return\n",
    "        if SentenceTransformer is None:\n",
    "            raise RuntimeError(\"SentenceTransformer를 불러올 수 없습니다.\")\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "    def _concat_text_fields(self, row: pd.Series, fields: List[str]) -> str:\n",
    "        parts = []\n",
    "        for f in fields:\n",
    "            val = str(row.get(f, \"\")).strip()\n",
    "            if val and val.lower() != \"nan\":\n",
    "                parts.append(val)\n",
    "        return \" \".join(parts)\n",
    "\n",
    "    def build_texts(self, df: pd.DataFrame, fields: Optional[List[str]] = None) -> List[str]:\n",
    "        fields = fields or config[\"text\"][\"fields_for_embedding\"]\n",
    "        return [self._concat_text_fields(row, fields) for _, row in df.iterrows()]\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str], cache_path: str) -> np.ndarray:\n",
    "        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "        if os.path.exists(cache_path):\n",
    "            embs = np.load(cache_path)\n",
    "            print(f\"🔁 임베딩 캐시 로드: {cache_path} {embs.shape}\")\n",
    "            return embs\n",
    "        self.load_model()\n",
    "        embs = self.model.encode(texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        np.save(cache_path, embs)\n",
    "        print(f\"✅ 임베딩 저장: {cache_path} {embs.shape}\")\n",
    "        return embs\n",
    "\n",
    "    def fit_dimension_reducer(self, X: np.ndarray, method: str = \"PCA\", n_components: int = 128):\n",
    "        method = (method or \"PCA\").upper()\n",
    "        if method == \"UMAP\":\n",
    "            if umap is None:\n",
    "                warnings.warn(\"UMAP이 설치되어 있지 않아 PCA로 대체합니다.\")\n",
    "            else:\n",
    "                self.dimension_reducer = umap.UMAP(\n",
    "                    n_components=n_components, n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=RANDOM_STATE\n",
    "                )\n",
    "                self.reduced_dim = n_components\n",
    "                self.dimension_reducer.fit(X)\n",
    "                return\n",
    "        # PCA fallback\n",
    "        self.dimension_reducer = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "        self.reduced_dim = n_components\n",
    "        self.dimension_reducer.fit(X)\n",
    "\n",
    "    def reduce_dimensions(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.dimension_reducer is None:\n",
    "            raise RuntimeError(\"dimension_reducer가 없습니다. fit_dimension_reducer 먼저 호출하세요.\")\n",
    "        return self.dimension_reducer.transform(X)\n",
    "\n",
    "# --- Data Loader -------------------------------------------------------------\n",
    "def load_and_validate_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"XLSX/CSV 자동 판별 + 13컬럼 검증 + processed 저장\"\"\"\n",
    "    candidates = [\n",
    "        config[\"data\"][\"raw_file\"],\n",
    "        \"data/raw/gangwon_places_1000.csv\",\n",
    "        file_path\n",
    "    ]\n",
    "    src = next((p for p in candidates if p and os.path.exists(p)), None)\n",
    "    if src is None:\n",
    "        raise FileNotFoundError(\"데이터 파일을 찾을 수 없습니다. data/raw에 1000개 파일을 두세요.\")\n",
    "\n",
    "    if src.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df = pd.read_excel(src)\n",
    "    else:\n",
    "        df = pd.read_csv(src)\n",
    "\n",
    "    missing = [c for c in EXPECTED_COLUMNS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"스키마 불일치: 누락 컬럼 {missing}\")\n",
    "    df = df[EXPECTED_COLUMNS].copy()\n",
    "\n",
    "    out = config[\"data\"][\"processed_file\"]\n",
    "    os.makedirs(os.path.dirname(out), exist_ok=True)\n",
    "    df.to_csv(out, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ 데이터 저장: {out} ({len(df):,} rows)\")\n",
    "    return df\n",
    "\n",
    "# --- XGBoost Trainer ---------------------------------------------------------\n",
    "class XGBoostTrainer:\n",
    "    def __init__(self, params: Optional[Dict[str, Any]] = None):\n",
    "        if XGBClassifier is None:\n",
    "            raise RuntimeError(\"XGBoost를 불러올 수 없습니다.\")\n",
    "        self.params = params or config[\"model\"][\"xgb_params\"]\n",
    "        self.models: Dict[str, Any] = {}\n",
    "\n",
    "    def train_models(self, X: np.ndarray, y_dict: Dict[str, np.ndarray]) -> None:\n",
    "        self.models = {}\n",
    "        for tag, y in y_dict.items():\n",
    "            clf = XGBClassifier(**self.params)\n",
    "            X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=(y.sum(axis=1)>0)\n",
    "            )\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            self.models[tag] = clf\n",
    "\n",
    "    def evaluate_models(self, X_test: np.ndarray, y_true: Dict[str, np.ndarray]) -> Dict[str, Any]:\n",
    "        report = {}\n",
    "        for tag, clf in self.models.items():\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                y_pred = (clf.predict_proba(X_test) > 0.5).astype(int)\n",
    "            else:\n",
    "                y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "            micro_f1 = f1_score(y_true[tag], y_pred, average=\"micro\", zero_division=0)\n",
    "            samples_f1 = f1_score(y_true[tag], y_pred, average=\"samples\", zero_division=0)\n",
    "            report[tag] = {\"micro_f1\": micro_f1, \"samples_f1\": samples_f1}\n",
    "        return report\n",
    "\n",
    "# --- Recommender (Public API kept) ------------------------------------------\n",
    "class GangwonPlaceRecommender:\n",
    "    def __init__(self, cfg: Dict[str, Any] = None):\n",
    "        self.config = cfg or config\n",
    "        self.dp = DataPreprocessor()\n",
    "        self.eg = EmbeddingGenerator()\n",
    "        self.trainer = None\n",
    "        self.df: Optional[pd.DataFrame] = None\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        self.reduced: Optional[np.ndarray] = None\n",
    "\n",
    "    def prepare(self) -> None:\n",
    "        # 1) 데이터 로드 & 저장\n",
    "        df = load_and_validate_csv(self.config[\"data\"][\"raw_file\"])\n",
    "        # 2) 전처리\n",
    "        df = self.dp.preprocess_data(df)\n",
    "        self.dp.fit_encoders(df)\n",
    "        self.df = df\n",
    "        # 3) 임베딩 (캐시)\n",
    "        texts = self.eg.build_texts(df, self.config[\"text\"][\"fields_for_embedding\"])\n",
    "        self.embeddings = self.eg.generate_embeddings(texts, self.config[\"data\"][\"embeddings_file\"])\n",
    "        # 4) 차원축소\n",
    "        method = self.config[\"model\"][\"dimensionality_reduction\"]\n",
    "        n_comp = self.config[\"model\"][\"reduced_dim\"]\n",
    "        self.eg.fit_dimension_reducer(self.embeddings, method=method, n_components=n_comp)\n",
    "        self.reduced = self.eg.reduce_dimensions(self.embeddings)\n",
    "        np.save(self.config[\"data\"][\"reduced_embeddings_file\"], self.reduced)\n",
    "        # 5) 분류기 학습\n",
    "        y = self.dp.encode_labels(df)\n",
    "        self.trainer = XGBoostTrainer(self.config[\"model\"][\"xgb_params\"])\n",
    "        self.trainer.train_models(self.reduced, y)\n",
    "\n",
    "    def recommend_places(self, query_text: str, top_k: int = 10) -> pd.DataFrame:\n",
    "        if self.df is None or self.reduced is None:\n",
    "            raise RuntimeError(\"모델이 준비되지 않았습니다. prepare()를 먼저 호출하세요.\")\n",
    "        # 쿼리 임베딩 → 차원축소\n",
    "        q_emb = self.eg.generate_embeddings([query_text], cache_path=\"data/embeddings/_tmp_query.npy\")\n",
    "        q_red = self.eg.reduce_dimensions(q_emb)[0]\n",
    "        # 코사인 유사도\n",
    "        a = self.reduced / (np.linalg.norm(self.reduced, axis=1, keepdims=True) + 1e-9)\n",
    "        b = q_red / (np.linalg.norm(q_red) + 1e-9)\n",
    "        sims = a @ b\n",
    "        idx = np.argsort(-sims)[:top_k]\n",
    "        out = self.df.iloc[idx].copy()\n",
    "        out[\"score\"] = sims[idx]\n",
    "        return out.reset_index(drop=True)\n",
    "\n",
    "    def recommend_places_api(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        query = payload.get(\"q\") or payload.get(\"query\") or \"\"\n",
    "        k = int(payload.get(\"k\", 10))\n",
    "        rec = self.recommend_places(query, top_k=k)\n",
    "        return {\"results\": rec.to_dict(orient=\"records\")}\n",
    "\n",
    "# --- Benchmark / Demo --------------------------------------------------------\n",
    "def _split_y_dict(y_dict, idx_train, idx_test):\n",
    "    y_tr, y_te = {}, {}\n",
    "    for tag, y in y_dict.items():\n",
    "        y_tr[tag] = y[idx_train]\n",
    "        y_te[tag] = y[idx_test]\n",
    "    return y_tr, y_te\n",
    "\n",
    "def benchmark_pipeline(config_override=None, demo_queries=None, top_k=5):\n",
    "    \"\"\"\n",
    "    파이프라인 준비 시간, 간단 홀드아웃 정확도, 추천 API 응답 시간까지 측정.\n",
    "    - 기존 함수/클래스 이름은 변경하지 않음.\n",
    "    \"\"\"\n",
    "    cfg = config_override or config\n",
    "\n",
    "    # 1) 준비(임베딩+차원축소+학습) 시간\n",
    "    t0 = time.perf_counter()\n",
    "    rec = GangwonPlaceRecommender(cfg)\n",
    "    rec.prepare()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # 2) 간단 홀드아웃 평가 (별도 트레이너로 재학습/평가)\n",
    "    y_full = rec.dp.encode_labels(rec.df)\n",
    "    idx = list(range(len(rec.reduced)))\n",
    "    idx_tr, idx_te = train_test_split(idx, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "    X_tr = rec.reduced[idx_tr]\n",
    "    X_te = rec.reduced[idx_te]\n",
    "    y_tr, y_te = _split_y_dict(y_full, idx_tr, idx_te)\n",
    "\n",
    "    trainer = XGBoostTrainer(cfg[\"model\"][\"xgb_params\"])\n",
    "    t2 = time.perf_counter()\n",
    "    trainer.train_models(X_tr, y_tr)\n",
    "    t3 = time.perf_counter()\n",
    "    report = trainer.evaluate_models(X_te, y_te)\n",
    "\n",
    "    # 3) 추천 API 데모 및 응답 시간\n",
    "    queries = demo_queries or [\"가을 감성 바다 카페\", \"가족 산책 코스\", \"스릴 액티비티\", \"역사 유적\"]\n",
    "    rec_times = []\n",
    "    rec_examples = []\n",
    "    for q in queries:\n",
    "        s = time.perf_counter()\n",
    "        df_rec = rec.recommend_places(q, top_k=top_k)\n",
    "        e = time.perf_counter()\n",
    "        rec_times.append({\"query\": q, \"elapsed_sec\": e - s})\n",
    "        rec_examples.append((q, df_rec[[\"name\",\"address\",\"score\"]].head(top_k)))\n",
    "\n",
    "    summary = {\n",
    "        \"prepare_time_sec\": t1 - t0,\n",
    "        \"retrain_time_sec\": t3 - t2,\n",
    "        \"eval_report\": report,\n",
    "        \"recommend_times\": rec_times,\n",
    "        \"examples\": rec_examples\n",
    "    }\n",
    "    return rec, summary\n",
    "\n",
    "# --- Main (optional) ---------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 벤치마크 실행\n",
    "    rec, summary = benchmark_pipeline()\n",
    "\n",
    "    print(f\"✅ 준비(임베딩+차원축소+학습) 시간: {summary['prepare_time_sec']:.2f}s\")\n",
    "    print(f\"✅ 홀드아웃 재학습 시간: {summary['retrain_time_sec']:.2f}s\")\n",
    "    print(\"✅ 평가 리포트(마이크로/샘플 F1):\")\n",
    "    for tag, rpt in summary[\"eval_report\"].items():\n",
    "        print(f\"  - {tag}: micro_f1={rpt['micro_f1']:.4f}, samples_f1={rpt['samples_f1']:.4f}\")\n",
    "\n",
    "    # 추천 질의 응답 시간(ms) 표\n",
    "    rec_times_df = pd.DataFrame(summary[\"recommend_times\"]).assign(ms=lambda d: d[\"elapsed_sec\"]*1000)\\\n",
    "                                                          .drop(columns=[\"elapsed_sec\"])\n",
    "    print(\"\\n[추천 질의 응답 시간(ms)]\")\n",
    "    print(rec_times_df.to_string(index=False))\n",
    "\n",
    "    # 결과 저장\n",
    "    rec_times_path = \"outputs/recommend_latency_ms.csv\"\n",
    "    rec_times_df.to_csv(rec_times_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"📁 저장: {rec_times_path}\")\n",
    "\n",
    "    # 각 예시 질의의 Top-K 추천 미리보기 테이블 표시 + 저장\n",
    "    for q, df_topk in summary[\"examples\"]:\n",
    "        print(f\"\\n[{q}] Top 추천 미리보기\")\n",
    "        print(df_topk.to_string(index=False))\n",
    "        out_path = f\"outputs/preview_{q.replace(' ','_')}.csv\"\n",
    "        df_topk.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"📁 저장: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0750e3-180e-4b56-9982-81252e481e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
