{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00861740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdwl\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tjdwl\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
    "import os, sys, json, math, time, warnings\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "try:\n",
    "    import umap  # Optional\n",
    "except Exception:\n",
    "    umap = None\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    SentenceTransformer = None\n",
    "    warnings.warn(f\"sentence-transformers ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    XGBClassifier = None\n",
    "    warnings.warn(f\"xgboost ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ë””ë ‰í„°ë¦¬ ì¤€ë¹„\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "os.makedirs('models/encoders', exist_ok=True)\n",
    "os.makedirs('models/checkpoints', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed782e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ì„¤ì • íŒŒì¼ ìƒì„±\n",
    "config: Dict[str, Any] = {\n",
    "    \"data\": {\n",
    "        \"raw_file\": \"data/raw/gangwon_places_1000.xlsx\",\n",
    "        \"processed_file\": \"data/processed/gangwon_places_1000.csv\",\n",
    "        \"embeddings_file\": \"data/embeddings/place_embeddings.npy\",\n",
    "        \"reduced_embeddings_file\": \"data/embeddings/place_embeddings_pca128.npy\",\n",
    "    },\n",
    "    \"text\": {\n",
    "        \"fields_for_embedding\": [\"name\", \"short_description\"],\n",
    "        \"max_tokens_per_field\": 64\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"sbert_model\": \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\",\n",
    "        \"dimensionality_reduction\": \"PCA\",   # 'PCA' or 'UMAP'\n",
    "        \"reduced_dim\": 128,\n",
    "        \"xgb_params\": {\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.08,\n",
    "            \"subsample\": 0.9,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "            \"reg_lambda\": 1.0,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"n_jobs\": -1,\n",
    "            \"tree_method\": \"hist\"\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948de1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"name\",\"season\",\"nature\",\"vibe\",\"target\",\"fee\",\"parking\",\"address\",\n",
    "    \"open_time\",\"latitude\",\"longitude\",\"full_address\",\"short_description\"\n",
    "]\n",
    "\n",
    "def _split_labels(raw: Any) -> List[str]:\n",
    "    if pd.isna(raw):\n",
    "        return []\n",
    "    s = str(raw).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    for sep in [',',';','/','|']:\n",
    "        s = s.replace(sep, ' ')\n",
    "    parts = [p.strip() for p in s.split() if p.strip()]\n",
    "    return parts\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, max_labels_per_tag: int = 5):\n",
    "        self.max_labels_per_tag = max_labels_per_tag\n",
    "        self.encoders: Dict[str, MultiLabelBinarizer] = {}\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        missing = [c for c in EXPECTED_COLUMNS if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜: ëˆ„ë½ ì»¬ëŸ¼ {missing}\")\n",
    "        df = df[EXPECTED_COLUMNS].copy()\n",
    "\n",
    "        for c in [\"name\",\"short_description\",\"address\",\"full_address\",\"open_time\",\"fee\",\"parking\",\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "        for c in [\"latitude\",\"longitude\"]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "        df.drop_duplicates(subset=[\"name\",\"address\"], inplace=True)\n",
    "\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            df[tag] = df[tag].apply(_split_labels).apply(lambda arr: arr[: self.max_labels_per_tag])\n",
    "        return df\n",
    "\n",
    "    def fit_encoders(self, df: pd.DataFrame) -> None:\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit(df[tag].tolist())\n",
    "            self.encoders[tag] = mlb\n",
    "        for tag, mlb in self.encoders.items():\n",
    "            np.save(f\"models/encoders/{tag}_classes.npy\", mlb.classes_)\n",
    "\n",
    "    def load_encoders(self) -> None:\n",
    "        enc = {}\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            path = f\"models/encoders/{tag}_classes.npy\"\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"ì¸ì½”ë” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
    "            classes = np.load(path, allow_pickle=True)\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit([classes.tolist()])\n",
    "            enc[tag] = mlb\n",
    "        self.encoders = enc\n",
    "\n",
    "    def encode_labels(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        if not self.encoders:\n",
    "            self.load_encoders()\n",
    "        y = {}\n",
    "        for tag, mlb in self.encoders.items():\n",
    "            y[tag] = mlb.transform(df[tag].tolist())\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125e2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ì„ë² ë”© ìƒì„± í´ë˜ìŠ¤ ì •ì˜\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: Optional[str] = None):\n",
    "        self.model_name = model_name or config[\"model\"][\"sbert_model\"]\n",
    "        self.model = None\n",
    "        self.dimension_reducer = None\n",
    "        self.reduced_dim = config[\"model\"][\"reduced_dim\"]\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model is not None:\n",
    "            return\n",
    "        if SentenceTransformer is None:\n",
    "            raise RuntimeError(\"SentenceTransformerë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "    def _concat_text_fields(self, row: pd.Series, fields: List[str]) -> str:\n",
    "        parts = []\n",
    "        for f in fields:\n",
    "            val = str(row.get(f, \"\")).strip()\n",
    "            if val and val.lower() != \"nan\":\n",
    "                parts.append(val)\n",
    "        return \" \".join(parts)\n",
    "\n",
    "    def build_texts(self, df: pd.DataFrame, fields: Optional[List[str]] = None) -> List[str]:\n",
    "        fields = fields or config[\"text\"][\"fields_for_embedding\"]\n",
    "        return [self._concat_text_fields(row, fields) for _, row in df.iterrows()]\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str], cache_path: str) -> np.ndarray:\n",
    "        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "        if os.path.exists(cache_path):\n",
    "            embs = np.load(cache_path)\n",
    "            print(f\"ğŸ” ì„ë² ë”© ìºì‹œ ë¡œë“œ: {cache_path} {embs.shape}\")\n",
    "            return embs\n",
    "        self.load_model()\n",
    "        embs = self.model.encode(texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        np.save(cache_path, embs)\n",
    "        print(f\"âœ… ì„ë² ë”© ì €ì¥: {cache_path} {embs.shape}\")\n",
    "        return embs\n",
    "\n",
    "    def fit_dimension_reducer(self, X: np.ndarray, method: str = \"PCA\", n_components: int = 128):\n",
    "        method = (method or \"PCA\").upper()\n",
    "        if method == \"UMAP\":\n",
    "            if umap is None:\n",
    "                warnings.warn(\"UMAPì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ PCAë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "            else:\n",
    "                self.dimension_reducer = umap.UMAP(\n",
    "                    n_components=n_components, n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=RANDOM_STATE\n",
    "                )\n",
    "                self.reduced_dim = n_components\n",
    "                self.dimension_reducer.fit(X)\n",
    "                return\n",
    "        self.dimension_reducer = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "        self.reduced_dim = n_components\n",
    "        self.dimension_reducer.fit(X)\n",
    "\n",
    "    def reduce_dimensions(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.dimension_reducer is None:\n",
    "            raise RuntimeError(\"dimension_reducerê°€ ì—†ìŠµë‹ˆë‹¤. fit_dimension_reducer ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.\")\n",
    "        return self.dimension_reducer.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b5b7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ì‹¤ì œ csv íŒŒì¼ ë¡œë“œ ë° ê²€ì¦\n",
    "def load_and_validate_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"XLSX/CSV ìë™ íŒë³„ + 13ì»¬ëŸ¼ ê²€ì¦ + processed ì €ì¥\"\"\"\n",
    "    candidates = [\n",
    "        config[\"data\"][\"raw_file\"],\n",
    "        \"data/raw/gangwon_places_1000.xlsx\",\n",
    "        file_path\n",
    "    ]\n",
    "    src = next((p for p in candidates if p and os.path.exists(p)), None)\n",
    "    if src is None:\n",
    "        raise FileNotFoundError(\"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. data/rawì— 1000ê°œ íŒŒì¼ì„ ë‘ì„¸ìš”.\")\n",
    "\n",
    "    if src.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df = pd.read_excel(src)\n",
    "    else:\n",
    "        df = pd.read_csv(src)\n",
    "\n",
    "    missing = [c for c in EXPECTED_COLUMNS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜: ëˆ„ë½ ì»¬ëŸ¼ {missing}\")\n",
    "    df = df[EXPECTED_COLUMNS].copy()\n",
    "\n",
    "    out = config[\"data\"][\"processed_file\"]\n",
    "    os.makedirs(os.path.dirname(out), exist_ok=True)\n",
    "    df.to_csv(out, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ë°ì´í„° ì €ì¥: {out} ({len(df):,} rows)\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15a9f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## XGBoost íŠ¸ë ˆì´ë„ˆ\n",
    "class XGBoostTrainer:\n",
    "    def __init__(self, params: Optional[Dict[str, Any]] = None):\n",
    "        if XGBClassifier is None:\n",
    "            raise RuntimeError(\"XGBoostë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        self.params = params or config[\"model\"][\"xgb_params\"]\n",
    "        self.models: Dict[str, Any] = {}\n",
    "\n",
    "    def train_models(self, X: np.ndarray, y_dict: Dict[str, np.ndarray]) -> None:\n",
    "        self.models = {}\n",
    "        for tag, y in y_dict.items():\n",
    "            clf = XGBClassifier(**self.params)\n",
    "            X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=(y.sum(axis=1)>0)\n",
    "            )\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            self.models[tag] = clf\n",
    "\n",
    "    def evaluate_models(self, X_test: np.ndarray, y_true: Dict[str, np.ndarray]) -> Dict[str, Any]:\n",
    "        report = {}\n",
    "        for tag, clf in self.models.items():\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                y_pred = (clf.predict_proba(X_test) > 0.5).astype(int)\n",
    "            else:\n",
    "                y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "            micro_f1 = f1_score(y_true[tag], y_pred, average=\"micro\", zero_division=0)\n",
    "            samples_f1 = f1_score(y_true[tag], y_pred, average=\"samples\", zero_division=0)\n",
    "            report[tag] = {\"micro_f1\": micro_f1, \"samples_f1\": samples_f1}\n",
    "        return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b403ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ì¶”ì²œê¸° (ê³µê°œ ë©”ì„œë“œ ì´ë¦„ ìœ ì§€)\n",
    "class GangwonPlaceRecommender:\n",
    "    def __init__(self, cfg: Dict[str, Any] = None):\n",
    "        self.config = cfg or config\n",
    "        self.dp = DataPreprocessor()\n",
    "        self.eg = EmbeddingGenerator()\n",
    "        self.trainer = None\n",
    "        self.df: Optional[pd.DataFrame] = None\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        self.reduced: Optional[np.ndarray] = None\n",
    "\n",
    "    def prepare(self) -> None:\n",
    "        df = load_and_validate_csv(self.config[\"data\"][\"raw_file\"])\n",
    "        df = self.dp.preprocess_data(df)\n",
    "        self.dp.fit_encoders(df)\n",
    "        self.df = df\n",
    "\n",
    "        texts = self.eg.build_texts(df, self.config[\"text\"][\"fields_for_embedding\"])\n",
    "        self.embeddings = self.eg.generate_embeddings(texts, self.config[\"data\"][\"embeddings_file\"])\n",
    "\n",
    "        method = self.config[\"model\"][\"dimensionality_reduction\"]\n",
    "        n_comp = self.config[\"model\"][\"reduced_dim\"]\n",
    "        self.eg.fit_dimension_reducer(self.embeddings, method=method, n_components=n_comp)\n",
    "        self.reduced = self.eg.reduce_dimensions(self.embeddings)\n",
    "        np.save(self.config[\"data\"][\"reduced_embeddings_file\"], self.reduced)\n",
    "\n",
    "        y = self.dp.encode_labels(df)\n",
    "        self.trainer = XGBoostTrainer(self.config[\"model\"][\"xgb_params\"])\n",
    "        self.trainer.train_models(self.reduced, y)\n",
    "\n",
    "    def recommend_places(self, query_text: str, top_k: int = 10) -> pd.DataFrame:\n",
    "        if self.df is None or self.reduced is None:\n",
    "            raise RuntimeError(\"ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. prepare()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.\")\n",
    "        q_emb = self.eg.generate_embeddings([query_text], cache_path=\"data/embeddings/_tmp_query.npy\")\n",
    "        q_red = self.eg.reduce_dimensions(q_emb)[0]\n",
    "\n",
    "        a = self.reduced / (np.linalg.norm(self.reduced, axis=1, keepdims=True) + 1e-9)\n",
    "        b = q_red / (np.linalg.norm(q_red) + 1e-9)\n",
    "        sims = a @ b\n",
    "\n",
    "        idx = np.argsort(-sims)[:top_k]\n",
    "        out = self.df.iloc[idx].copy()\n",
    "        out[\"score\"] = sims[idx]\n",
    "        return out.reset_index(drop=True)\n",
    "\n",
    "    def recommend_places_api(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        query = payload.get(\"q\") or payload.get(\"query\") or \"\"\n",
    "        k = int(payload.get(\"k\", 10))\n",
    "        rec = self.recommend_places(query, top_k=k)\n",
    "        return {\"results\": rec.to_dict(orient=\"records\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ea2ed8e-adc8-4624-a563-4a439e91175d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'benchmark_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m rec, summary \u001b[38;5;241m=\u001b[39m benchmark_pipeline()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ì¤€ë¹„(ì„ë² ë”©+ì°¨ì›ì¶•ì†Œ+í•™ìŠµ) ì‹œê°„: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprepare_time_sec\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… í™€ë“œì•„ì›ƒ ì¬í•™ìŠµ ì‹œê°„: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretrain_time_sec\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'benchmark_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "## ì‹¤í–‰ ì˜ˆì‹œ: ë²¤ì¹˜ë§ˆí¬ + ì¶”ì²œ ê²°ê³¼ ì¶œë ¥ (í‘œì¤€ Jupyter í‘œì‹œ)\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\n",
    "rec, summary = benchmark_pipeline()\n",
    "\n",
    "print(f\"âœ… ì¤€ë¹„(ì„ë² ë”©+ì°¨ì›ì¶•ì†Œ+í•™ìŠµ) ì‹œê°„: {summary['prepare_time_sec']:.2f}s\")\n",
    "print(f\"âœ… í™€ë“œì•„ì›ƒ ì¬í•™ìŠµ ì‹œê°„: {summary['retrain_time_sec']:.2f}s\")\n",
    "print(\"âœ… í‰ê°€ ë¦¬í¬íŠ¸(ë§ˆì´í¬ë¡œ/ìƒ˜í”Œ F1):\")\n",
    "for tag, rpt in summary[\"eval_report\"].items():\n",
    "    print(f\"  - {tag}: micro_f1={rpt['micro_f1']:.4f}, samples_f1={rpt['samples_f1']:.4f}\")\n",
    "\n",
    "# ì¶”ì²œ ì§ˆì˜ ì‘ë‹µ ì‹œê°„(ms) í‘œ\n",
    "rec_times_df = pd.DataFrame(summary[\"recommend_times\"]).assign(ms=lambda d: d[\"elapsed_sec\"]*1000)\\\n",
    "                                                      .drop(columns=[\"elapsed_sec\"])\n",
    "print(\"\\n[ì¶”ì²œ ì§ˆì˜ ì‘ë‹µ ì‹œê°„(ms)]\")\n",
    "display(rec_times_df)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥(ì›í•˜ë©´ ì²¨ë¶€/ê³µìœ  ê°€ëŠ¥)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "rec_times_path = \"outputs/recommend_latency_ms.csv\"\n",
    "rec_times_df.to_csv(rec_times_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"ğŸ“ ì €ì¥: {rec_times_path}\")\n",
    "\n",
    "# ê° ì˜ˆì‹œ ì§ˆì˜ì˜ Top-K ì¶”ì²œ ë¯¸ë¦¬ë³´ê¸° í…Œì´ë¸” í‘œì‹œ + ì €ì¥\n",
    "for q, df_topk in summary[\"examples\"]:\n",
    "    print(f\"\\n[{q}] Top ì¶”ì²œ ë¯¸ë¦¬ë³´ê¸°\")\n",
    "    display(df_topk)\n",
    "    out_path = f\"outputs/preview_{q.replace(' ','_')}.csv\"\n",
    "    df_topk.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"ğŸ“ ì €ì¥: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "703c65a9-b13b-47de-987a-bb5c282b8d04",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'caas_jupyter_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## ì‹¤í–‰ ì˜ˆì‹œ: ë²¤ì¹˜ë§ˆí¬ + ì¶”ì²œ ê²°ê³¼ ì¶œë ¥\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaas_jupyter_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display_dataframe_to_user\n\u001b[0;32m      3\u001b[0m rec, summary \u001b[38;5;241m=\u001b[39m benchmark_pipeline()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ì¤€ë¹„(ì„ë² ë”©+ì°¨ì›ì¶•ì†Œ+í•™ìŠµ) ì‹œê°„: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprepare_time_sec\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'caas_jupyter_tools'"
     ]
    }
   ],
   "source": [
    "## ì‹¤í–‰ ì˜ˆì‹œ: ë²¤ì¹˜ë§ˆí¬ + ì¶”ì²œ ê²°ê³¼ ì¶œë ¥\n",
    "from caas_jupyter_tools import display_dataframe_to_user\n",
    "rec, summary = benchmark_pipeline()\n",
    "\n",
    "print(f\"âœ… ì¤€ë¹„(ì„ë² ë”©+ì°¨ì›ì¶•ì†Œ+í•™ìŠµ) ì‹œê°„: {summary['prepare_time_sec']:.2f}s\")\n",
    "print(f\"âœ… í™€ë“œì•„ì›ƒ ì¬í•™ìŠµ ì‹œê°„: {summary['retrain_time_sec']:.2f}s\")\n",
    "print(\"âœ… í‰ê°€ ë¦¬í¬íŠ¸(ë§ˆì´í¬ë¡œ/ìƒ˜í”Œ F1):\")\n",
    "for tag, rpt in summary[\"eval_report\"].items():\n",
    "    print(f\"  - {tag}: micro_f1={rpt['micro_f1']:.4f}, samples_f1={rpt['samples_f1']:.4f}\")\n",
    "\n",
    "import pandas as pd\n",
    "rec_times_df = pd.DataFrame(summary[\"recommend_times\"])\n",
    "display_dataframe_to_user(\"ì¶”ì²œ ì§ˆì˜ ì‘ë‹µ ì‹œê°„(ms)\", (rec_times_df.assign(ms=(rec_times_df[\"elapsed_sec\"]*1000)).drop(columns=[\"elapsed_sec\"])))\n",
    "\n",
    "# ê° ì˜ˆì‹œ ì¿¼ë¦¬ì˜ Top-K ì¶”ì²œ ë¯¸ë¦¬ë³´ê¸° í…Œì´ë¸”ë„ ë„ìš°ê¸°\n",
    "for q, df in summary[\"examples\"]:\n",
    "    display_dataframe_to_user(f\"[{q}] Top ì¶”ì²œ ë¯¸ë¦¬ë³´ê¸°\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c74853ec-fbbc-4ed4-8087-e6a96633b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ì €ì¥: data/processed/gangwon_places_1000.csv (1,000 rows)\n",
      "ğŸ” ì„ë² ë”© ìºì‹œ ë¡œë“œ: data/embeddings/place_embeddings.npy (923, 768)\n",
      "ğŸ” ì„ë² ë”© ìºì‹œ ë¡œë“œ: data/embeddings/_tmp_query.npy (1, 768)\n",
      "ğŸ” ì„ë² ë”© ìºì‹œ ë¡œë“œ: data/embeddings/_tmp_query.npy (1, 768)\n",
      "ğŸ” ì„ë² ë”© ìºì‹œ ë¡œë“œ: data/embeddings/_tmp_query.npy (1, 768)\n",
      "ğŸ” ì„ë² ë”© ìºì‹œ ë¡œë“œ: data/embeddings/_tmp_query.npy (1, 768)\n",
      "âœ… ì¤€ë¹„(ì„ë² ë”©+ì°¨ì›ì¶•ì†Œ+í•™ìŠµ) ì‹œê°„: 25.60s\n",
      "âœ… í™€ë“œì•„ì›ƒ ì¬í•™ìŠµ ì‹œê°„: 20.25s\n",
      "âœ… í‰ê°€ ë¦¬í¬íŠ¸(ë§ˆì´í¬ë¡œ/ìƒ˜í”Œ F1):\n",
      "  - season: micro_f1=0.9426, samples_f1=0.9315\n",
      "  - nature: micro_f1=0.7083, samples_f1=0.6807\n",
      "  - vibe: micro_f1=0.8262, samples_f1=0.8568\n",
      "  - target: micro_f1=0.9100, samples_f1=0.8973\n",
      "\n",
      "[ì¶”ì²œ ì§ˆì˜ ì‘ë‹µ ì‹œê°„(ms)]\n",
      "      query     ms\n",
      "ê°€ì„ ê°ì„± ë°”ë‹¤ ì¹´í˜ 8.8387\n",
      "   ê°€ì¡± ì‚°ì±… ì½”ìŠ¤ 3.6357\n",
      "    ìŠ¤ë¦´ ì•¡í‹°ë¹„í‹° 2.6373\n",
      "      ì—­ì‚¬ ìœ ì  4.5844\n",
      "ğŸ“ ì €ì¥: outputs/recommend_latency_ms.csv\n",
      "\n",
      "[ê°€ì„ ê°ì„± ë°”ë‹¤ ì¹´í˜] Top ì¶”ì²œ ë¯¸ë¦¬ë³´ê¸°\n",
      "       name                      address    score\n",
      "   ê²½í¬ ì•„ì¿ ì•„ë¦¬ì›€         ê°•ì›íŠ¹ë³„ìì¹˜ë„ ê°•ë¦‰ì‹œ ë‚œì„¤í—Œë¡œ 131 0.508496\n",
      "      ìì‘ë„í•´ë³€  ê°•ì›íŠ¹ë³„ìì¹˜ë„ ê³ ì„±êµ° ì£½ì™•ë©´ ìì‘ë„ì„ ì‚¬ê¸¸(ì£½ì™•ë©´) 0.507921\n",
      "   ì‘ì€í›„ì§„í•´ìˆ˜ìš•ì¥    ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì‚¼ì²™ì‹œ ìƒˆì²œë…„ë„ë¡œ 467(êµë™) 0.499675\n",
      "ì ë¹„ì¹˜ì‚¼ì²™ ì˜¤ì…˜í”Œë ˆì´  ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì‚¼ì²™ì‹œ ìˆ˜ë¡œë¶€ì¸ê¸¸ 453 (ê°ˆì²œë™) 0.458971\n",
      "      ê°¯ë§ˆì„í•´ë³€ ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì–‘ì–‘êµ° í˜„ë‚¨ë©´ ì•ˆë‚¨ì• ê¸¸ 48(ë‚¨ì• ë¦¬) 0.456861\n",
      "ğŸ“ ì €ì¥: outputs/preview_ê°€ì„_ê°ì„±_ë°”ë‹¤_ì¹´í˜.csv\n",
      "\n",
      "[ê°€ì¡± ì‚°ì±… ì½”ìŠ¤] Top ì¶”ì²œ ë¯¸ë¦¬ë³´ê¸°\n",
      "       name                      address    score\n",
      "   ê²½í¬ ì•„ì¿ ì•„ë¦¬ì›€         ê°•ì›íŠ¹ë³„ìì¹˜ë„ ê°•ë¦‰ì‹œ ë‚œì„¤í—Œë¡œ 131 0.508496\n",
      "      ìì‘ë„í•´ë³€  ê°•ì›íŠ¹ë³„ìì¹˜ë„ ê³ ì„±êµ° ì£½ì™•ë©´ ìì‘ë„ì„ ì‚¬ê¸¸(ì£½ì™•ë©´) 0.507921\n",
      "   ì‘ì€í›„ì§„í•´ìˆ˜ìš•ì¥    ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì‚¼ì²™ì‹œ ìƒˆì²œë…„ë„ë¡œ 467(êµë™) 0.499675\n",
      "ì ë¹„ì¹˜ì‚¼ì²™ ì˜¤ì…˜í”Œë ˆì´  ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì‚¼ì²™ì‹œ ìˆ˜ë¡œë¶€ì¸ê¸¸ 453 (ê°ˆì²œë™) 0.458971\n",
      "      ê°¯ë§ˆì„í•´ë³€ ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì–‘ì–‘êµ° í˜„ë‚¨ë©´ ì•ˆë‚¨ì• ê¸¸ 48(ë‚¨ì• ë¦¬) 0.456861\n",
      "ğŸ“ ì €ì¥: outputs/preview_ê°€ì¡±_ì‚°ì±…_ì½”ìŠ¤.csv\n",
      "\n",
      "[ìŠ¤ë¦´ ì•¡í‹°ë¹„í‹°] Top ì¶”ì²œ ë¯¸ë¦¬ë³´ê¸°\n",
      "       name                      address    score\n",
      "   ê²½í¬ ì•„ì¿ ì•„ë¦¬ì›€         ê°•ì›íŠ¹ë³„ìì¹˜ë„ ê°•ë¦‰ì‹œ ë‚œì„¤í—Œë¡œ 131 0.508496\n",
      "      ìì‘ë„í•´ë³€  ê°•ì›íŠ¹ë³„ìì¹˜ë„ ê³ ì„±êµ° ì£½ì™•ë©´ ìì‘ë„ì„ ì‚¬ê¸¸(ì£½ì™•ë©´) 0.507921\n",
      "   ì‘ì€í›„ì§„í•´ìˆ˜ìš•ì¥    ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì‚¼ì²™ì‹œ ìƒˆì²œë…„ë„ë¡œ 467(êµë™) 0.499675\n",
      "ì ë¹„ì¹˜ì‚¼ì²™ ì˜¤ì…˜í”Œë ˆì´  ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì‚¼ì²™ì‹œ ìˆ˜ë¡œë¶€ì¸ê¸¸ 453 (ê°ˆì²œë™) 0.458971\n",
      "      ê°¯ë§ˆì„í•´ë³€ ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì–‘ì–‘êµ° í˜„ë‚¨ë©´ ì•ˆë‚¨ì• ê¸¸ 48(ë‚¨ì• ë¦¬) 0.456861\n",
      "ğŸ“ ì €ì¥: outputs/preview_ìŠ¤ë¦´_ì•¡í‹°ë¹„í‹°.csv\n",
      "\n",
      "[ì—­ì‚¬ ìœ ì ] Top ì¶”ì²œ ë¯¸ë¦¬ë³´ê¸°\n",
      "       name                      address    score\n",
      "   ê²½í¬ ì•„ì¿ ì•„ë¦¬ì›€         ê°•ì›íŠ¹ë³„ìì¹˜ë„ ê°•ë¦‰ì‹œ ë‚œì„¤í—Œë¡œ 131 0.508496\n",
      "      ìì‘ë„í•´ë³€  ê°•ì›íŠ¹ë³„ìì¹˜ë„ ê³ ì„±êµ° ì£½ì™•ë©´ ìì‘ë„ì„ ì‚¬ê¸¸(ì£½ì™•ë©´) 0.507921\n",
      "   ì‘ì€í›„ì§„í•´ìˆ˜ìš•ì¥    ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì‚¼ì²™ì‹œ ìƒˆì²œë…„ë„ë¡œ 467(êµë™) 0.499675\n",
      "ì ë¹„ì¹˜ì‚¼ì²™ ì˜¤ì…˜í”Œë ˆì´  ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì‚¼ì²™ì‹œ ìˆ˜ë¡œë¶€ì¸ê¸¸ 453 (ê°ˆì²œë™) 0.458971\n",
      "      ê°¯ë§ˆì„í•´ë³€ ê°•ì›íŠ¹ë³„ìì¹˜ë„ ì–‘ì–‘êµ° í˜„ë‚¨ë©´ ì•ˆë‚¨ì• ê¸¸ 48(ë‚¨ì• ë¦¬) 0.456861\n",
      "ğŸ“ ì €ì¥: outputs/preview_ì—­ì‚¬_ìœ ì .csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Gangwon Recommendation System â€” Upgraded (1000ê°œ + ì°¨ì›ì¶•ì†Œ + ë²¤ì¹˜ë§ˆí¬)\n",
    "# â€» í•¨ìˆ˜/í´ë˜ìŠ¤/ê³µê°œ ë©”ì„œë“œ ì´ë¦„ì€ ê¸°ì¡´ê³¼ í˜¸í™˜ë˜ë„ë¡ ìœ ì§€\n",
    "# ============================================================\n",
    "\n",
    "# --- Imports / Dirs / Constants --------------------------------------------\n",
    "import os, sys, json, math, time, warnings\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "try:\n",
    "    import umap  # Optional\n",
    "except Exception:\n",
    "    umap = None\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    SentenceTransformer = None\n",
    "    warnings.warn(f\"sentence-transformers ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    XGBClassifier = None\n",
    "    warnings.warn(f\"xgboost ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# ë””ë ‰í„°ë¦¬ ì¤€ë¹„\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "os.makedirs('models/encoders', exist_ok=True)\n",
    "os.makedirs('models/checkpoints', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# --- Config -----------------------------------------------------------------\n",
    "config: Dict[str, Any] = {\n",
    "    \"data\": {\n",
    "        \"raw_file\": \"data/raw/gangwon_places_1000.xlsx\",          # ìš°ì„  ì‚¬ìš©\n",
    "        \"processed_file\": \"data/processed/gangwon_places_1000.csv\",\n",
    "        \"embeddings_file\": \"data/embeddings/place_embeddings.npy\",\n",
    "        \"reduced_embeddings_file\": \"data/embeddings/place_embeddings_pca128.npy\",\n",
    "    },\n",
    "    \"text\": {\n",
    "        \"fields_for_embedding\": [\"name\", \"short_description\"],     # ì„ë² ë”©ì— ì‚¬ìš©í•  í•„ë“œ\n",
    "        \"max_tokens_per_field\": 64\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"sbert_model\": \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\",\n",
    "        \"dimensionality_reduction\": \"PCA\",   # 'PCA' or 'UMAP'\n",
    "        \"reduced_dim\": 128,\n",
    "        \"xgb_params\": {\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": 6,\n",
    "            \"learning_rate\": 0.08,\n",
    "            \"subsample\": 0.9,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "            \"reg_lambda\": 1.0,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"n_jobs\": -1,\n",
    "            \"tree_method\": \"hist\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Schema / Preprocessor ---------------------------------------------------\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"name\",\"season\",\"nature\",\"vibe\",\"target\",\"fee\",\"parking\",\"address\",\n",
    "    \"open_time\",\"latitude\",\"longitude\",\"full_address\",\"short_description\"\n",
    "]\n",
    "\n",
    "def _split_labels(raw: Any) -> List[str]:\n",
    "    if pd.isna(raw):\n",
    "        return []\n",
    "    s = str(raw).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    for sep in [',',';','/','|']:\n",
    "        s = s.replace(sep, ' ')\n",
    "    parts = [p.strip() for p in s.split() if p.strip()]\n",
    "    return parts\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, max_labels_per_tag: int = 5):\n",
    "        self.max_labels_per_tag = max_labels_per_tag\n",
    "        self.encoders: Dict[str, MultiLabelBinarizer] = {}\n",
    "\n",
    "    def preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # ìŠ¤í‚¤ë§ˆ ê²€ì¦/ì •ë ¬\n",
    "        missing = [c for c in EXPECTED_COLUMNS if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜: ëˆ„ë½ ì»¬ëŸ¼ {missing}\")\n",
    "        df = df[EXPECTED_COLUMNS].copy()\n",
    "\n",
    "        # ë¬¸ìì—´ í´ë¦°\n",
    "        for c in [\"name\",\"short_description\",\"address\",\"full_address\",\"open_time\",\"fee\",\"parking\",\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "        # ìœ„/ê²½ë„ ìˆ«ìí™”\n",
    "        for c in [\"latitude\",\"longitude\"]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "        # ì¤‘ë³µ ì œê±°\n",
    "        df.drop_duplicates(subset=[\"name\",\"address\"], inplace=True)\n",
    "\n",
    "        # ë¼ë²¨ íŒŒì‹±(ë‹¤ì¤‘ë¼ë²¨)\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            df[tag] = df[tag].apply(_split_labels).apply(lambda arr: arr[: self.max_labels_per_tag])\n",
    "        return df\n",
    "\n",
    "    def fit_encoders(self, df: pd.DataFrame) -> None:\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit(df[tag].tolist())\n",
    "            self.encoders[tag] = mlb\n",
    "        # ì €ì¥\n",
    "        for tag, mlb in self.encoders.items():\n",
    "            np.save(f\"models/encoders/{tag}_classes.npy\", mlb.classes_)\n",
    "\n",
    "    def load_encoders(self) -> None:\n",
    "        enc = {}\n",
    "        for tag in [\"season\",\"nature\",\"vibe\",\"target\"]:\n",
    "            path = f\"models/encoders/{tag}_classes.npy\"\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"ì¸ì½”ë” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {path}\")\n",
    "            classes = np.load(path, allow_pickle=True)\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit([classes.tolist()])  # ë™ì¼ í´ë˜ìŠ¤ ì„¸íŠ¸ë¡œ ë§ì¶¤\n",
    "            enc[tag] = mlb\n",
    "        self.encoders = enc\n",
    "\n",
    "    def encode_labels(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        if not self.encoders:\n",
    "            self.load_encoders()\n",
    "        y = {}\n",
    "        for tag, mlb in self.encoders.items():\n",
    "            y[tag] = mlb.transform(df[tag].tolist())\n",
    "        return y\n",
    "\n",
    "# --- Embedding Generator -----------------------------------------------------\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: Optional[str] = None):\n",
    "        self.model_name = model_name or config[\"model\"][\"sbert_model\"]\n",
    "        self.model = None\n",
    "        self.dimension_reducer = None\n",
    "        self.reduced_dim = config[\"model\"][\"reduced_dim\"]\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.model is not None:\n",
    "            return\n",
    "        if SentenceTransformer is None:\n",
    "            raise RuntimeError(\"SentenceTransformerë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "    def _concat_text_fields(self, row: pd.Series, fields: List[str]) -> str:\n",
    "        parts = []\n",
    "        for f in fields:\n",
    "            val = str(row.get(f, \"\")).strip()\n",
    "            if val and val.lower() != \"nan\":\n",
    "                parts.append(val)\n",
    "        return \" \".join(parts)\n",
    "\n",
    "    def build_texts(self, df: pd.DataFrame, fields: Optional[List[str]] = None) -> List[str]:\n",
    "        fields = fields or config[\"text\"][\"fields_for_embedding\"]\n",
    "        return [self._concat_text_fields(row, fields) for _, row in df.iterrows()]\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str], cache_path: str) -> np.ndarray:\n",
    "        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "        if os.path.exists(cache_path):\n",
    "            embs = np.load(cache_path)\n",
    "            print(f\"ğŸ” ì„ë² ë”© ìºì‹œ ë¡œë“œ: {cache_path} {embs.shape}\")\n",
    "            return embs\n",
    "        self.load_model()\n",
    "        embs = self.model.encode(texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        np.save(cache_path, embs)\n",
    "        print(f\"âœ… ì„ë² ë”© ì €ì¥: {cache_path} {embs.shape}\")\n",
    "        return embs\n",
    "\n",
    "    def fit_dimension_reducer(self, X: np.ndarray, method: str = \"PCA\", n_components: int = 128):\n",
    "        method = (method or \"PCA\").upper()\n",
    "        if method == \"UMAP\":\n",
    "            if umap is None:\n",
    "                warnings.warn(\"UMAPì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ PCAë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "            else:\n",
    "                self.dimension_reducer = umap.UMAP(\n",
    "                    n_components=n_components, n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=RANDOM_STATE\n",
    "                )\n",
    "                self.reduced_dim = n_components\n",
    "                self.dimension_reducer.fit(X)\n",
    "                return\n",
    "        # PCA fallback\n",
    "        self.dimension_reducer = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "        self.reduced_dim = n_components\n",
    "        self.dimension_reducer.fit(X)\n",
    "\n",
    "    def reduce_dimensions(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.dimension_reducer is None:\n",
    "            raise RuntimeError(\"dimension_reducerê°€ ì—†ìŠµë‹ˆë‹¤. fit_dimension_reducer ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.\")\n",
    "        return self.dimension_reducer.transform(X)\n",
    "\n",
    "# --- Data Loader -------------------------------------------------------------\n",
    "def load_and_validate_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"XLSX/CSV ìë™ íŒë³„ + 13ì»¬ëŸ¼ ê²€ì¦ + processed ì €ì¥\"\"\"\n",
    "    candidates = [\n",
    "        config[\"data\"][\"raw_file\"],\n",
    "        \"data/raw/gangwon_places_1000.csv\",\n",
    "        file_path\n",
    "    ]\n",
    "    src = next((p for p in candidates if p and os.path.exists(p)), None)\n",
    "    if src is None:\n",
    "        raise FileNotFoundError(\"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. data/rawì— 1000ê°œ íŒŒì¼ì„ ë‘ì„¸ìš”.\")\n",
    "\n",
    "    if src.lower().endswith((\".xlsx\", \".xls\")):\n",
    "        df = pd.read_excel(src)\n",
    "    else:\n",
    "        df = pd.read_csv(src)\n",
    "\n",
    "    missing = [c for c in EXPECTED_COLUMNS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜: ëˆ„ë½ ì»¬ëŸ¼ {missing}\")\n",
    "    df = df[EXPECTED_COLUMNS].copy()\n",
    "\n",
    "    out = config[\"data\"][\"processed_file\"]\n",
    "    os.makedirs(os.path.dirname(out), exist_ok=True)\n",
    "    df.to_csv(out, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ë°ì´í„° ì €ì¥: {out} ({len(df):,} rows)\")\n",
    "    return df\n",
    "\n",
    "# --- XGBoost Trainer ---------------------------------------------------------\n",
    "class XGBoostTrainer:\n",
    "    def __init__(self, params: Optional[Dict[str, Any]] = None):\n",
    "        if XGBClassifier is None:\n",
    "            raise RuntimeError(\"XGBoostë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        self.params = params or config[\"model\"][\"xgb_params\"]\n",
    "        self.models: Dict[str, Any] = {}\n",
    "\n",
    "    def train_models(self, X: np.ndarray, y_dict: Dict[str, np.ndarray]) -> None:\n",
    "        self.models = {}\n",
    "        for tag, y in y_dict.items():\n",
    "            clf = XGBClassifier(**self.params)\n",
    "            X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=(y.sum(axis=1)>0)\n",
    "            )\n",
    "            clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            self.models[tag] = clf\n",
    "\n",
    "    def evaluate_models(self, X_test: np.ndarray, y_true: Dict[str, np.ndarray]) -> Dict[str, Any]:\n",
    "        report = {}\n",
    "        for tag, clf in self.models.items():\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                y_pred = (clf.predict_proba(X_test) > 0.5).astype(int)\n",
    "            else:\n",
    "                y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "            micro_f1 = f1_score(y_true[tag], y_pred, average=\"micro\", zero_division=0)\n",
    "            samples_f1 = f1_score(y_true[tag], y_pred, average=\"samples\", zero_division=0)\n",
    "            report[tag] = {\"micro_f1\": micro_f1, \"samples_f1\": samples_f1}\n",
    "        return report\n",
    "\n",
    "# --- Recommender (Public API kept) ------------------------------------------\n",
    "class GangwonPlaceRecommender:\n",
    "    def __init__(self, cfg: Dict[str, Any] = None):\n",
    "        self.config = cfg or config\n",
    "        self.dp = DataPreprocessor()\n",
    "        self.eg = EmbeddingGenerator()\n",
    "        self.trainer = None\n",
    "        self.df: Optional[pd.DataFrame] = None\n",
    "        self.embeddings: Optional[np.ndarray] = None\n",
    "        self.reduced: Optional[np.ndarray] = None\n",
    "\n",
    "    def prepare(self) -> None:\n",
    "        # 1) ë°ì´í„° ë¡œë“œ & ì €ì¥\n",
    "        df = load_and_validate_csv(self.config[\"data\"][\"raw_file\"])\n",
    "        # 2) ì „ì²˜ë¦¬\n",
    "        df = self.dp.preprocess_data(df)\n",
    "        self.dp.fit_encoders(df)\n",
    "        self.df = df\n",
    "        # 3) ì„ë² ë”© (ìºì‹œ)\n",
    "        texts = self.eg.build_texts(df, self.config[\"text\"][\"fields_for_embedding\"])\n",
    "        self.embeddings = self.eg.generate_embeddings(texts, self.config[\"data\"][\"embeddings_file\"])\n",
    "        # 4) ì°¨ì›ì¶•ì†Œ\n",
    "        method = self.config[\"model\"][\"dimensionality_reduction\"]\n",
    "        n_comp = self.config[\"model\"][\"reduced_dim\"]\n",
    "        self.eg.fit_dimension_reducer(self.embeddings, method=method, n_components=n_comp)\n",
    "        self.reduced = self.eg.reduce_dimensions(self.embeddings)\n",
    "        np.save(self.config[\"data\"][\"reduced_embeddings_file\"], self.reduced)\n",
    "        # 5) ë¶„ë¥˜ê¸° í•™ìŠµ\n",
    "        y = self.dp.encode_labels(df)\n",
    "        self.trainer = XGBoostTrainer(self.config[\"model\"][\"xgb_params\"])\n",
    "        self.trainer.train_models(self.reduced, y)\n",
    "\n",
    "    def recommend_places(self, query_text: str, top_k: int = 10) -> pd.DataFrame:\n",
    "        if self.df is None or self.reduced is None:\n",
    "            raise RuntimeError(\"ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. prepare()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.\")\n",
    "        # ì¿¼ë¦¬ ì„ë² ë”© â†’ ì°¨ì›ì¶•ì†Œ\n",
    "        q_emb = self.eg.generate_embeddings([query_text], cache_path=\"data/embeddings/_tmp_query.npy\")\n",
    "        q_red = self.eg.reduce_dimensions(q_emb)[0]\n",
    "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
    "        a = self.reduced / (np.linalg.norm(self.reduced, axis=1, keepdims=True) + 1e-9)\n",
    "        b = q_red / (np.linalg.norm(q_red) + 1e-9)\n",
    "        sims = a @ b\n",
    "        idx = np.argsort(-sims)[:top_k]\n",
    "        out = self.df.iloc[idx].copy()\n",
    "        out[\"score\"] = sims[idx]\n",
    "        return out.reset_index(drop=True)\n",
    "\n",
    "    def recommend_places_api(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        query = payload.get(\"q\") or payload.get(\"query\") or \"\"\n",
    "        k = int(payload.get(\"k\", 10))\n",
    "        rec = self.recommend_places(query, top_k=k)\n",
    "        return {\"results\": rec.to_dict(orient=\"records\")}\n",
    "\n",
    "# --- Benchmark / Demo --------------------------------------------------------\n",
    "def _split_y_dict(y_dict, idx_train, idx_test):\n",
    "    y_tr, y_te = {}, {}\n",
    "    for tag, y in y_dict.items():\n",
    "        y_tr[tag] = y[idx_train]\n",
    "        y_te[tag] = y[idx_test]\n",
    "    return y_tr, y_te\n",
    "\n",
    "def benchmark_pipeline(config_override=None, demo_queries=None, top_k=5):\n",
    "    \"\"\"\n",
    "    íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì‹œê°„, ê°„ë‹¨ í™€ë“œì•„ì›ƒ ì •í™•ë„, ì¶”ì²œ API ì‘ë‹µ ì‹œê°„ê¹Œì§€ ì¸¡ì •.\n",
    "    - ê¸°ì¡´ í•¨ìˆ˜/í´ë˜ìŠ¤ ì´ë¦„ì€ ë³€ê²½í•˜ì§€ ì•ŠìŒ.\n",
    "    \"\"\"\n",
    "    cfg = config_override or config\n",
    "\n",
    "    # 1) ì¤€ë¹„(ì„ë² ë”©+ì°¨ì›ì¶•ì†Œ+í•™ìŠµ) ì‹œê°„\n",
    "    t0 = time.perf_counter()\n",
    "    rec = GangwonPlaceRecommender(cfg)\n",
    "    rec.prepare()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # 2) ê°„ë‹¨ í™€ë“œì•„ì›ƒ í‰ê°€ (ë³„ë„ íŠ¸ë ˆì´ë„ˆë¡œ ì¬í•™ìŠµ/í‰ê°€)\n",
    "    y_full = rec.dp.encode_labels(rec.df)\n",
    "    idx = list(range(len(rec.reduced)))\n",
    "    idx_tr, idx_te = train_test_split(idx, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "    X_tr = rec.reduced[idx_tr]\n",
    "    X_te = rec.reduced[idx_te]\n",
    "    y_tr, y_te = _split_y_dict(y_full, idx_tr, idx_te)\n",
    "\n",
    "    trainer = XGBoostTrainer(cfg[\"model\"][\"xgb_params\"])\n",
    "    t2 = time.perf_counter()\n",
    "    trainer.train_models(X_tr, y_tr)\n",
    "    t3 = time.perf_counter()\n",
    "    report = trainer.evaluate_models(X_te, y_te)\n",
    "\n",
    "    # 3) ì¶”ì²œ API ë°ëª¨ ë° ì‘ë‹µ ì‹œê°„\n",
    "    queries = demo_queries or [\"ê°€ì„ ê°ì„± ë°”ë‹¤ ì¹´í˜\", \"ê°€ì¡± ì‚°ì±… ì½”ìŠ¤\", \"ìŠ¤ë¦´ ì•¡í‹°ë¹„í‹°\", \"ì—­ì‚¬ ìœ ì \"]\n",
    "    rec_times = []\n",
    "    rec_examples = []\n",
    "    for q in queries:\n",
    "        s = time.perf_counter()\n",
    "        df_rec = rec.recommend_places(q, top_k=top_k)\n",
    "        e = time.perf_counter()\n",
    "        rec_times.append({\"query\": q, \"elapsed_sec\": e - s})\n",
    "        rec_examples.append((q, df_rec[[\"name\",\"address\",\"score\"]].head(top_k)))\n",
    "\n",
    "    summary = {\n",
    "        \"prepare_time_sec\": t1 - t0,\n",
    "        \"retrain_time_sec\": t3 - t2,\n",
    "        \"eval_report\": report,\n",
    "        \"recommend_times\": rec_times,\n",
    "        \"examples\": rec_examples\n",
    "    }\n",
    "    return rec, summary\n",
    "\n",
    "# --- Main (optional) ---------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\n",
    "    rec, summary = benchmark_pipeline()\n",
    "\n",
    "    print(f\"âœ… ì¤€ë¹„(ì„ë² ë”©+ì°¨ì›ì¶•ì†Œ+í•™ìŠµ) ì‹œê°„: {summary['prepare_time_sec']:.2f}s\")\n",
    "    print(f\"âœ… í™€ë“œì•„ì›ƒ ì¬í•™ìŠµ ì‹œê°„: {summary['retrain_time_sec']:.2f}s\")\n",
    "    print(\"âœ… í‰ê°€ ë¦¬í¬íŠ¸(ë§ˆì´í¬ë¡œ/ìƒ˜í”Œ F1):\")\n",
    "    for tag, rpt in summary[\"eval_report\"].items():\n",
    "        print(f\"  - {tag}: micro_f1={rpt['micro_f1']:.4f}, samples_f1={rpt['samples_f1']:.4f}\")\n",
    "\n",
    "    # ì¶”ì²œ ì§ˆì˜ ì‘ë‹µ ì‹œê°„(ms) í‘œ\n",
    "    rec_times_df = pd.DataFrame(summary[\"recommend_times\"]).assign(ms=lambda d: d[\"elapsed_sec\"]*1000)\\\n",
    "                                                          .drop(columns=[\"elapsed_sec\"])\n",
    "    print(\"\\n[ì¶”ì²œ ì§ˆì˜ ì‘ë‹µ ì‹œê°„(ms)]\")\n",
    "    print(rec_times_df.to_string(index=False))\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    rec_times_path = \"outputs/recommend_latency_ms.csv\"\n",
    "    rec_times_df.to_csv(rec_times_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"ğŸ“ ì €ì¥: {rec_times_path}\")\n",
    "\n",
    "    # ê° ì˜ˆì‹œ ì§ˆì˜ì˜ Top-K ì¶”ì²œ ë¯¸ë¦¬ë³´ê¸° í…Œì´ë¸” í‘œì‹œ + ì €ì¥\n",
    "    for q, df_topk in summary[\"examples\"]:\n",
    "        print(f\"\\n[{q}] Top ì¶”ì²œ ë¯¸ë¦¬ë³´ê¸°\")\n",
    "        print(df_topk.to_string(index=False))\n",
    "        out_path = f\"outputs/preview_{q.replace(' ','_')}.csv\"\n",
    "        df_topk.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"ğŸ“ ì €ì¥: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0750e3-180e-4b56-9982-81252e481e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
